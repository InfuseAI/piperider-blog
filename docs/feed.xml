<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://blog.piperider.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.piperider.io/" rel="alternate" type="text/html" /><updated>2022-12-22T13:53:24+08:00</updated><id>https://blog.piperider.io/feed.xml</id><title type="html">PipeRider</title><subtitle>Data Reliability Automated</subtitle><author><name>PipeRider</name></author><entry><title type="html">Data reliability testing for dbt state with PipeRider</title><link href="https://blog.piperider.io/data-reliability-dbt-state-piperider.html" rel="alternate" type="text/html" title="Data reliability testing for dbt state with PipeRider" /><published>2022-12-06T00:00:00+08:00</published><updated>2022-12-06T00:00:00+08:00</updated><id>https://blog.piperider.io/data-reliability-dbt-state-piperider</id><content type="html" xml:base="https://blog.piperider.io/data-reliability-dbt-state-piperider.html"><![CDATA[<p><em>tl;dr PipeRider now supports <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">dbt state</a>! You can profile and run data reliability tests on only the tables that have been modified as part of your dbt state</em></p>

<hr />

<p>One of the great features of dbt is <a href="https://docs.getdbt.com/reference/node-selection/syntax">node selection</a> and the <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">‘state’ method</a>. Using state, you are able to specify a subset of models (or other resources) to work on. For instance, you might use <code class="language-plaintext highlighter-rouge">state:modified</code> to only build your dbt models that have changed.</p>

<p>The logic behind state is that <strong>there’s no point rebuilding everything if you’ve only changed part of your project</strong>. Likewise, if you’re using a data quality tool with your dbt project (and you should), then the same logic should apply - <strong>you only want to run your data quality and reliability tests against the modified models</strong>.</p>

<p>The latest version of <a href="https://github.com/infuseai/piperider">PipeRider (0.14)</a>, the open-source data reliability tool, adds exactly this feature. If you’re a dbt user and you’re not using PipeRider, you really should check it out. PipeRider adds profiling and data assertions to a dbt project with zero config.</p>

<p>Here’s a <a href="https://www.youtube.com/watch?v=2J2Cu84HonU">video</a> that shows it in action, or scroll down for the relevant commands.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/2J2Cu84HonU" frameborder="0" allowfullscreen=""></iframe></div>

<h2 id="run-data-profiling-and-assertions-on-dbt-state">Run data profiling and assertions on dbt state</h2>

<p>It’s really straightforward to use. Let’s say you’ve modified a dbt model and then built your project specifying only the affected models:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dbt build <span class="nt">--select</span> state:modified+ <span class="nt">--state</span> target
</code></pre></div></div>

<p>Now, when you run PipeRider, all you need to do is specify the dbt state using the <code class="language-plaintext highlighter-rouge">--dbt-state</code> option:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider run <span class="nt">--dbt-state</span> target
</code></pre></div></div>

<p>PipeRider will only profile and test models that are included in the specified dbt state, saving you time and computing resources.</p>

<h2 id="compare-data-profiles">Compare data profiles</h2>

<p>PipeRider also has a handy feature to quickly compare data profile reports:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">piperider</span> <span class="n">compare</span><span class="o">-</span><span class="n">reports</span> <span class="c1">--last</span>
</code></pre></div></div>

<p>You’ll be prompted to select the reports to compare or, if you specify the <code class="language-plaintext highlighter-rouge">--last</code> option, it’ll automatically compare the last two reports - great!</p>

<p>Taking it a step further, you can specify to only compare tables that appear either in the base or target report. This ties in nicely to dbt state because you might not want to include all of the unmodified tables in the comparison report.</p>

<p>To specify which report’s tables to limit the comparison to, use the <code class="language-plaintext highlighter-rouge">--tables-from</code> option.</p>

<div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">piperider</span> <span class="nx">compare</span><span class="o">-</span><span class="nx">reports</span> <span class="o">--</span><span class="nx">last</span> <span class="o">--</span><span class="nx">tables</span><span class="o">-</span><span class="k">from</span> <span class="nx">target</span><span class="o">-</span><span class="nx">only</span>
</code></pre></div></div>

<p>This command auto compares the last two reports, and only compares tables that appeared in the <em>target</em> report. Which, in this case, would be the report that only includes tables modified as per the dbt state.  This is perfect if you want a report that only focuses on the comparison of modified models before-and-after your latest changes.</p>

<h2 id="get-started-with-piperider">Get started with PipeRider</h2>

<p>As I mentioned above, if you’re a dbt user then PipeRider should be your go-to data reliability tool.  If you already have a dbt project, all you need to do is <a href="https://docs.piperider.io/cli/dbt-integration">initialize PipeRider inside the project</a> and your data source settings will be automatically detected:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># inside your dbt project folder</span>
piperider init
</code></pre></div></div>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider now supports dbt state! You can profile and run data reliability tests on only the tables that have been modified as part of your dbt state]]></summary></entry><entry><title type="html">5 Database Schema Changes Data Engineers Need to Beware Of</title><link href="https://blog.piperider.io/database-schema-changes-to-beware-of.html" rel="alternate" type="text/html" title="5 Database Schema Changes Data Engineers Need to Beware Of" /><published>2022-11-21T00:00:00+08:00</published><updated>2022-11-21T00:00:00+08:00</updated><id>https://blog.piperider.io/database-schema-changes-to-beware-of</id><content type="html" xml:base="https://blog.piperider.io/database-schema-changes-to-beware-of.html"><![CDATA[<p>As a data or analytics engineer you need to anticipate the unexpected, especially when building pipelines from data sources that you have no control over. While a data pipeline might be a house of cards, with data observability you’re able to keep an eye on any changes to the underlying structure, and be prepared for schema changes.</p>

<p>Structure is the key point here, as one of the main ways that a data source can change is the schema, or structure, of a database table. The schema is important because the pipelines you build may have specific downstream uses that are directly tied to the current structure of the data. By staying ahead of schema changes you’ll know about potential issues before they affect the data and downstream uses.</p>

<p>As Angela, a data engineer at Sam’s Club, put it in a <a href="https://twitter.com/i/spaces/1OwGWwZAZDnGQ?s=20">recent interview</a>:</p>

<blockquote>
  <p>“you never want your end users to actually contact you, (and if they do) you want to be already in the know”</p>
</blockquote>

<p>Here are 5 schema changes to look out for, and why you should beware of them:</p>

<h2 id="1-additional-columns">1. Additional columns</h2>

<p>Additional columns might seem like one of the more benign changes at first. After all, how could adding some columns to a database affect your pipeline?</p>

<h3 id="why-you-should-beware">Why you should beware</h3>

<p>After a new column is added, there’s no easy way to know if the data was backfilled, without manually checking. That is, for each row, was the data retroactively added? If not, there may be null, zero, or empty values, and you won’t know if they represent valid data or not. This could be problematic if you start using the new column without first knowing the backfill status.</p>

<h2 id="2-column-type-and-attribute-changes">2. Column type and attribute changes</h2>

<p>Column types are important because they enforce the type of data contained in the column. The way you use the data of a column is based on the expectation that the data is of a certain type. Examples of column type changes include:</p>

<ul>
  <li>A numeric column changing to string, and vice-versa.</li>
  <li>A numeric type change, such as a decimal changing to an integer.</li>
  <li>Date is a tricky one — is it Date, Datetime, or Timestamp?</li>
</ul>

<h3 id="why-you-should-beware-1">Why you should beware</h3>

<p>If you’re expecting a float and get an integer, this could affect the accuracy of any calculations based on the data, especially if you require a certain precision (see column attributes).</p>

<p>Sometimes, column types might not even be enforced, so you might run into problems with mixing types, such as storing text and numbers in the same column. This means you need to be very careful about checking the format of the data.</p>

<h2 id="3-column-attributes-and-constraints">3. Column attributes and constraints</h2>

<p>Column attributes refer to the format of the stored data. Things like:</p>

<ul>
  <li>String length</li>
  <li>Size of an integer</li>
  <li>Decimal precision</li>
  <li>Date format</li>
  <li>Default values</li>
  <li>If NULL values are allowed</li>
</ul>

<h3 id="why-you-should-beware-2">Why you should beware</h3>

<p>Changes to numeric types such as integer size, or precision, could affect downstream calculations.</p>

<p>The format of the date could also wreak havoc on your dashboard — is it DD-MM-YYYY, or MM-DD-YYYY? What time zone is this for? These are important questions to ask if you’re building a dashboard that relies on time-specific data.</p>

<p>If the limit on the length of strings changes, it could result in truncated data. An issue that might go unnoticed if not manually checked for.</p>

<h2 id="4-column-renaming">4. Column renaming</h2>

<p>Renaming a column is a big change. If tables are being used in production you’d hope to be notified of a change such as this, but with third party data sources that might not happen. This one could really get you if you’re not paying attention.</p>

<h3 id="why-you-should-beware-3">Why you should beware</h3>

<p>Your transformations and data modeling queries rely heavily on column naming. One column name-change could result in a broken pipeline causing data to stop flowing downstream.</p>

<h2 id="5-column-misuse">5. Column misuse</h2>

<p>Column misuse is more of a data quality issue than a schema change but, like a schema change, could cause issues for you when using the data. When a column is repurposed, it’s used for a purpose it wasn’t originally intended for. If there’s no data dictionary, or documentation is not kept up-to-date, then you could easily be caught out by this one.</p>

<h3 id="why-you-should-beware-4">Why you should beware</h3>

<p>One example of column misuse is putting additional data into a column, instead of creating another column. For instance, a product sales table might start including product attributes, such as size and color, in the description column, instead of creating new columns for the attributes.</p>

<p>The meaning of a column might also change over time and, instead of adjusting the database accordingly, an existing column is ‘repurposed’ for use. Without you knowing, the column now includes completely different data than what you expect.</p>

<h2 id="how-to-avoid-being-caught-out">How to avoid being caught out</h2>

<p>As mentioned in the introduction, it’s all a matter of having the knowledge that a change has taken place — As the saying goes “<a href="https://knowyourmeme.com/memes/france-is-bacon">knowledge is power, France is bacon</a>”. If you know a change has taken place, you can react and make sure that data pipeline continues to work, before downstream issues occur.</p>

<p>For data teams, that knowledge comes from <a href="https://blog.infuseai.io/data-monitoring-be-the-master-of-your-pipeline-ba464b66888">data monitoring and observability</a>. With proper data observability in place you can know the current structure of the database, and how that compares to previous states.</p>

<p>Highlighting that schema changes have taken place, along with other data changes, is one of the things <a href="https://blog.infuseai.io/how-to-be-more-confident-making-data-model-changes-76a2f65feffa">we’re working on</a> with <a href="https://github.com/infuseai/piperider">PipeRider</a>.</p>

<h2 id="your-experiences">Your experiences</h2>

<p>Have you been bitten by schema change? Let me know what happened and how you discovered the change.</p>

<p>If I missed any important or obvious schema changes to look out for, please tell me and I’ll improve the list.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[As a data or analytics engineer you need to anticipate the unexpected, especially when building pipelines from data sources that you have no control over. While a data pipeline might be a house of cards, with data observability you’re able to keep an eye on any changes to the underlying structure, and be prepared for schema changes.]]></summary></entry><entry><title type="html">How do you know you can trust your data?</title><link href="https://blog.piperider.io/how-do-you-know-you-can-trust-your-data.html" rel="alternate" type="text/html" title="How do you know you can trust your data?" /><published>2022-10-23T00:00:00+08:00</published><updated>2022-10-23T00:00:00+08:00</updated><id>https://blog.piperider.io/how-do-you-know-you-can-trust-your-data</id><content type="html" xml:base="https://blog.piperider.io/how-do-you-know-you-can-trust-your-data.html"><![CDATA[<p>Every decision in business is made based on supporting data. “Data-driven” is more than just a buzzword for meetings, it’s a way for a company to be self-aware. Using metrics derived from all sorts of data, it’s possible to understand the performance of each operational unit in a business. From sales figures to marketing efforts, from time sheets to billable hours, data helps us determine what is the correct decision to make, backed up by proof in the form of metrics.</p>

<p>With data being such a valuable commodity, there has to be trust in it. By the time data makes its way downstream, it could have undergone many transformations on the way. With each ingestion, transformation, or abstraction, there is the possibility of mistakes being introduced into the data pipeline.</p>

<p>At each step of the way along the pipeline, how do you ensure that the data is correct and can be trusted?</p>

<p><img src="/img/posts/221023-1.webp" alt="" />
<em>If you can’t trust your data, you can’t make data-drive decisions.</em></p>

<h2>　</h2>
<h2 id="can-we-put-that-in-writing">Can we put that in writing?</h2>
<p>Recently, <a href="https://dataproducts.substack.com/p/the-rise-of-data-contracts">data contracts</a> have been hailed as the savior of the relationship between data producer and data consumer. Data contracts define the ‘rules of the game’ by explicitly stating things like schema structure, ingestion frequency, source tables, and even contact information for stakeholders.</p>

<p>By clearly <a href="https://davidsj.substack.com/p/yet-another-post-on-data-contracts#%C2%A7what-a-data-contract-should-contain">defining the requirements of the the data</a>, the data producer knows what data to serve, and in what format, and the data consumer knows what they will be receiving.</p>

<p>Once the scale of your data pipeline reaches a certain threshold, and you’re <a href="https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline.html">collecting metrics <em>about</em> the pipeline</a>, then it might be time to introduce data contracts. As Gleb, of Data Fold, <a href="https://www.datafold.com/blog/the-best-data-contract-is-the-pull-request">puts it</a>, when “metadata becomes big data… when something breaks, figuring out what the origin is becomes a huge task”. If you’re clear about what’s happening between the connections in the pipeline, this task is made a whole lot easier.</p>

<h2 id="-1">　</h2>
<h2 id="quality-data-prove-it">Quality data? Prove it!</h2>

<p>If you don’t have the luxury of implementing a new system such as data contracts, there are still other things you can do to ensure data quality, and subsequently data trust. Through data profiling and data assertions you’re able to ensure that the data continually meets your requirements.</p>

<p>Data assertions are data ‘contracts’ on some level. Assertions enable you to verify the structure and contents of the data source. If a data assertion fails then you know that some predetermined requirement of the data has not been met. If the the data assertions pass then this is proof that the data meets the needs for downstream use.</p>

<p>Actual proof could take the form of a <a href="https://docs.piperider.io/data-profile-and-metrics/data-profile">data profile</a> that shows a breakdown of metrics about the data, or even better, a set of <a href="https://docs.piperider.io/cli/data-quality-assertions">data assertions</a> and results that prove the data is fit for purpose. By generating profiling reports regularly and automating assertions after ingestions or transformations, you can be sure that unexpected errors are not introduced and the pipeline is functioning correctly.</p>

<h2 id="-2">　</h2>
<h2 id="the-path-to-data-trust">The path to data trust</h2>
<p>The path to trust in a data pipeline comes from confidence that the data is complete. In turn, confidence in that data comes from proving that the data meets the needs of the data consumer, either enforced through data contracts, or demonstrated with assertion test results.</p>

<p>Without some form of proof of data quality, or method to determine that data is not changing in a way that might break downstream usage, there’s no real way to trust that data is accurate.</p>

<p>What methods are you using to test data after ingestions or transformations?</p>

<p>Are you using data contracts, either in production or testing?</p>

<p>I’d love to hear about your use-cases and experiences — please leave a comment and share your experience.</p>

<h2 id="-3">　</h2>
<h3 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h3>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is an open-source data reliability CLI tool that adds data profiling and assertions to data warehouses such as BigQuery, Snowflake, Redshift and more. Data profile and assertions results are provided in an HTML report each time you run PipeRider.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Every decision in business is made based on supporting data. “Data-driven” is more than just a buzzword for meetings, it’s a way for a company to be self-aware. Using metrics derived from all sorts of data, it’s possible to understand the performance of each operational unit in a business. From sales figures to marketing efforts, from time sheets to billable hours, data helps us determine what is the correct decision to make, backed up by proof in the form of metrics.]]></summary></entry><entry><title type="html">Building Reliable Data Oipelines With Profiling and Assertions</title><link href="https://blog.piperider.io/building-reliable-data-pipelines-with-profiling-and-assertions.html" rel="alternate" type="text/html" title="Building Reliable Data Oipelines With Profiling and Assertions" /><published>2022-10-19T00:00:00+08:00</published><updated>2022-10-19T00:00:00+08:00</updated><id>https://blog.piperider.io/building-reliable-data-pipelines-with-profiling-and-assertions</id><content type="html" xml:base="https://blog.piperider.io/building-reliable-data-pipelines-with-profiling-and-assertions.html"><![CDATA[<p>The process of creating a data pipeline involves taking data from various sources and transforming it to make it useful for specific analytical and/or business purposes. It sounds easy enough but, with increasingly complex pipelines, errors can easily go unnoticed.</p>

<p>Creating error-free data pipelines takes a lot of effort from data engineers, but by using data profiling the task of building and maintaining reliable data pipelines is a lot more manageable.</p>

<hr />

<h2 id="data-profiling-and-assertions-for-quality-assurance">Data profiling and assertions for quality assurance</h2>
<p>The metrics that data profiling provides can be used to ensure that data meets the requirements it’s intended for, and also enables you (the data engineer) to spot anomalies or anything out of the ordinary. Essentially providing a layer of quality assurance to the pipeline.</p>

<p>When paired with data assertions, the profile can be tested to ensure that metrics are within desired thresholds. This is not only useful for building the pipeline, but also for continued testing throughout the life of the pipeline. Data profiling is an essential part of creating a dedicated data monitoring pipeline so you can become <a href="https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline.html">‘master of your pipeline’</a>.</p>

<h2>　</h2>
<h2 id="add-data-profiling-and-assertion-to-your-elt-pipeline">Add data profiling and assertion to your ELT pipeline</h2>
<p>The idea is to augment the ELT process with two new stages — profiling, and assertions, creating (drum-roll) ELTPA. In this updated process, a testing loop occurs during the pipeline creation to help ensure that the resulting data is robust and reliable, and then continually once the pipeline is in use.</p>

<p>The following diagram demonstrates the process.
<img src="/img/posts/221019-1.webp" alt="" /></p>

<h2 id="-1">　</h2>
<h2 id="pipeline-creation">Pipeline creation</h2>
<p>After data is transformed, it is profiled and tested with assertions. You can write assertions based on stakeholder requirements or, if you are modifying an existing pipeline, then you may already be aware of common issues.</p>

<p>If an assertion fails and generates an error, you can go back to the transformation phase and find the root cause of the problem before continuing the process.</p>

<h2 id="-2">　</h2>
<h2 id="pipeline-monitoring-and-observability">Pipeline monitoring and observability</h2>
<p>For continual monitoring, a data profile should also be regularly generated. If the data profile metrics reveal a change in data that needs investigation, you can compare the most recent report with previous reports to see what might have happened.</p>

<p>After the issue is resolved, you can update the data assertions to account for the issue and ensure that testing covers similar situations going forward.</p>

<h2 id="-3">　</h2>
<h2 id="conclusion">Conclusion</h2>
<p>A data profile is useful for creating the data pipeline and, when generated regularly, can also be used for monitoring data distribution over time and debugging issues. The addition of assertions makes it a great way to test for data reliability. In many cases, a data profile report can reduce the need for a complex BI dashboard.</p>

<p>If you like this article, it’s a big encouragement to give us a like on <a href="https://twitter.com/InfuseAI/status/1582733321594753024?s=20&amp;t=Nw2rTYJOoFJAWdNIdMTxfw">Twitter</a>.</p>

<h2 id="-4">　</h2>
<h3 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h3>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is an open-source data reliability CLI tool that adds data profiling and assertions to data warehouses such as BigQuery, Snowflake, Redshift and more. Data profile and assertions results are provided in an HTML report each time you run PipeRider.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The process of creating a data pipeline involves taking data from various sources and transforming it to make it useful for specific analytical and/or business purposes. It sounds easy enough but, with increasingly complex pipelines, errors can easily go unnoticed.]]></summary></entry><entry><title type="html">Data Monitoring — Be the Master of Your Pipeline</title><link href="https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline.html" rel="alternate" type="text/html" title="Data Monitoring — Be the Master of Your Pipeline" /><published>2022-10-18T00:00:00+08:00</published><updated>2022-10-18T00:00:00+08:00</updated><id>https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline</id><content type="html" xml:base="https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline.html"><![CDATA[<p>Data pipelines have evolved from a single series of workloads (or DAG within a system), to more abstract semantics with a mix of orchestrated workflows, internal tools, in-warehouse transformations and 3rd party tools exports or ingestion.</p>

<p>The increasing complexity of data pipelines makes monitoring difficult, to implement an effective data monitoring strategy you need to understand the pipeline completely. This could range from tracking the status of pipeline tasks and setting up alerts, right up to being the ultimate “master of your pipeline” by setting up a dedicated monitoring pipeline with metrics.</p>

<hr />

<h2 id="data-monitoring-is-essential">Data monitoring is essential</h2>
<p>Once your data pipeline reaches a certain complexity, the requirement for some kind of monitoring is unavoidable. When you get the call (hopefully monitoring can help you avoid the call) that a dashboard is broken because data isn’t being updated, if you’ve got the pipeline metrics available you’ll know where to look first.</p>

<h3 id="understand-whats-happening">Understand what’s happening</h3>
<p>In its basic form, data monitoring means that you understand what’s happening in the pipeline. Start with understanding the normal execution time for tasks in your data pipelines, then set up SLAs that will raise alerts when they fail.</p>

<p>If you find that individual tasks are hanging, you might set up monitoring for each task separately, and then trigger an alert once a certain time threshold has been exceeded.</p>

<h3 id="how-long-is-too-long">How long is too long?</h3>
<p>How you determine what is a “normal” amount of time for tasks in your pipeline to execute will differ for each case. This is where some sleuthing on your part is required. You’ll need to track the execution time and then work out what is acceptable</p>

<p>Some data warehouses offer execution statistics and, if you use a data monitoring service, check if event logs are available that will contain lots of metadata about your task executions.</p>

<p>Writing tests for each pipeline can be an extensive task, and it requires a lot of effort to maintain. At some point you might consider a more structured approach.</p>

<h2>　</h2>
<h2 id="take-it-to-the-next-level">Take it to the next level</h2>
<p>To get serious about your data monitoring, you can set up a dedicated pipeline for monitoring metrics. In these more complex data monitoring efforts, you can monitor many metrics and set up alerts for each kind of metric.</p>

<h3 id="what-should-you-monitor">What should you monitor?</h3>
<p>Examples of metrics to monitor could include things like -</p>

<ul>
  <li>Data freshness</li>
  <li>Schema types</li>
  <li>The number of rows in a table</li>
  <li>The number of inserted rows</li>
  <li>The percentage of null values</li>
  <li>The acceptable range of values</li>
  <li>The distribution of data</li>
</ul>

<p>The type of metrics you monitor will depend on a mix of what is important to the stakeholder, what issues you are running into on a regular basis and, of course, metrics that you need to monitor to ensure that the pipeline is functioning.</p>

<h3 id="what-are-the-acceptable-thresholds">What are the acceptable thresholds?</h3>
<p>Once you know the metrics that need to be monitored, you need to figure out acceptable thresholds for these metrics. This will either involve asking the stakeholder, or employing some form of data profiling so you can track the metric over time and determine acceptable values and/or ranges.</p>

<h2 id="-1">　</h2>
<h2 id="conclusion">Conclusion</h2>
<p>The complexity of the data monitoring solution you employ will be based on the complexity of your pipeline. I’ve mentioned a few methods that you might use to monitor your data pipelines, but there are many more things to monitor — I’ve not even touched on business logic as that introduced a whole other set of complexity.</p>

<p>What methods are you using for monitoring your data pipelines?</p>

<p>How do you determine what are the important things to monitor and track the metrics over time?</p>

<h2 id="-2">　</h2>
<h3 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h3>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is an open-source data reliability CLI tool that adds data profiling and assertions to data warehouses such as BigQuery, Snowflake, Redshift and more. Data profile and assertions results are provided in an HTML report each time you run PipeRider.</p>

<p>If you found this article useful please consider <a href="https://twitter.com/InfuseAI/status/1582554616092184576?s=20&amp;t=U8_JhQpFZRqjShVTA98vTg">retweeting</a>❤.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Data pipelines have evolved from a single series of workloads (or DAG within a system), to more abstract semantics with a mix of orchestrated workflows, internal tools, in-warehouse transformations and 3rd party tools exports or ingestion.]]></summary></entry><entry><title type="html">5 Metrics Elon Musk Should Check In Twitter’s Data</title><link href="https://blog.piperider.io/5-metrics-elon-musk-should-check-in-twitter-data.html" rel="alternate" type="text/html" title="5 Metrics Elon Musk Should Check In Twitter’s Data" /><published>2022-10-13T00:00:00+08:00</published><updated>2022-10-13T00:00:00+08:00</updated><id>https://blog.piperider.io/5-metrics-elon-musk-should-check-in-twitter-data</id><content type="html" xml:base="https://blog.piperider.io/5-metrics-elon-musk-should-check-in-twitter-data.html"><![CDATA[<p>The value of Twitter has been <a href="https://www.theverge.com/2022/10/4/23387592/elon-musk-twitter-deal-lawsuit-faq">in the news</a> a lot recently. Antics related to Elon Musk’s possible Twitter buyout has led to a legal battle that may result in Twitter releasing a snapshot of data on its users and tweets. If Musk actually gets his hands on this data, what kind of things might he be looking for?</p>

<hr />

<p>Data profiling is a great way to analyze and explore a dataset. A data profile not only helps you to understand the structure of a dataset, but the statistical metrics are also great for exploratory analysis.</p>

<p>In this article, I take some publicly available Twitter datasets and run them through a <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">data profiler</a> to see what interesting metrics I can find.</p>

<h2>　</h2>
<h2 id="the-datasets">The datasets</h2>
<p>I used two datasets for this analysis:</p>

<ul>
  <li>A portion of the <a href="https://archive.org/details/archiveteam-twitter-stream-2021-06">Archive Team Twitter Stream</a> dataset from June 2021. This dataset comes in JSON form, so you’ll need to use a tool such as <a href="https://github.com/cantino/twitter_to_csv">twitter_to_csv</a> to convert the raw Twitter API JSON to CSV.</li>
  <li>A pre-prepared dataset from Kaggle. I found the Gender classification set, with data from 2015, was the most useful as it retained much of the user data, such as profile image URL, retweet count, and profile description.</li>
</ul>

<p>For both methods I used <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a>, an open-source CLI tool for profiling datasets. It outputs an HTML report which makes it easy to see the metrics. There are also data assertions if you need to add testing to your dataset.</p>

<h2 id="-1">　</h2>
<h2 id="the-metrics">The metrics</h2>
<p>So, what things might Elon Musk, or anyone analyzing the value of a social network data need to look out for? What interesting metrics can be found, and what are the implications in regard to determining the ‘value’ of Twitter users and Tweets.</p>

<h2 id="-2">　</h2>
<h3 id="1-account-creation-date">1. Account creation date</h3>
<p>Looking at the account created date in the gender dataset, there are active accounts that date back all the way to 2006.</p>

<p><strong>Implication</strong></p>

<p>Twitter has good user retention, as these are users that have been coming back for years.</p>

<p>Twitter account creation date metric via PipeRider: 
<img src="/img/posts/221013-1.webp" alt="" /></p>

<h2 id="-3">　</h2>
<h3 id="2-profile-image">2. Profile image</h3>
<p>You might not think that a profile image could indicate much, but the notorious <a href="https://www.vox.com/2017/3/31/15139192/twitter-egg-profile-default-change">Twitter egg</a> fiasco proves otherwise. 18.3% of profile images in the Kaggle dataset are duplicates, and it looks like this is because they are using the default image.</p>

<p>The Archive Team dataset was around the same, at 17.1% for my sampling.</p>

<p>Twitter profile image metrics via PipeRider:
<img src="/img/posts/221013-2.webp" alt="" /></p>

<p><strong>Implication</strong></p>

<p>This could be an indication of throw-away accounts, bots, trolls, or generally users who aren’t invested enough to update their profile image.</p>

<h2 id="-4">　</h2>
<h3 id="3-duplicated-tweet-content">3. Duplicated Tweet content</h3>
<p>This doesn’t refer to retweets, or quote tweets, but to tweets with identical content. Why would a tweet contain the same content? There was 8.7% of tweets with duplicated content in the Kaggle dataset.</p>

<p><strong>Implication</strong></p>

<p>This could be an indication of a bot farm spreading propaganda, or users could simply be clicking the “share this article” button on news websites.</p>

<p>Twitter metrics indicate possible bot messages via PipeRider: 
<img src="/img/posts/221013-3.webp" alt="" /></p>

<p>The Kaggle dataset shows a lot bizarre of Weather Chanel tweets.</p>

<p>Interestingly, these tweets cannot be found by searching Twitter now. All I can find is <a href="https://twitter.com/ArdentCrayon/status/722136691809525764?s=20&amp;t=9IY8SlHv4k813B-xxwjGxw">someone asking</a> why there are so many bots spamming the weather channel.</p>

<p>The Archive Team dataset shows many retweets of an account for a Korean band (I think). The <a href="https://twitter.com/WeareoneEXO">account</a> has 13million followers, so I suppose a few thousand fans retweeting isn’t out of the ordinary.</p>

<p>Tweet content metrics via PipeRider:
<img src="/img/posts/221013-4.webp" alt="" /></p>

<h2 id="-5">　</h2>
<h3 id="4-retweet-count">4. Retweet count</h3>
<p>Retweet count was extremely low in both datasets — 99.9% of the tweets in the Kaggle set were not retweeted.</p>

<p><strong>Implication</strong></p>

<p>It could mean low interaction rates on Twitter, but is probably just an indication that the dataset time frame is too narrow — only an hour, and that it takes a while for a tweet to gain momentum. This could be one to analyze over a period of a few hours or days.</p>

<p>Retweet metrics via PipeRider:
<img src="/img/posts/221013-5.webp" alt="" /></p>

<h2 id="-6">　</h2>
<h3 id="5-follower-count">5. Follower count</h3>
<p>Follower count not only indicates how popular you are, but also if users are interacting and ‘listening’ to each other on Twitter. Are most users shouting into the wind? According to the Archive Team dataset, 4800 users were talking to themselves during that time frame.</p>

<p><strong>Implication</strong></p>

<p>It’s difficult to gauge this one without more analysis of the accounts with zero followers. They could be the bots we mentioned earlier, or just angry loners shouting in the wind.</p>

<p>Follower count metrics via PipeRider: 
<img src="/img/posts/221013-6.webp" alt="" /></p>

<h2 id="-7">　</h2>
<h2 id="conclusion">Conclusion</h2>
<p>Data exploration is fun, even if you’re not a billionaire mulling over the potential purchase of a $44 billion social network.</p>

<p>Data profiling can provide interesting insights into a dataset that you may not have previously thought of. Even the process of cleaning and transforming data in preparation for analysis can yield interesting results. For instance, while importing the Archive Team dataset, I found that there were many ‘deleted’ entries listed. I presume these are deleted tweets, a figure that could also offer some valuable insight.</p>

<h2 id="-8">　</h2>
<h2 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h2>
<p>If you’re interested in the tool I used here, check out PipeRider — it’s a CLI tool that profiles all the popular data warehouses and provides an HTML data profile report.</p>

<p>Also, before you finish the reading the contracts for your Twitter purchase, please follow us on Twitter — we’re <a href="https://twitter.com/infuseai">@infuseai</a> ❤</p>

<p>If you found this article interesting please <a href="https://twitter.com/InfuseAI/status/1580574430442377217?s=20&amp;t=GwuHVBwmjPMAU8nWlXetBw">retweet</a> to support more content like this.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The value of Twitter has been in the news a lot recently. Antics related to Elon Musk’s possible Twitter buyout has led to a legal battle that may result in Twitter releasing a snapshot of data on its users and tweets. If Musk actually gets his hands on this data, what kind of things might he be looking for?]]></summary></entry><entry><title type="html">Guide to Data Reliability</title><link href="https://blog.piperider.io/guide-to-data-reliability.html" rel="alternate" type="text/html" title="Guide to Data Reliability" /><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>https://blog.piperider.io/guide-to-data-reliability</id><content type="html" xml:base="https://blog.piperider.io/guide-to-data-reliability.html"><![CDATA[<p>Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.</p>

<p>You may be collecting a large amount of information within your data stack, but are you certain that these data sets are complete, accurate, and up-to-date? If not, these data sets might cost you a lot.</p>

<p>IBM estimated that the yearly cost of poor quality data, in the US alone, in 2016, is a whopping $3.1 trillion. In 2021, Gartner reported that every year, unreliable data costs organizations an average of $12.9 million. And it’s safe to say that the number has very likely increased as data-driven decision-making is adopted by every business imaginable.</p>

<p>That’s why ensuring your data is trustworthy by improving data reliability is very important.</p>

<h2 id="what-is-data-reliability">What is Data Reliability?</h2>

<p>Data reliability means that data is complete, accurate, and valid. It’s the foundation for building trust in your data across the organization. One of the main objectives of ensuring data reliability is building data trust, which is also used to maintain data security, data quality, and regulatory compliance.</p>

<p>Reliable data helps decision-makers take the guesswork out of the daily and strategic decision-making process to keep their organizations running. But if your data is unreliable, those same decisions become less accurate and can ultimately affect your organization.</p>

<h2 id="why-data-reliability">Why Data Reliability</h2>

<p>When unreliable data is used in making a key strategic decision, it can result in a mistake that damages an organization’s reputation, and bottom line, or even causes its future. Data reliability issues might not seem like a big deal at first glance, but they can snowball over time if left unchecked.</p>

<p>For example, you use customer data to develop targeted online ads or recommend products to your consumers. If the data you use isn’t accurate, then there’s a good chance that the advertising budget will be wasted on either poor results or zero return on investment.</p>

<p>The unsettling feeling when you are not sure if you can trust your data to make a decision can be highly stressful, ut there are actions you can take to improve your data reliability.</p>

<h2 id="how-to-improve-data-reliability">How to Improve Data Reliability</h2>

<p>Like many other managerial tasks, the process to improve your data reliability follows a series of logical steps. There are eight action items that your organization can take to improve your data reliability:</p>

<ol>
  <li>Assess Data Status</li>
  <li>Build Data Infrastructure</li>
  <li>Clean Existing Data</li>
  <li>Optimize Data Collection Processes</li>
  <li>Break Down All Data Silos</li>
  <li>Integrate Data Stack to Connect Data</li>
  <li>Organize Your Data</li>
  <li>Use Reports and Dashboards</li>
</ol>

<h3 id="assess-data-status">Assess Data Status</h3>

<p>Assessing your current data status is the first thing to do to improve data reliability. It helps you to get a general view of how your organization treats data. You should also employ data profiling. Data profiling is the process of examining and analyzing data. This helps you understand if your data is healthy. Assess your current situation to understand:</p>

<p>What are your data sources;
How and where you have stored the data;
How and where you have used the data;
The criteria used to determine data reliability.</p>

<h3 id="build-data-infrastructure">Build Data Infrastructure</h3>

<p>Once you’ve assessed your current situation, you can start updating your data infrastructure. No matter what the original data sources are, you need a secure and easy-to-use data repository. You need to define how your data will be stored, formatted, and organized. There are several steps you can take to create a data infrastructure:</p>

<p>Refine your strategy.
Build a data model.
Choose your data repository type – data lake, data warehouse, or hybrid.
Build an extract, transform, and load (ETL) process.
Implement ongoing data governance.</p>

<h3 id="clean-existing-data">Clean Existing Data</h3>

<p>If you have data sets in place already, you should examine the existing data and remove data that is:</p>

<ul>
  <li>inaccurate;</li>
  <li>incomplete;</li>
  <li>duplicative;</li>
  <li>outdated;</li>
  <li>incorrectly formatted.</li>
</ul>

<p>You should employ data profiling to analyze your data continually, so you can clean, and update data errors as soon as they are spotted.</p>

<h3 id="optimize-data-collection-processes">Optimize Data Collection Processes</h3>

<p>Start by analyzing internal processes for data input. Automate data entry wherever possible to minimize human errors. Make sure that all data entry follows your standardized formats and is accurate, complete, and valid.</p>

<p>Next, look at other data sources you obtain new data from. Make sure that their data formats follow your standardized format and remove inaccurate and unreliable data.</p>

<h3 id="break-down-all-data-silos">Break Down All Data Silos</h3>

<p>Organizations collect data from different departments or locations. This is necessary due to operational requirements or structure setup. But this might create independent data silos that would affect data reliability.</p>

<p>Not only do silos make it difficult to find and share data across your organization, but they also often adhere to different standards of organization and quality.</p>

<p>To ensure the most reliable data is available to those who need it internally, you need to break down your organization’s data silos. You should employ a central data repository for all departments and locations to minimize potential damage to data quality.</p>

<h3 id="integrate-data-stack-to-connect-data">Integrate Data Stack to Connect Data</h3>

<p>Quite often, different departments or locations use various tools and platforms. If you can get everyone to use the same tool and platform, great. If not, you should connect data from these tools and platforms across your entire organization to have a unified view of all your data. Therefore, when a piece of data is updated in one location, it is automatically updated wherever else it is used.</p>

<h3 id="organize-your-data">Organize Your Data</h3>
<p>Every organization has its unique way of organizing data to meet its unique needs. Organizing data makes it easier to locate specific data and speeds up your data retrieval process.</p>

<p>Typically, you will find labels, tags, groups, and other information stored in metadata. Depending on the type and use of your data, you may find data segmented by customer age, gender, geographic location, demographics, etc. No matter how you organize your data, make sure you understand the overall organization’s expectations and what it would like to achieve using the data.</p>

<h3 id="use-reports-and-dashboards">Use Reports and Dashboards</h3>

<p>Finally, make sure you are able to get insight from your data with reports and dashboards. For example, a data profile report can continue to alert you of data errors when it occurs. Other reports that track key metrics in a visual way with detailed analyses put peace of mind in you when it comes to making data-driven decisions.</p>

<h2 id="automate-your-data-reliability-with-piperider">Automate Your Data Reliability With PipeRider</h2>

<p>It may feel overwhelming when you manage a large amount of data, but once you lay the groundwork and build the foundation, there are many tools you can use to make the journey easier. If you’re interested in learning more about your data, with the aim of improving your data reliability, PipeRider can help you.</p>

<p>PipeRider is an open-source, free, and easy-to-use data reliability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements, enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.]]></summary></entry><entry><title type="html">Data Observability Explained and How to Integrate It into Your Workflow</title><link href="https://blog.piperider.io/guide-to-data-observability.html" rel="alternate" type="text/html" title="Data Observability Explained and How to Integrate It into Your Workflow" /><published>2022-09-23T00:00:00+08:00</published><updated>2022-09-23T00:00:00+08:00</updated><id>https://blog.piperider.io/guide-to-data-observability</id><content type="html" xml:base="https://blog.piperider.io/guide-to-data-observability.html"><![CDATA[<p>The number of data sources that data teams have to deal with is ever increasing. According to a <a href="https://www.matillion.com/resources/blog/matillion-and-idg-survey-data-growth-is-real-and-3-other-key-findings">recent survey</a> by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.</p>

<h2 id="what-is-data-observability">What is Data Observability?</h2>

<p>Data observability is the capability to comprehend, assess, and manage the state of data consumed by various technologies throughout the entire data lifecycle.</p>

<p>With data observability, your team can have a better understanding of your data. So they can gather consistent, standardized data from APIs, support data lake observability, facilitate routine queries to data warehouses, and share high-quality data across the entire organization.</p>

<h2 id="why-data-observability">Why Data Observability?</h2>

<p>One of the benefits of data observability is that teams can monitor data pipelines and quickly identify data issues with end-to-end data visibility.</p>

<p>Before data observability, teams might struggle with various data issues such as outdated data, broken data pipelines, or missing data. These issues might be caused by uncertainty in data standards or different data models from different data providers.</p>

<p>With data observability, your team can</p>

<ul>
  <li>standardize data for monitoring;</li>
  <li>debug and triage proactively;</li>
  <li>understand how data interacts with different tools;</li>
  <li>identify issues early;</li>
  <li>minimize the negative impact of data issues.</li>
</ul>

<p>Data observability also makes it possible for your team to automate parts of your monitoring process to constantly improve data quality with less time spent.</p>

<h2 id="what-does-data-observability-track">What Does Data Observability Track?</h2>

<p>Data profiling is an essential part of data observability. Through the following data profiling techniques, you can further understand your data and apply checks that will alert you to issues with your data.</p>

<p>Row-level validation and column-level profiling provide information about the system-wide performance of your data.
Anomaly detection helps spot problems before they damage data quality.
A statistics summary provides an in-depth understanding of the elements of your data observability framework.
Execution metadata and delays analysis throughout data pipelines to prevent data downtime.</p>

<p>These observability techniques should give you a comprehensive insight into the overall data health, potential data issues, and the quality of your data.</p>

<p>Incorporate Data Observability into Work to Improve Data Quality (h2)
According to research, one-third of data analysts spend more than 40 percent of their time on standardizing data to make it ready for analysis, and 57 percent of organizations still regard the “work of transforming their data to be very difficult.” It is obvious that ensuring consistent and accurate data can be a difficult and expensive task for organizations.</p>

<p>Therefore, having proper and solid data observability set up not only saves time but also a lot of resources, including money - But how do you incorporate data observability into your data quality workflow? You should start by developing a framework, then a strategy, and based on these two, choose the right tool for data observability.</p>

<h2 id="how-to-develop-a-data-observability-framework">How to Develop a Data Observability Framework</h2>

<p>Start your data observability journey by creating an efficient data-driven framework focusing on data quality, consistency, and reliability.</p>

<p>A data observability framework should answer the following questions:</p>

<ul>
  <li>How fresh and up-to-date is our data?</li>
  <li>What expected data value should we verify to ensure credible data?</li>
  <li>What data do we need to track and test to see when the data is broken?</li>
  <li>What is the responsibility of each team to various data sets?</li>
  <li>What other workflow, such as gathering metadata, or mapping upstream data sources and downstream users, do we need?</li>
</ul>

<p>The framework should give your team an overall view of standardized data across the organization, letting them quickly identify and fix problems.</p>

<h2 id="how-to-develop-a-data-observability-strategy">How to Develop a Data Observability Strategy</h2>

<p>Once you have a framework in place, many teams may jump right into integrating data observability into the entire data stack. But putting data observability into practice goes beyond the tools you employ.</p>

<p>You should start with preparing your team to adopt a culture of data-driven collaboration. Think about how to integrate data across different teams and sources, and also consider if implementing a new observability tool will affect existing workflow and resources.</p>

<p>Then incorporate the framework into your strategy to determine a standardized library/guidelines with the characteristics of quality data. Your team can use the guidelines to connect data from all sources.</p>

<p>Finally, incorporate your data sources into the observability tool. To obtain the metrics, logs, and traces required to provide end-to-end visibility, you might need to create new observability pipelines. Correlate the metrics you are tracking in your tool with targeted organization goals after adding the governance and data management rules. By using your observability tool to identify and address problems, you can also find new ways to automate some of your data management processes.</p>

<h2 id="how-to-choose-the-right-data-observability-tool">How to Choose the Right Data Observability Tool</h2>

<p>While there’s no one tool to fit every organization’s needs, a good observability tool should be able to:</p>

<ul>
  <li>gather, examine, sample, and process telemetry data from various data sources;</li>
  <li>detect and alert problems in datasets;</li>
  <li>provide end-to-end visibility;</li>
  <li>display data visualizations.</li>
</ul>

<p>In order to choose a suitable data observability tool, you’ll need to examine your current data stack and get a full picture of how data is gathered and distributed. Then you can look into a tool that is ready to integrate all of your data sources. Your chosen tool should be able to monitor your data in real-time throughout its lifecycle and monitor existing data without extraction. In addition, it should also be able to automate your data observability with minimum effort.</p>

<p>Ultimately, your organization’s specific data stack and data engineering requirements will determine the right tool for you. For the best implementation experience, give top priority to finding a tool that requires less work to standardize your data, map your data, or monitor your data.</p>

<h2 id="integrate-data-observability-with-piperider">Integrate Data Observability with PipeRider</h2>

<p>PipeRider is an open-source, free, and easy-to-use data observability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The number of data sources that data teams have to deal with is ever increasing. According to a recent survey by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.]]></summary></entry><entry><title type="html">Adding Data Observability and Alerts to your Data Pipeline is easier than you think</title><link href="https://blog.piperider.io/adding-data-observability-and-alerts-to-your-data-pipeline-is-easier-than-you-think.html" rel="alternate" type="text/html" title="Adding Data Observability and Alerts to your Data Pipeline is easier than you think" /><published>2022-09-05T00:00:00+08:00</published><updated>2022-09-05T00:00:00+08:00</updated><id>https://blog.piperider.io/adding-data-observability-and-alerts-to-your-data-pipeline-is-easier-than-you-think</id><content type="html" xml:base="https://blog.piperider.io/adding-data-observability-and-alerts-to-your-data-pipeline-is-easier-than-you-think.html"><![CDATA[<p>After you’ve transformed the data in your data warehouse and sent it on its way, you might think that your job is done. That is, until you get a call that there’s missing data, an unexpected schema change, or some unexpected data or outlier has been introduced. To understand when these issues might have occurred, you need some form of data observability for your pipeline.</p>

<p>Data observability means that you can monitor the data moving through the pipeline, be alerted to any changes or issues with the data structure, and compare data to help visualize change and aid in tracking down issues.</p>

<hr />

<p>With the open-source data observability toolkit, <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a>, you can add data observability to your data source and start understanding more about your data in minutes with:</p>

<p>Non-intrusive implementation — Focus on understanding your data without changing it
Data profiling — In-depth analysis of the structure of your data source
Data assertions — Ensure your data stays within acceptable ranges through testing
Reporting — The data profile and testing results are exported to an HTML report
Following are the steps you need to get starting adding data observability and data assertions to your existing data pipeline.</p>

<h2 id="1-install-piperider">1. Install PipeRider</h2>
<p>PipeRider is installed via pip:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -U piperider
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/vZVHo09fD-c" frameborder="0" allowfullscreen=""></iframe></div>

<p>By default it comes with SQLite, but the following connectors are also available:</p>

<ul>
  <li>Postgres</li>
  <li>Snowflake</li>
  <li>BigQuery</li>
  <li>Redshift</li>
  <li>dbt (with one of the supported connectors)</li>
  <li>duckdb</li>
  <li>CSV</li>
  <li>Parquet</li>
</ul>

<p>Install PipeRider with a connector like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -U 'piperider[postgres,snowflake]'
</code></pre></div></div>
<h2>　</h2>
<h2 id="2-initialize-a-piperider-project">2. Initialize a PipeRider project</h2>
<p>Once installed, initialize a new project with the following command.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider init
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/jRxZQJoMQGc" frameborder="0" allowfullscreen=""></iframe></div>

<p>Just select your data source, enter the relevant details and you’re ready to go. dbt project settings will be auto-detected, so dbt projects really are zero config!</p>

<p>Verify your connection settings with the diagnose command.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider diagnose
</code></pre></div></div>
<h2 id="-1">　</h2>
<h2 id="3-run-piperider">3. Run PipeRider</h2>
<p>With a data source connected you’re ready to run PipeRider.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider run
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/Y9KIqu2DOsg" frameborder="0" allowfullscreen=""></iframe></div>

<p>This one command will do the following:</p>

<ul>
  <li>Profile your data source</li>
  <li>Generate recommended data assertions (on first run)</li>
  <li>Test the data profile against the data assertions</li>
  <li>Display the data assertion test results on the CLI</li>
  <li>Generate an HTML report with the data profile and test results</li>
</ul>

<h2 id="-2">　</h2>
<h2 id="4-test-your-piperider-data-profile-with-data-assertions">4. Test your PipeRider data profile with data assertions</h2>
<p>PipeRider creates a set of recommended assertions based on the current state of your data. You can add to or edit these using the available suite of <a href="https://docs.piperider.io/cli/data-quality-assertions/assertion-configuration">built-in assertions</a>, and through custom assertions you can <a href="https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider.html">create your own data reliability tests</a>.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/t4YHkynRIpI" frameborder="0" allowfullscreen=""></iframe></div>

<h2 id="-3">　</h2>
<h2 id="5-compare-data-profile-reports">5. Compare data profile reports</h2>
<p>When your data changes and you have multiple PipeRider runs, compare reports easily with the following command.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider compare-reports
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/COohk2TAiPA" frameborder="0" allowfullscreen=""></iframe></div>

<p>You can also compare the last two reports automatically (without needing to manually select them) by using the <code class="language-plaintext highlighter-rouge">--last</code> flag.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider compare-reports —last
</code></pre></div></div>

<h2 id="-4">　</h2>
<h2 id="sample-reports">Sample Reports</h2>
<p>Links to example reports will always be available in the PipeRider documentation. Here are samples created with PipeRider 0.7:</p>

<p><a href="https://piperider-github-readme.s3.ap-northeast-1.amazonaws.com/run-0.7.0/index.html#/">Sample PipeRider data profile report</a>
<a href="https://piperider-github-readme.s3.ap-northeast-1.amazonaws.com/comparison-0.7.0/index.html">Sample PipeRider report comparison</a></p>

<h2 id="-5">　</h2>
<hr />
<h2 id="who-makes-piperider">Who makes PipeRider?</h2>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is developed by <a href="https://infuseai.io/">InfuseAI</a>, the company behind the end-to-end machine learning platform <a href="https://www.infuseai.io/primehub-ai-platform">PrimeHub</a>.</p>

<p>InfuseAI has an impressive <a href="https://github.com/InfuseAI/">portfolio of open-source projects</a> so you know you’re in good hands!</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[After you’ve transformed the data in your data warehouse and sent it on its way, you might think that your job is done. That is, until you get a call that there’s missing data, an unexpected schema change, or some unexpected data or outlier has been introduced. To understand when these issues might have occurred, you need some form of data observability for your pipeline.]]></summary></entry><entry><title type="html">Define your own Data Quality Tests in PipeRider</title><link href="https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider.html" rel="alternate" type="text/html" title="Define your own Data Quality Tests in PipeRider" /><published>2022-09-02T00:00:00+08:00</published><updated>2022-09-02T00:00:00+08:00</updated><id>https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider</id><content type="html" xml:base="https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider.html"><![CDATA[<p><strong>tl;dr</strong> <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> comes with a built-in suite of data quality assertions, but you can also define your own assertions to meet your needs. This guide will show you how to implement a custom assertion to allow a specific number of nulls without raising an alert.</p>

<hr />

<p>You don’t always have the luxury of a complete dataset, and that’s not always a bad thing — there are some cases in which a certain amount of missing data, or null values, can be acceptable in a data source.</p>

<p>Sometimes a certain amount of NULL values is OK</p>

<h2 id="missing-data-no-problem">Missing data? No problem</h2>
<p>Let’s say you have some incomplete data in a sales database and, even so, you still need to perform some calculations based on the data. You might be willing to allow a certain percentage of missing values before worrying about it affecting your results.</p>

<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> has <a href="https://docs.piperider.io/cli/data-quality-assertions/assertion-configuration">built-in assertions</a> for testing if a column does not contain nulls, or that a it must be null, but there is no middle-ground for allowing for a certain quantity of nulls — custom assertions to the rescue!</p>

<p><img src="/img/posts/220902-1.webp" alt="" /></p>

<p><em>For the rest of this article, I’ll assume you already have a PipeRider project set up — If not, it’s really easy to do, the <a href="https://docs.piperider.io/cli/quick-start">Quick Start</a> guide will have you up and running in minutes.</em></p>

<h2 id="from-data-profile-to-data-assertions">From data profile to data assertions</h2>
<p>Each time you run PipeRider it creates a <a href="https://docs.piperider.io/data-profile-and-metrics/data-profile">profile of your data</a>, and then by applying data assertions you can test the contents of the profile to ensure the data meets your structural requirements.</p>

<p>Using the missing data example from above, we’ll create a custom assertion that checks the number of nulls in a column, and raises an alert if there are more than our specified amount.</p>

<h2 id="custom-column-assertion-template">Custom Column-Assertion Template</h2>
<p>Here’s the template we’ll be using to create our custom column-assertion, Steps 1 to 5 in the code-comments are what we’ll be working on:</p>

<script src="https://gist.github.com/DaveFlynn/7f26ee7cdecedef332d4b5601dec721f.js"></script>

<h2 id="create-your-custom-column-assertion">Create your custom column-assertion</h2>
<p>In your PipeRider project, open <code class="language-plaintext highlighter-rouge">.piperider/plugins/customized_assertions.py</code> in your favorite text editor and paste the above template at the bottom, above where it says <code class="language-plaintext highlighter-rouge"># register new assertions</code></p>

<p>Give the assertion a name. Here, I’ve named the class <code class="language-plaintext highlighter-rouge">AssertNullCount</code> and the name of <code class="language-plaintext highlighter-rouge">assert_null_count</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class AssertNullCount<span class="o">(</span>BaseAssertionType<span class="o">)</span>:
  def name<span class="o">(</span>self<span class="o">)</span>:
    <span class="k">return</span> <span class="s2">"assert_null_count"</span>
</code></pre></div></div>

<p>Now, I’ll run you through the commented steps 1 - 5 in the assertion template.</p>

<h3 id="1-get-the-desired-metric">1: Get the desired metric</h3>
<p>The first thing we need to do is get the value of the desired metric we want to test. For this assertion, we’ll be testing the number of <strong>nulls</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 1. Get the metric for the desired column, e.g. num of nulls in column
nulls = column_metrics.get('nulls')
</code></pre></div></div>

<p>For a full list of available metrics, check the <a href="https://docs.piperider.io/data-profile-and-metrics/data-profile">Data Profile</a> page in the PipeRider documentation.</p>

<h3 id="2-get-the-expected-value-from-the-assertion-file">2: Get the expected value from the assertion file</h3>
<p>You could hard-code the the number of allowed nulls, but then we couldn’t re-use the assertion. Luckily, it’s possible to pass a value from the assertion YAML file into the custom assertion.</p>

<p>Here we grab the value of <code class="language-plaintext highlighter-rouge">allowed_null</code>s`. I’ll show you how to define the value of this variable in your assertion YAML later on.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 2. Get expectation from assertion file
allowed_nulls = context.asserts.get('allowed_nulls', [])
</code></pre></div></div>

<h3 id="34-implement-logic-to-test-the-metric-and-either-pass-or-fail">3,4: Implement logic to test the metric and either pass or fail</h3>
<p>After comparing the value of nulls with the value of allowed_nulls, we return a pass result if nulls is less than or equal to our threshold, and a fail otherwise.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 3. Implement logic to compare expected and actual metric value
if (nulls &lt;= allowed_nulls):
	# 4. return pass (acceptable number of nulls)
	return context.result.success('There are {} null values'.format(nulls))
else:
	# 4. or return fail (more nulls than we expected)
	return context.result.fail(nulls)
</code></pre></div></div>

<h3 id="5-register-the-new-assertion">5: Register the new assertion</h3>
<p>Now that you’ve created the assertion, register it with PipeRider. <strong>This goes outside of your new class</strong>, at the bottom of the <code class="language-plaintext highlighter-rouge">customized_assertions.py</code> file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#register new assertion
register_assertion_function(AssertNullCount)
</code></pre></div></div>

<h2 id="completed-assertion-class">Completed assertion class</h2>
<p>The completed custom assertion class should now look like this:</p>

<script src="https://gist.github.com/DaveFlynn/37b9f92e2784187566a6b4f8d3101250.js"></script>

<h2 id="put-your-custom-assertion-into-action">Put your custom assertion into action</h2>
<p>Open the assertions file for the table you want to run the new assertion on. You can find the assertion YAML files in <code class="language-plaintext highlighter-rouge">.piperider/assertions/&lt;table&gt;.yml</code>. If you generated <a href="https://docs.piperider.io/cli/quick-start#generate-data-assertions">recommended assertions</a> then your table assertions file should already be populated.</p>

<p>Find the desired column and apply your custom assertion. In example below, I have applied the new assertion to the Global Sales column:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Global_sales: # Column Name
  description: 'Combined global sales figures'
  # Test Cases for Column
  tests:
  - name: assert_allowed_nulls
    assert:
        allowed_nulls: 5
</code></pre></div></div>

<p>The last line, <code class="language-plaintext highlighter-rouge">allowed_nulls</code>, is the value we pass through to the custom assertion and check the actual number of nulls against.</p>

<h2 id="run-piperider-to-check-that-it-works">Run PipeRider to check that it works</h2>
<p>The next time you run PipeRider your custom assertion will be tested.</p>

<p><strong>Passed assertion</strong>
If it passes, you’ll see the test with the status <code class="language-plaintext highlighter-rouge">[OK]</code>.</p>

<p>all.Global_sales assertion passed with 5 nulls out of 10 allowed
<img src="/img/posts/220902-2.webp" alt="all.Global_sales assertion passed with 5 nulls out of 10 allowed" /></p>

<p><strong>Failed assertion</strong>
If it fails you’ll see it in the list of <strong>failed assertions</strong>.</p>

<p>all.Global_sales failed with 15 nulls — 5 more than the 10 allowed
<img src="/img/posts/220902-3.webp" alt="all.Global_sales failed with 15 nulls — 5 more than the 10 allowed" /></p>

<h2 id="take-it-a-step-further">Take it a step further</h2>
<p>Specifying an exact number of allowed nulls might not be an ideal solution for all datasets. For instance, when dealing with large datasets, you might prefer to allow a certain percentage of nulls. In that case, the code we wrote only needs a small adjustment.</p>

<p>All you need to do is get the total number of rows for the table; the number of nulls; then work out the percentage of nulls and test if the percentage exceeds your threshold of allowed nulls.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># get total number of rows
total = column_metrics.get('total')

# get the number of nulls
nulls = column_metrics.get('nulls')

#work out the percentage of nulls
percent_nulls = round(((100 / total) * nulls), 2)

# get the number of allowed nulls from your assertion definition
allowed_nulls = context.asserts.get('allowed_nulls')
if (nulls &gt; percent_nulls * allowed_nulls): 
    # too many nulls
    return context.result.fail('{}%'.format(percent_nulls))
else: 
    # acceptable percentage of nulls
    return context.result.success('There are {}% nulls'.format(percent_nulls))
</code></pre></div></div>

<p>You’re only limited by the metrics available in the data profile, and your ability to write Python, so check the <a href="https://docs.piperider.io/">PipeRider documentation</a> and <strong>start making your data more reliable today</strong>!</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider comes with a built-in suite of data quality assertions, but you can also define your own assertions to meet your needs. This guide will show you how to implement a custom assertion to allow a specific number of nulls without raising an alert.]]></summary></entry></feed>