<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://blog.piperider.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.piperider.io/" rel="alternate" type="text/html" /><updated>2022-12-21T18:36:07+08:00</updated><id>https://blog.piperider.io/feed.xml</id><title type="html">PipeRider</title><subtitle>Data Reliability Automated</subtitle><author><name>PipeRider</name></author><entry><title type="html">Data reliability testing for dbt state with PipeRider</title><link href="https://blog.piperider.io/data-reliability-dbt-state-piperider.html" rel="alternate" type="text/html" title="Data reliability testing for dbt state with PipeRider" /><published>2022-12-06T00:00:00+08:00</published><updated>2022-12-06T00:00:00+08:00</updated><id>https://blog.piperider.io/data-reliability-dbt-state-piperider</id><content type="html" xml:base="https://blog.piperider.io/data-reliability-dbt-state-piperider.html"><![CDATA[<p><em>tl;dr PipeRider now supports <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">dbt state</a>! You can profile and run data reliability tests on only the tables that have been modified as part of your dbt state</em></p>

<hr />

<p>One of the great features of dbt is <a href="https://docs.getdbt.com/reference/node-selection/syntax">node selection</a> and the <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">‘state’ method</a>. Using state, you are able to specify a subset of models (or other resources) to work on. For instance, you might use <code class="language-plaintext highlighter-rouge">state:modified</code> to only build your dbt models that have changed.</p>

<p>The logic behind state is that <strong>there’s no point rebuilding everything if you’ve only changed part of your project</strong>. Likewise, if you’re using a data quality tool with your dbt project (and you should), then the same logic should apply - <strong>you only want to run your data quality and reliability tests against the modified models</strong>.</p>

<p>The latest version of <a href="https://github.com/infuseai/piperider">PipeRider (0.14)</a>, the open-source data reliability tool, adds exactly this feature. If you’re a dbt user and you’re not using PipeRider, you really should check it out. PipeRider adds profiling and data assertions to a dbt project with zero config.</p>

<p>Here’s a <a href="https://www.youtube.com/watch?v=2J2Cu84HonU">video</a> that shows it in action, or scroll down for the relevant commands.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/2J2Cu84HonU" frameborder="0" allowfullscreen=""></iframe></div>

<h2 id="run-data-profiling-and-assertions-on-dbt-state">Run data profiling and assertions on dbt state</h2>

<p>It’s really straightforward to use. Let’s say you’ve modified a dbt model and then built your project specifying only the affected models:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dbt build <span class="nt">--select</span> state:modified+ <span class="nt">--state</span> target
</code></pre></div></div>

<p>Now, when you run PipeRider, all you need to do is specify the dbt state using the <code class="language-plaintext highlighter-rouge">--dbt-state</code> option:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider run <span class="nt">--dbt-state</span> target
</code></pre></div></div>

<p>PipeRider will only profile and test models that are included in the specified dbt state, saving you time and computing resources.</p>

<h2 id="compare-data-profiles">Compare data profiles</h2>

<p>PipeRider also has a handy feature to quickly compare data profile reports:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">piperider</span> <span class="n">compare</span><span class="o">-</span><span class="n">reports</span> <span class="c1">--last</span>
</code></pre></div></div>

<p>You’ll be prompted to select the reports to compare or, if you specify the <code class="language-plaintext highlighter-rouge">--last</code> option, it’ll automatically compare the last two reports - great!</p>

<p>Taking it a step further, you can specify to only compare tables that appear either in the base or target report. This ties in nicely to dbt state because you might not want to include all of the unmodified tables in the comparison report.</p>

<p>To specify which report’s tables to limit the comparison to, use the <code class="language-plaintext highlighter-rouge">--tables-from</code> option.</p>

<div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">piperider</span> <span class="nx">compare</span><span class="o">-</span><span class="nx">reports</span> <span class="o">--</span><span class="nx">last</span> <span class="o">--</span><span class="nx">tables</span><span class="o">-</span><span class="k">from</span> <span class="nx">target</span><span class="o">-</span><span class="nx">only</span>
</code></pre></div></div>

<p>This command auto compares the last two reports, and only compares tables that appeared in the <em>target</em> report. Which, in this case, would be the report that only includes tables modified as per the dbt state.  This is perfect if you want a report that only focuses on the comparison of modified models before-and-after your latest changes.</p>

<h2 id="get-started-with-piperider">Get started with PipeRider</h2>

<p>As I mentioned above, if you’re a dbt user then PipeRider should be your go-to data reliability tool.  If you already have a dbt project, all you need to do is <a href="https://docs.piperider.io/cli/dbt-integration">initialize PipeRider inside the project</a> and your data source settings will be automatically detected:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># inside your dbt project folder</span>
piperider init
</code></pre></div></div>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider now supports dbt state! You can profile and run data reliability tests on only the tables that have been modified as part of your dbt state]]></summary></entry><entry><title type="html">5 Database Schema Changes Data Engineers Need to Beware Of</title><link href="https://blog.piperider.io/database-schema-changes-to-beware-of.html" rel="alternate" type="text/html" title="5 Database Schema Changes Data Engineers Need to Beware Of" /><published>2022-11-21T00:00:00+08:00</published><updated>2022-11-21T00:00:00+08:00</updated><id>https://blog.piperider.io/database-schema-changes-to-beware-of</id><content type="html" xml:base="https://blog.piperider.io/database-schema-changes-to-beware-of.html"><![CDATA[<p>As a data or analytics engineer you need to anticipate the unexpected, especially when building pipelines from data sources that you have no control over. While a data pipeline might be a house of cards, with data observability you’re able to keep an eye on any changes to the underlying structure, and be prepared for schema changes.</p>

<p>Structure is the key point here, as one of the main ways that a data source can change is the schema, or structure, of a database table. The schema is important because the pipelines you build may have specific downstream uses that are directly tied to the current structure of the data. By staying ahead of schema changes you’ll know about potential issues before they affect the data and downstream uses.</p>

<p>As Angela, a data engineer at Sam’s Club, put it in a <a href="https://twitter.com/i/spaces/1OwGWwZAZDnGQ?s=20">recent interview</a>:</p>

<blockquote>
  <p>“you never want your end users to actually contact you, (and if they do) you want to be already in the know”</p>
</blockquote>

<p>Here are 5 schema changes to look out for, and why you should beware of them:</p>

<h2 id="1-additional-columns">1. Additional columns</h2>

<p>Additional columns might seem like one of the more benign changes at first. After all, how could adding some columns to a database affect your pipeline?</p>

<h3 id="why-you-should-beware">Why you should beware</h3>

<p>After a new column is added, there’s no easy way to know if the data was backfilled, without manually checking. That is, for each row, was the data retroactively added? If not, there may be null, zero, or empty values, and you won’t know if they represent valid data or not. This could be problematic if you start using the new column without first knowing the backfill status.</p>

<h2 id="2-column-type-and-attribute-changes">2. Column type and attribute changes</h2>

<p>Column types are important because they enforce the type of data contained in the column. The way you use the data of a column is based on the expectation that the data is of a certain type. Examples of column type changes include:</p>

<ul>
  <li>A numeric column changing to string, and vice-versa.</li>
  <li>A numeric type change, such as a decimal changing to an integer.</li>
  <li>Date is a tricky one — is it Date, Datetime, or Timestamp?</li>
</ul>

<h3 id="why-you-should-beware-1">Why you should beware</h3>

<p>If you’re expecting a float and get an integer, this could affect the accuracy of any calculations based on the data, especially if you require a certain precision (see column attributes).</p>

<p>Sometimes, column types might not even be enforced, so you might run into problems with mixing types, such as storing text and numbers in the same column. This means you need to be very careful about checking the format of the data.</p>

<h2 id="3-column-attributes-and-constraints">3. Column attributes and constraints</h2>

<p>Column attributes refer to the format of the stored data. Things like:</p>

<ul>
  <li>String length</li>
  <li>Size of an integer</li>
  <li>Decimal precision</li>
  <li>Date format</li>
  <li>Default values</li>
  <li>If NULL values are allowed</li>
</ul>

<h3 id="why-you-should-beware-2">Why you should beware</h3>

<p>Changes to numeric types such as integer size, or precision, could affect downstream calculations.</p>

<p>The format of the date could also wreak havoc on your dashboard — is it DD-MM-YYYY, or MM-DD-YYYY? What time zone is this for? These are important questions to ask if you’re building a dashboard that relies on time-specific data.</p>

<p>If the limit on the length of strings changes, it could result in truncated data. An issue that might go unnoticed if not manually checked for.</p>

<h2 id="4-column-renaming">4. Column renaming</h2>

<p>Renaming a column is a big change. If tables are being used in production you’d hope to be notified of a change such as this, but with third party data sources that might not happen. This one could really get you if you’re not paying attention.</p>

<h3 id="why-you-should-beware-3">Why you should beware</h3>

<p>Your transformations and data modeling queries rely heavily on column naming. One column name-change could result in a broken pipeline causing data to stop flowing downstream.</p>

<h2 id="5-column-misuse">5. Column misuse</h2>

<p>Column misuse is more of a data quality issue than a schema change but, like a schema change, could cause issues for you when using the data. When a column is repurposed, it’s used for a purpose it wasn’t originally intended for. If there’s no data dictionary, or documentation is not kept up-to-date, then you could easily be caught out by this one.</p>

<h3 id="why-you-should-beware-4">Why you should beware</h3>

<p>One example of column misuse is putting additional data into a column, instead of creating another column. For instance, a product sales table might start including product attributes, such as size and color, in the description column, instead of creating new columns for the attributes.</p>

<p>The meaning of a column might also change over time and, instead of adjusting the database accordingly, an existing column is ‘repurposed’ for use. Without you knowing, the column now includes completely different data than what you expect.</p>

<h2 id="how-to-avoid-being-caught-out">How to avoid being caught out</h2>

<p>As mentioned in the introduction, it’s all a matter of having the knowledge that a change has taken place — As the saying goes “<a href="https://knowyourmeme.com/memes/france-is-bacon">knowledge is power, France is bacon</a>”. If you know a change has taken place, you can react and make sure that data pipeline continues to work, before downstream issues occur.</p>

<p>For data teams, that knowledge comes from <a href="https://blog.infuseai.io/data-monitoring-be-the-master-of-your-pipeline-ba464b66888">data monitoring and observability</a>. With proper data observability in place you can know the current structure of the database, and how that compares to previous states.</p>

<p>Highlighting that schema changes have taken place, along with other data changes, is one of the things <a href="https://blog.infuseai.io/how-to-be-more-confident-making-data-model-changes-76a2f65feffa">we’re working on</a> with <a href="https://github.com/infuseai/piperider">PipeRider</a>.</p>

<h2 id="your-experiences">Your experiences</h2>

<p>Have you been bitten by schema change? Let me know what happened and how you discovered the change.</p>

<p>If I missed any important or obvious schema changes to look out for, please tell me and I’ll improve the list.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[As a data or analytics engineer you need to anticipate the unexpected, especially when building pipelines from data sources that you have no control over. While a data pipeline might be a house of cards, with data observability you’re able to keep an eye on any changes to the underlying structure, and be prepared for schema changes.]]></summary></entry><entry><title type="html">Guide to Data Reliability</title><link href="https://blog.piperider.io/guide-to-data-reliability.html" rel="alternate" type="text/html" title="Guide to Data Reliability" /><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>https://blog.piperider.io/guide-to-data-reliability</id><content type="html" xml:base="https://blog.piperider.io/guide-to-data-reliability.html"><![CDATA[<p>Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.</p>

<p>You may be collecting a large amount of information within your data stack, but are you certain that these data sets are complete, accurate, and up-to-date? If not, these data sets might cost you a lot.</p>

<p>IBM estimated that the yearly cost of poor quality data, in the US alone, in 2016, is a whopping $3.1 trillion. In 2021, Gartner reported that every year, unreliable data costs organizations an average of $12.9 million. And it’s safe to say that the number has very likely increased as data-driven decision-making is adopted by every business imaginable.</p>

<p>That’s why ensuring your data is trustworthy by improving data reliability is very important.</p>

<h2 id="what-is-data-reliability">What is Data Reliability?</h2>

<p>Data reliability means that data is complete, accurate, and valid. It’s the foundation for building trust in your data across the organization. One of the main objectives of ensuring data reliability is building data trust, which is also used to maintain data security, data quality, and regulatory compliance.</p>

<p>Reliable data helps decision-makers take the guesswork out of the daily and strategic decision-making process to keep their organizations running. But if your data is unreliable, those same decisions become less accurate and can ultimately affect your organization.</p>

<h2 id="why-data-reliability">Why Data Reliability</h2>

<p>When unreliable data is used in making a key strategic decision, it can result in a mistake that damages an organization’s reputation, and bottom line, or even causes its future. Data reliability issues might not seem like a big deal at first glance, but they can snowball over time if left unchecked.</p>

<p>For example, you use customer data to develop targeted online ads or recommend products to your consumers. If the data you use isn’t accurate, then there’s a good chance that the advertising budget will be wasted on either poor results or zero return on investment.</p>

<p>The unsettling feeling when you are not sure if you can trust your data to make a decision can be highly stressful, ut there are actions you can take to improve your data reliability.</p>

<h2 id="how-to-improve-data-reliability">How to Improve Data Reliability</h2>

<p>Like many other managerial tasks, the process to improve your data reliability follows a series of logical steps. There are eight action items that your organization can take to improve your data reliability:</p>

<ol>
  <li>Assess Data Status</li>
  <li>Build Data Infrastructure</li>
  <li>Clean Existing Data</li>
  <li>Optimize Data Collection Processes</li>
  <li>Break Down All Data Silos</li>
  <li>Integrate Data Stack to Connect Data</li>
  <li>Organize Your Data</li>
  <li>Use Reports and Dashboards</li>
</ol>

<h3 id="assess-data-status">Assess Data Status</h3>

<p>Assessing your current data status is the first thing to do to improve data reliability. It helps you to get a general view of how your organization treats data. You should also employ data profiling. Data profiling is the process of examining and analyzing data. This helps you understand if your data is healthy. Assess your current situation to understand:</p>

<p>What are your data sources;
How and where you have stored the data;
How and where you have used the data;
The criteria used to determine data reliability.</p>

<h3 id="build-data-infrastructure">Build Data Infrastructure</h3>

<p>Once you’ve assessed your current situation, you can start updating your data infrastructure. No matter what the original data sources are, you need a secure and easy-to-use data repository. You need to define how your data will be stored, formatted, and organized. There are several steps you can take to create a data infrastructure:</p>

<p>Refine your strategy.
Build a data model.
Choose your data repository type – data lake, data warehouse, or hybrid.
Build an extract, transform, and load (ETL) process.
Implement ongoing data governance.</p>

<h3 id="clean-existing-data">Clean Existing Data</h3>

<p>If you have data sets in place already, you should examine the existing data and remove data that is:</p>

<ul>
  <li>inaccurate;</li>
  <li>incomplete;</li>
  <li>duplicative;</li>
  <li>outdated;</li>
  <li>incorrectly formatted.</li>
</ul>

<p>You should employ data profiling to analyze your data continually, so you can clean, and update data errors as soon as they are spotted.</p>

<h3 id="optimize-data-collection-processes">Optimize Data Collection Processes</h3>

<p>Start by analyzing internal processes for data input. Automate data entry wherever possible to minimize human errors. Make sure that all data entry follows your standardized formats and is accurate, complete, and valid.</p>

<p>Next, look at other data sources you obtain new data from. Make sure that their data formats follow your standardized format and remove inaccurate and unreliable data.</p>

<h3 id="break-down-all-data-silos">Break Down All Data Silos</h3>

<p>Organizations collect data from different departments or locations. This is necessary due to operational requirements or structure setup. But this might create independent data silos that would affect data reliability.</p>

<p>Not only do silos make it difficult to find and share data across your organization, but they also often adhere to different standards of organization and quality.</p>

<p>To ensure the most reliable data is available to those who need it internally, you need to break down your organization’s data silos. You should employ a central data repository for all departments and locations to minimize potential damage to data quality.</p>

<h3 id="integrate-data-stack-to-connect-data">Integrate Data Stack to Connect Data</h3>

<p>Quite often, different departments or locations use various tools and platforms. If you can get everyone to use the same tool and platform, great. If not, you should connect data from these tools and platforms across your entire organization to have a unified view of all your data. Therefore, when a piece of data is updated in one location, it is automatically updated wherever else it is used.</p>

<h3 id="organize-your-data">Organize Your Data</h3>
<p>Every organization has its unique way of organizing data to meet its unique needs. Organizing data makes it easier to locate specific data and speeds up your data retrieval process.</p>

<p>Typically, you will find labels, tags, groups, and other information stored in metadata. Depending on the type and use of your data, you may find data segmented by customer age, gender, geographic location, demographics, etc. No matter how you organize your data, make sure you understand the overall organization’s expectations and what it would like to achieve using the data.</p>

<h3 id="use-reports-and-dashboards">Use Reports and Dashboards</h3>

<p>Finally, make sure you are able to get insight from your data with reports and dashboards. For example, a data profile report can continue to alert you of data errors when it occurs. Other reports that track key metrics in a visual way with detailed analyses put peace of mind in you when it comes to making data-driven decisions.</p>

<h2 id="automate-your-data-reliability-with-piperider">Automate Your Data Reliability With PipeRider</h2>

<p>It may feel overwhelming when you manage a large amount of data, but once you lay the groundwork and build the foundation, there are many tools you can use to make the journey easier. If you’re interested in learning more about your data, with the aim of improving your data reliability, PipeRider can help you.</p>

<p>PipeRider is an open-source, free, and easy-to-use data reliability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements, enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.]]></summary></entry><entry><title type="html">Data Observability Explained and How to Integrate It into Your Workflow</title><link href="https://blog.piperider.io/guide-to-data-observability.html" rel="alternate" type="text/html" title="Data Observability Explained and How to Integrate It into Your Workflow" /><published>2022-09-23T00:00:00+08:00</published><updated>2022-09-23T00:00:00+08:00</updated><id>https://blog.piperider.io/guide-to-data-observability</id><content type="html" xml:base="https://blog.piperider.io/guide-to-data-observability.html"><![CDATA[<p>The number of data sources that data teams have to deal with is ever increasing. According to a <a href="https://www.matillion.com/resources/blog/matillion-and-idg-survey-data-growth-is-real-and-3-other-key-findings">recent survey</a> by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.</p>

<h2 id="what-is-data-observability">What is Data Observability?</h2>

<p>Data observability is the capability to comprehend, assess, and manage the state of data consumed by various technologies throughout the entire data lifecycle.</p>

<p>With data observability, your team can have a better understanding of your data. So they can gather consistent, standardized data from APIs, support data lake observability, facilitate routine queries to data warehouses, and share high-quality data across the entire organization.</p>

<h2 id="why-data-observability">Why Data Observability?</h2>

<p>One of the benefits of data observability is that teams can monitor data pipelines and quickly identify data issues with end-to-end data visibility.</p>

<p>Before data observability, teams might struggle with various data issues such as outdated data, broken data pipelines, or missing data. These issues might be caused by uncertainty in data standards or different data models from different data providers.</p>

<p>With data observability, your team can</p>

<ul>
  <li>standardize data for monitoring;</li>
  <li>debug and triage proactively;</li>
  <li>understand how data interacts with different tools;</li>
  <li>identify issues early;</li>
  <li>minimize the negative impact of data issues.</li>
</ul>

<p>Data observability also makes it possible for your team to automate parts of your monitoring process to constantly improve data quality with less time spent.</p>

<h2 id="what-does-data-observability-track">What Does Data Observability Track?</h2>

<p>Data profiling is an essential part of data observability. Through the following data profiling techniques, you can further understand your data and apply checks that will alert you to issues with your data.</p>

<p>Row-level validation and column-level profiling provide information about the system-wide performance of your data.
Anomaly detection helps spot problems before they damage data quality.
A statistics summary provides an in-depth understanding of the elements of your data observability framework.
Execution metadata and delays analysis throughout data pipelines to prevent data downtime.</p>

<p>These observability techniques should give you a comprehensive insight into the overall data health, potential data issues, and the quality of your data.</p>

<p>Incorporate Data Observability into Work to Improve Data Quality (h2)
According to research, one-third of data analysts spend more than 40 percent of their time on standardizing data to make it ready for analysis, and 57 percent of organizations still regard the “work of transforming their data to be very difficult.” It is obvious that ensuring consistent and accurate data can be a difficult and expensive task for organizations.</p>

<p>Therefore, having proper and solid data observability set up not only saves time but also a lot of resources, including money - But how do you incorporate data observability into your data quality workflow? You should start by developing a framework, then a strategy, and based on these two, choose the right tool for data observability.</p>

<h2 id="how-to-develop-a-data-observability-framework">How to Develop a Data Observability Framework</h2>

<p>Start your data observability journey by creating an efficient data-driven framework focusing on data quality, consistency, and reliability.</p>

<p>A data observability framework should answer the following questions:</p>

<ul>
  <li>How fresh and up-to-date is our data?</li>
  <li>What expected data value should we verify to ensure credible data?</li>
  <li>What data do we need to track and test to see when the data is broken?</li>
  <li>What is the responsibility of each team to various data sets?</li>
  <li>What other workflow, such as gathering metadata, or mapping upstream data sources and downstream users, do we need?</li>
</ul>

<p>The framework should give your team an overall view of standardized data across the organization, letting them quickly identify and fix problems.</p>

<h2 id="how-to-develop-a-data-observability-strategy">How to Develop a Data Observability Strategy</h2>

<p>Once you have a framework in place, many teams may jump right into integrating data observability into the entire data stack. But putting data observability into practice goes beyond the tools you employ.</p>

<p>You should start with preparing your team to adopt a culture of data-driven collaboration. Think about how to integrate data across different teams and sources, and also consider if implementing a new observability tool will affect existing workflow and resources.</p>

<p>Then incorporate the framework into your strategy to determine a standardized library/guidelines with the characteristics of quality data. Your team can use the guidelines to connect data from all sources.</p>

<p>Finally, incorporate your data sources into the observability tool. To obtain the metrics, logs, and traces required to provide end-to-end visibility, you might need to create new observability pipelines. Correlate the metrics you are tracking in your tool with targeted organization goals after adding the governance and data management rules. By using your observability tool to identify and address problems, you can also find new ways to automate some of your data management processes.</p>

<h2 id="how-to-choose-the-right-data-observability-tool">How to Choose the Right Data Observability Tool</h2>

<p>While there’s no one tool to fit every organization’s needs, a good observability tool should be able to:</p>

<ul>
  <li>gather, examine, sample, and process telemetry data from various data sources;</li>
  <li>detect and alert problems in datasets;</li>
  <li>provide end-to-end visibility;</li>
  <li>display data visualizations.</li>
</ul>

<p>In order to choose a suitable data observability tool, you’ll need to examine your current data stack and get a full picture of how data is gathered and distributed. Then you can look into a tool that is ready to integrate all of your data sources. Your chosen tool should be able to monitor your data in real-time throughout its lifecycle and monitor existing data without extraction. In addition, it should also be able to automate your data observability with minimum effort.</p>

<p>Ultimately, your organization’s specific data stack and data engineering requirements will determine the right tool for you. For the best implementation experience, give top priority to finding a tool that requires less work to standardize your data, map your data, or monitor your data.</p>

<h2 id="integrate-data-observability-with-piperider">Integrate Data Observability with PipeRider</h2>

<p>PipeRider is an open-source, free, and easy-to-use data observability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The number of data sources that data teams have to deal with is ever increasing. According to a recent survey by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.]]></summary></entry><entry><title type="html">Adding Data Observability and Alerts to your Data Pipeline is easier than you think</title><link href="https://blog.piperider.io/adding-data-observability-and-alerts-to-your-data-pipeline-is-easier-than-you-think.html" rel="alternate" type="text/html" title="Adding Data Observability and Alerts to your Data Pipeline is easier than you think" /><published>2022-09-05T00:00:00+08:00</published><updated>2022-09-05T00:00:00+08:00</updated><id>https://blog.piperider.io/adding-data-observability-and-alerts-to-your-data-pipeline-is-easier-than-you-think</id><content type="html" xml:base="https://blog.piperider.io/adding-data-observability-and-alerts-to-your-data-pipeline-is-easier-than-you-think.html"><![CDATA[<p>After you’ve transformed the data in your data warehouse and sent it on its way, you might think that your job is done. That is, until you get a call that there’s missing data, an unexpected schema change, or some unexpected data or outlier has been introduced. To understand when these issues might have occurred, you need some form of data observability for your pipeline.</p>

<p>Data observability means that you can monitor the data moving through the pipeline, be alerted to any changes or issues with the data structure, and compare data to help visualize change and aid in tracking down issues.</p>

<hr />

<p>With the open-source data observability toolkit, <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a>, you can add data observability to your data source and start understanding more about your data in minutes with:</p>

<p>Non-intrusive implementation — Focus on understanding your data without changing it
Data profiling — In-depth analysis of the structure of your data source
Data assertions — Ensure your data stays within acceptable ranges through testing
Reporting — The data profile and testing results are exported to an HTML report
Following are the steps you need to get starting adding data observability and data assertions to your existing data pipeline.</p>

<h2 id="1-install-piperider">1. Install PipeRider</h2>
<p>PipeRider is installed via pip:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -U piperider
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/vZVHo09fD-c" frameborder="0" allowfullscreen=""></iframe></div>

<p>By default it comes with SQLite, but the following connectors are also available:</p>

<ul>
  <li>Postgres</li>
  <li>Snowflake</li>
  <li>BigQuery</li>
  <li>Redshift</li>
  <li>dbt (with one of the supported connectors)</li>
  <li>duckdb</li>
  <li>CSV</li>
  <li>Parquet</li>
</ul>

<p>Install PipeRider with a connector like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -U 'piperider[postgres,snowflake]'
</code></pre></div></div>
<h2>　</h2>
<h2 id="2-initialize-a-piperider-project">2. Initialize a PipeRider project</h2>
<p>Once installed, initialize a new project with the following command.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider init
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/jRxZQJoMQGc" frameborder="0" allowfullscreen=""></iframe></div>

<p>Just select your data source, enter the relevant details and you’re ready to go. dbt project settings will be auto-detected, so dbt projects really are zero config!</p>

<p>Verify your connection settings with the diagnose command.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider diagnose
</code></pre></div></div>
<h2 id="-1">　</h2>
<h2 id="3-run-piperider">3. Run PipeRider</h2>
<p>With a data source connected you’re ready to run PipeRider.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider run
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/Y9KIqu2DOsg" frameborder="0" allowfullscreen=""></iframe></div>

<p>This one command will do the following:</p>

<ul>
  <li>Profile your data source</li>
  <li>Generate recommended data assertions (on first run)</li>
  <li>Test the data profile against the data assertions</li>
  <li>Display the data assertion test results on the CLI</li>
  <li>Generate an HTML report with the data profile and test results</li>
</ul>

<h2 id="-2">　</h2>
<h2 id="4-test-your-piperider-data-profile-with-data-assertions">4. Test your PipeRider data profile with data assertions</h2>
<p>PipeRider creates a set of recommended assertions based on the current state of your data. You can add to or edit these using the available suite of <a href="https://docs.piperider.io/cli/data-quality-assertions/assertion-configuration">built-in assertions</a>, and through custom assertions you can <a href="https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider.html">create your own data reliability tests</a>.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/t4YHkynRIpI" frameborder="0" allowfullscreen=""></iframe></div>

<h2 id="-3">　</h2>
<h2 id="5-compare-data-profile-reports">5. Compare data profile reports</h2>
<p>When your data changes and you have multiple PipeRider runs, compare reports easily with the following command.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider compare-reports
</code></pre></div></div>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/COohk2TAiPA" frameborder="0" allowfullscreen=""></iframe></div>

<p>You can also compare the last two reports automatically (without needing to manually select them) by using the <code class="language-plaintext highlighter-rouge">--last</code> flag.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider compare-reports —last
</code></pre></div></div>

<h2 id="-4">　</h2>
<h2 id="sample-reports">Sample Reports</h2>
<p>Links to example reports will always be available in the PipeRider documentation. Here are samples created with PipeRider 0.7:</p>

<p><a href="https://piperider-github-readme.s3.ap-northeast-1.amazonaws.com/run-0.7.0/index.html#/">Sample PipeRider data profile report</a>
<a href="https://piperider-github-readme.s3.ap-northeast-1.amazonaws.com/comparison-0.7.0/index.html">Sample PipeRider report comparison</a></p>

<h2 id="-5">　</h2>
<hr />
<h2 id="who-makes-piperider">Who makes PipeRider?</h2>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is developed by <a href="https://infuseai.io/">InfuseAI</a>, the company behind the end-to-end machine learning platform <a href="https://www.infuseai.io/primehub-ai-platform">PrimeHub</a>.</p>

<p>InfuseAI has an impressive <a href="https://github.com/InfuseAI/">portfolio of open-source projects</a> so you know you’re in good hands!</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[After you’ve transformed the data in your data warehouse and sent it on its way, you might think that your job is done. That is, until you get a call that there’s missing data, an unexpected schema change, or some unexpected data or outlier has been introduced. To understand when these issues might have occurred, you need some form of data observability for your pipeline.]]></summary></entry><entry><title type="html">Define your own Data Quality Tests in PipeRider</title><link href="https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider.html" rel="alternate" type="text/html" title="Define your own Data Quality Tests in PipeRider" /><published>2022-09-02T00:00:00+08:00</published><updated>2022-09-02T00:00:00+08:00</updated><id>https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider</id><content type="html" xml:base="https://blog.piperider.io/define-your-own-data-quality-tests-in-piperider.html"><![CDATA[<p><strong>tl;dr</strong> <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> comes with a built-in suite of data quality assertions, but you can also define your own assertions to meet your needs. This guide will show you how to implement a custom assertion to allow a specific number of nulls without raising an alert.</p>

<hr />

<p>You don’t always have the luxury of a complete dataset, and that’s not always a bad thing — there are some cases in which a certain amount of missing data, or null values, can be acceptable in a data source.</p>

<p>Sometimes a certain amount of NULL values is OK</p>

<h2 id="missing-data-no-problem">Missing data? No problem</h2>
<p>Let’s say you have some incomplete data in a sales database and, even so, you still need to perform some calculations based on the data. You might be willing to allow a certain percentage of missing values before worrying about it affecting your results.</p>

<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> has <a href="https://docs.piperider.io/cli/data-quality-assertions/assertion-configuration">built-in assertions</a> for testing if a column does not contain nulls, or that a it must be null, but there is no middle-ground for allowing for a certain quantity of nulls — custom assertions to the rescue!</p>

<p><img src="/img/posts/220902-1.webp" alt="" /></p>

<p><em>For the rest of this article, I’ll assume you already have a PipeRider project set up — If not, it’s really easy to do, the <a href="https://docs.piperider.io/cli/quick-start">Quick Start</a> guide will have you up and running in minutes.</em></p>

<h2 id="from-data-profile-to-data-assertions">From data profile to data assertions</h2>
<p>Each time you run PipeRider it creates a <a href="https://docs.piperider.io/data-profile-and-metrics/data-profile">profile of your data</a>, and then by applying data assertions you can test the contents of the profile to ensure the data meets your structural requirements.</p>

<p>Using the missing data example from above, we’ll create a custom assertion that checks the number of nulls in a column, and raises an alert if there are more than our specified amount.</p>

<h2 id="custom-column-assertion-template">Custom Column-Assertion Template</h2>
<p>Here’s the template we’ll be using to create our custom column-assertion, Steps 1 to 5 in the code-comments are what we’ll be working on:</p>

<script src="https://gist.github.com/DaveFlynn/7f26ee7cdecedef332d4b5601dec721f.js"></script>

<h2 id="create-your-custom-column-assertion">Create your custom column-assertion</h2>
<p>In your PipeRider project, open <code class="language-plaintext highlighter-rouge">.piperider/plugins/customized_assertions.py</code> in your favorite text editor and paste the above template at the bottom, above where it says <code class="language-plaintext highlighter-rouge"># register new assertions</code></p>

<p>Give the assertion a name. Here, I’ve named the class <code class="language-plaintext highlighter-rouge">AssertNullCount</code> and the name of <code class="language-plaintext highlighter-rouge">assert_null_count</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class AssertNullCount<span class="o">(</span>BaseAssertionType<span class="o">)</span>:
  def name<span class="o">(</span>self<span class="o">)</span>:
    <span class="k">return</span> <span class="s2">"assert_null_count"</span>
</code></pre></div></div>

<p>Now, I’ll run you through the commented steps 1 - 5 in the assertion template.</p>

<h3 id="1-get-the-desired-metric">1: Get the desired metric</h3>
<p>The first thing we need to do is get the value of the desired metric we want to test. For this assertion, we’ll be testing the number of <strong>nulls</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 1. Get the metric for the desired column, e.g. num of nulls in column
nulls = column_metrics.get('nulls')
</code></pre></div></div>

<p>For a full list of available metrics, check the <a href="https://docs.piperider.io/data-profile-and-metrics/data-profile">Data Profile</a> page in the PipeRider documentation.</p>

<h3 id="2-get-the-expected-value-from-the-assertion-file">2: Get the expected value from the assertion file</h3>
<p>You could hard-code the the number of allowed nulls, but then we couldn’t re-use the assertion. Luckily, it’s possible to pass a value from the assertion YAML file into the custom assertion.</p>

<p>Here we grab the value of <code class="language-plaintext highlighter-rouge">allowed_null</code>s`. I’ll show you how to define the value of this variable in your assertion YAML later on.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 2. Get expectation from assertion file
allowed_nulls = context.asserts.get('allowed_nulls', [])
</code></pre></div></div>

<h3 id="34-implement-logic-to-test-the-metric-and-either-pass-or-fail">3,4: Implement logic to test the metric and either pass or fail</h3>
<p>After comparing the value of nulls with the value of allowed_nulls, we return a pass result if nulls is less than or equal to our threshold, and a fail otherwise.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 3. Implement logic to compare expected and actual metric value
if (nulls &lt;= allowed_nulls):
	# 4. return pass (acceptable number of nulls)
	return context.result.success('There are {} null values'.format(nulls))
else:
	# 4. or return fail (more nulls than we expected)
	return context.result.fail(nulls)
</code></pre></div></div>

<h3 id="5-register-the-new-assertion">5: Register the new assertion</h3>
<p>Now that you’ve created the assertion, register it with PipeRider. <strong>This goes outside of your new class</strong>, at the bottom of the <code class="language-plaintext highlighter-rouge">customized_assertions.py</code> file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#register new assertion
register_assertion_function(AssertNullCount)
</code></pre></div></div>

<h2 id="completed-assertion-class">Completed assertion class</h2>
<p>The completed custom assertion class should now look like this:</p>

<script src="https://gist.github.com/DaveFlynn/37b9f92e2784187566a6b4f8d3101250.js"></script>

<h2 id="put-your-custom-assertion-into-action">Put your custom assertion into action</h2>
<p>Open the assertions file for the table you want to run the new assertion on. You can find the assertion YAML files in <code class="language-plaintext highlighter-rouge">.piperider/assertions/&lt;table&gt;.yml</code>. If you generated <a href="https://docs.piperider.io/cli/quick-start#generate-data-assertions">recommended assertions</a> then your table assertions file should already be populated.</p>

<p>Find the desired column and apply your custom assertion. In example below, I have applied the new assertion to the Global Sales column:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Global_sales: # Column Name
  description: 'Combined global sales figures'
  # Test Cases for Column
  tests:
  - name: assert_allowed_nulls
    assert:
        allowed_nulls: 5
</code></pre></div></div>

<p>The last line, <code class="language-plaintext highlighter-rouge">allowed_nulls</code>, is the value we pass through to the custom assertion and check the actual number of nulls against.</p>

<h2 id="run-piperider-to-check-that-it-works">Run PipeRider to check that it works</h2>
<p>The next time you run PipeRider your custom assertion will be tested.</p>

<p><strong>Passed assertion</strong>
If it passes, you’ll see the test with the status <code class="language-plaintext highlighter-rouge">[OK]</code>.</p>

<p>all.Global_sales assertion passed with 5 nulls out of 10 allowed
<img src="/img/posts/220902-2.webp" alt="all.Global_sales assertion passed with 5 nulls out of 10 allowed" /></p>

<p><strong>Failed assertion</strong>
If it fails you’ll see it in the list of <strong>failed assertions</strong>.</p>

<p>all.Global_sales failed with 15 nulls — 5 more than the 10 allowed
<img src="/img/posts/220902-3.webp" alt="all.Global_sales failed with 15 nulls — 5 more than the 10 allowed" /></p>

<h2 id="take-it-a-step-further">Take it a step further</h2>
<p>Specifying an exact number of allowed nulls might not be an ideal solution for all datasets. For instance, when dealing with large datasets, you might prefer to allow a certain percentage of nulls. In that case, the code we wrote only needs a small adjustment.</p>

<p>All you need to do is get the total number of rows for the table; the number of nulls; then work out the percentage of nulls and test if the percentage exceeds your threshold of allowed nulls.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># get total number of rows
total = column_metrics.get('total')

# get the number of nulls
nulls = column_metrics.get('nulls')

#work out the percentage of nulls
percent_nulls = round(((100 / total) * nulls), 2)

# get the number of allowed nulls from your assertion definition
allowed_nulls = context.asserts.get('allowed_nulls')
if (nulls &gt; percent_nulls * allowed_nulls): 
    # too many nulls
    return context.result.fail('{}%'.format(percent_nulls))
else: 
    # acceptable percentage of nulls
    return context.result.success('There are {}% nulls'.format(percent_nulls))
</code></pre></div></div>

<p>You’re only limited by the metrics available in the data profile, and your ability to write Python, so check the <a href="https://docs.piperider.io/">PipeRider documentation</a> and <strong>start making your data more reliable today</strong>!</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider comes with a built-in suite of data quality assertions, but you can also define your own assertions to meet your needs. This guide will show you how to implement a custom assertion to allow a specific number of nulls without raising an alert.]]></summary></entry><entry><title type="html">Spotlight on Data Reliability</title><link href="https://blog.piperider.io/spotlight-on-data-reliability.html" rel="alternate" type="text/html" title="Spotlight on Data Reliability" /><published>2022-08-23T00:00:00+08:00</published><updated>2022-08-23T00:00:00+08:00</updated><id>https://blog.piperider.io/spotlight-on-data-reliability</id><content type="html" xml:base="https://blog.piperider.io/spotlight-on-data-reliability.html"><![CDATA[<p><strong>tl;dr</strong> Open-Source Spotlight welcomed us back to to demo <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a>, the open-source data reliability toolkit. In the video, CL Kao, InfuseAI’s CEO, shows how it’s possible to profile and test a data set, and then generate a visualization report with just a few commands.</p>
<hr />

<p>Data quality has a new hero!</p>

<p>We’ve been on Open-Source Spotlight a <a href="https://www.youtube.com/watch?v=Q3aMaxBH1-o">few</a> <a href="https://www.youtube.com/watch?v=2ebo1H0XgEs">times</a> <a href="https://www.youtube.com/watch?v=akhj7or9gmQ">already</a>, and this time CL Kao, the CEO of InfuseAI, was invited to demonstrate <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a>.</p>

<p>PipeRider is your open-source data reliability toolkit for adding data profiling and data quality assertions with visualization reports to your data warehouse.</p>

<h2 id="break-it-down">Break it down</h2>
<p>That’s quite a mouthful of features, so let’s break down what PipeRider can do (I’ve linked to the relevant documentation pages):</p>

<ul>
  <li>Data profiling: PipeRider will profile your data source, providing insightful metrics about the structure of your data.</li>
  <li>Data Assertions: Use PipeRider’s built-in data assertion suite to test the structure and contents of your data profile. You’ll be alerted when your data assertions fail.</li>
  <li>Visualization Report: From the data profile, PipeRider will create an HTML report that shows the structure of your data source, including test results.</li>
  <li>Report Comparison: Compare past data profiling reports to see how your data source has changed between runs.</li>
  <li>Supported Data Warehouses: SQLite, Postgres, Snowflake, and dbt are supported (PipeRider v0.6), with BigQuery and Redshift support just days away (PipeRider v0.7). And you can find more data source on <a href="https://docs.piperider.io/cli/supported-data-sources">Supported Data Sources documentation</a>.</li>
</ul>

<h2 id="watch-the-demo">Watch the demo</h2>

<p>Watch the video to see PipeRider in action:</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/03MyOkIo8Hg" frameborder="0" allowfullscreen=""></iframe></div>

<p><em><a href="https://www.youtube.com/playlist?list=PL3MmuxUbc_hJ5t5nnjzC0F2zan76Dpsz0">Open Source Spotlight</a>, the video series by the <a href="http://datatalks.club/">DataTalks.club</a> community continues to give back to the open-source community by providing a platform for projects to demo what they’re working on. Definitely check out the <a href="https://www.youtube.com/datatalksclub">YouTube channel</a> if you’re involved in data and/or machine learning.</em></p>

<h2 id="how-to-get-started-with-data-reliability">How to get started with data reliability</h2>
<p>PipeRider is available now. It’s open-source, free to use, and well documented. As of writing, PipeRider is about to release version 0.7. The version demoed in the video above is 0.3, so make sure to check the websites below for up-to-date information:</p>

<ul>
  <li><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">Official PipeRider website</a></li>
  <li><a href="https://github.com/InfuseAI/piperider">GitHub Repo</a></li>
  <li><a href="https://docs.piperider.io/">Documentation</a></li>
</ul>

<h2 id="we-need-your-feedback">We need your feedback</h2>
<p>After you’ve used PipeRider, tell us what you think. Good or bad, we want to know. Get in touch in one of the following ways:</p>

<ul>
  <li>Twitter<a href="https://twitter.com/infuseai">@infuseai</a></li>
  <li><a href="https://discord.gg/xKxsdPx4d5">Discord Community</a></li>
  <li><a href="https://github.com/InfuseAI/piperider/issues">GitHub Issue</a></li>
  <li>Mastodon <a href="https://fosstodon.org/@piperider">@piperider@fosstondon.org</a></li>
</ul>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr Open-Source Spotlight welcomed us back to to demo PipeRider, the open-source data reliability toolkit. In the video, CL Kao, InfuseAI’s CEO, shows how it’s possible to profile and test a data set, and then generate a visualization report with just a few commands.]]></summary></entry><entry><title type="html">Transfer the CSV Files into an SQLite Database</title><link href="https://blog.piperider.io/transfer-the-csv-files-into-an-sqlite-database.html" rel="alternate" type="text/html" title="Transfer the CSV Files into an SQLite Database" /><published>2022-08-04T00:00:00+08:00</published><updated>2022-08-04T00:00:00+08:00</updated><id>https://blog.piperider.io/transfer-the-csv-files-into-an-sqlite-database</id><content type="html" xml:base="https://blog.piperider.io/transfer-the-csv-files-into-an-sqlite-database.html"><![CDATA[<p>Datasets are readily available as CSV files on websites such as Kaggle and other real-world datasets. However, CSV isn’t really suitable for querying and transforming easily. Therefore, it’s better to store the data in a database.</p>

<p>In this article, you will use csvs-to-sqlite to convert CSV files into SQLite databases. Once the data is in a database, it’s easier for users to query with SQL and add to their ETL service.</p>

<p>We have to get the dataset first, here is an example of the <a href="https://github.com/ldkrsi/cpbl-opendata">CPBL dataset</a>.</p>

<hr />

<h2 id="convert-csv-to-sqlite">Convert CSV to SQLite</h2>

<p>“<a href="https://github.com/simonw/csvs-to-sqlite">csvs-to-sqlite</a>” tool is an open-source project and put it in the Github. We can easily use the pip install method to install the tools.</p>

<p>You can use the following commands to download the data and install the <a href="https://pypi.org/project/csvs-to-sqlite/">python package</a>.</p>

<script src="https://gist.github.com/LiuYuWei/a9ec256303f6c63b5958a4b12d03e625.js"></script>

<p>After configuring the environment, we can convert the CSV files into several SQLite databases.</p>

<p>The format of the csvs-to-sqlite command is:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>csvs-to-sqlite /path/to/your.csv sqlite-db-name.db
</code></pre></div></div>
<script src="https://gist.github.com/LiuYuWei/e4cad03c0bee591b6105bc1056a22f0b.js"></script>

<p>The result after we convert the CSV files into a database:
<img src="/img/posts/220804-1.webp" alt="The result after we convert the CSV files into a database" /></p>

<h2 id="verify-the-csv-was-imported-correctly">Verify the CSV was imported correctly</h2>
<p>We can now check if the data was successfully inserted into the SQLite database using the “sqlite3” command line tool to open the database and query the data.</p>

<script src="https://gist.github.com/LiuYuWei/29cd132ca02351bb3f66a1680c521d39.js"></script>

<p>We have successfully queried the SQLite data, which is the same as the CSV data. Now, If a data engineer wanted to query the historical data of an individual baseball player, they could use SQL to query the data faster.</p>

<p>The result of querying data from the SQLite database:
<img src="/img/posts/220804-2.webp" alt="The result of querying data from the SQLite database" /></p>

<hr />
<h3 id="i-am-simon">I am Simon</h3>
<p>Hi, I am Simon, Customer Success Engineer in InfuseAI. If you think the article is helpful to you, please give me applause. Welcome to provide some suggestions and discuss with me in InfuseAI <a href="https://discord.gg/xKxsdPx4d5">Discord</a>.
Linkedin: <a href="https://www.linkedin.com/in/simonliuyuwei/">https://www.linkedin.com/in/simonliuyuwei/</a></p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Datasets are readily available as CSV files on websites such as Kaggle and other real-world datasets. However, CSV isn’t really suitable for querying and transforming easily. Therefore, it’s better to store the data in a database.]]></summary></entry><entry><title type="html">PipeRider data profiling just got more colorful (and useful)</title><link href="https://blog.piperider.io/piperider-data-profiling-just-got-more-colorful-and-useful.html" rel="alternate" type="text/html" title="PipeRider data profiling just got more colorful (and useful)" /><published>2022-08-02T00:00:00+08:00</published><updated>2022-08-02T00:00:00+08:00</updated><id>https://blog.piperider.io/piperider-data-profiling-just-got-more-colorful-and%20useful</id><content type="html" xml:base="https://blog.piperider.io/piperider-data-profiling-just-got-more-colorful-and-useful.html"><![CDATA[<p><a href="https://github.com/InfuseAI/piperider/releases/tag/v0.5.0">PipeRider 0.5.0</a> was released last week and introduces some CLI updates that improve visual feedback during profiling, and the reporting feature adds some options to make it more suitable for use in your CI pipeline.</p>

<p><em>In case you missed it — <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is a new open-source <a href="https://blog.piperider.io/test-your-data-quality-in-minutes-with-piperider.html">data reliability</a> tool with data profiling, data assertions, and support for <a href="https://blog.piperider.io/add-data-profiling-and-assertions-to-dbt-with-piperider.html">popular data sources</a>.</em></p>

<hr />

<h2 id="cli-now-with-extra-prettiness">CLI now with extra prettiness</h2>
<p>The PipeRider CLI is now includes extra visual feedback when profiling in the form of per-table progress bars.</p>

<h3 id="progress-bars--so-satisfying">Progress bars — so satisfying</h3>
<p>The progress bars show how many tables are left to profile, the number of columns that are being profiled, and a timer.</p>

<p><img src="/img/posts/220802-1.gif" alt="Progress bars — satisfying :)" /></p>

<p>This enables you to see where you are in the profiling progress — no more thinking that the profiler hung when profiling your beefy dbt models.</p>

<h3 id="tabular-summary">Tabular Summary</h3>
<p>The profiling and data assertions summary is now displayed in a table so you can see test results at a glance.</p>

<p><img src="/img/posts/220802-2.webp" alt="Profiling and data assertion test results are now tabular — awesome!
" /></p>

<p>Extra color has also been sprinkled around to make the CLI output generally easier to read. So now it’ll fit better with your colorful terminal theme (<a href="https://draculatheme.com/terminal">Dracula</a>, anyone?).</p>

<p>If you’re working on the command line all day, then these should be some welcome additions.</p>

<h2 id="reports-where-you-want-them-when-you-want-them">Reports where you want them, when you want them</h2>
<p>For reporting, we’ve got a some great features to make generating and exporting reports more convenient.</p>

<h3 id="output-location">Output Location</h3>
<p>The default location for reports is <code class="language-plaintext highlighter-rouge">.piperider/outputs</code>, but this isn’t always the best location, especially if you want to share the reports with BI or other users who don’t want to dig around in a hidden folder.</p>

<p>In PipeRider v0.5.0 you can choose the output location for a report by using the <code class="language-plaintext highlighter-rouge">-o</code> option and specifying a location, e.g.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider generate-report -o ~/piperider-reports
</code></pre></div></div>

<p>This works for <code class="language-plaintext highlighter-rouge">piperider run</code> and <code class="language-plaintext highlighter-rouge">piperider compare-reports</code>, too!</p>

<h2 id="compare-the-last-two-reports-automatically">Compare the last two reports automatically</h2>
<p>On the subject of comparing reports, now you can compare the last two reports without having to select them manually — Just use the <code class="language-plaintext highlighter-rouge">--last</code> option:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider compare-reports --last
</code></pre></div></div>

<p>Pair this with a custom output location and you can easily automate PipeRider reports as part of your CI pipeline — we’ll have a tutorial/show-case on how to do this very soon. Make sure to follow us to be notified.</p>

<p>Check out the documentation <a href="https://docs.piperider.io/cli/piperider-cli">command reference</a> for more info, there are also a couple of how-to docs for <a href="https://docs.piperider.io/cli/quick-start#run-piperider-profile-data-and-generate-report">generating</a> and <a href="https://docs.piperider.io/cli/quick-start#run-piperider-profile-data-test-assertions-generate-report">comparing</a> reports.</p>

<hr />]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[PipeRider 0.5.0 was released last week and introduces some CLI updates that improve visual feedback during profiling, and the reporting feature adds some options to make it more suitable for use in your CI pipeline.]]></summary></entry><entry><title type="html">How to Use PipeRider’s Built-in Assertion and Custom Assertion to Test the Water Quality Kaggle Dataset</title><link href="https://blog.piperider.io/how-to-use-piperider-s-built-in-assertion-and-custom-assertion-to-test-the-water-quality-kaggle-dataset.html" rel="alternate" type="text/html" title="How to Use PipeRider’s Built-in Assertion and Custom Assertion to Test the Water Quality Kaggle Dataset" /><published>2022-07-27T00:00:00+08:00</published><updated>2022-07-27T00:00:00+08:00</updated><id>https://blog.piperider.io/how-to-use-piperider%E2%80%99s-built-in-assertion-and-custom-assertion-to-test-the-water-quality-kaggle-dataset</id><content type="html" xml:base="https://blog.piperider.io/how-to-use-piperider-s-built-in-assertion-and-custom-assertion-to-test-the-water-quality-kaggle-dataset.html"><![CDATA[<p><em>tl;dr PipeRider can help detect data issues in a water quality dataset through data profiling and custom assertions, which can be useful for determining the drinkability of water and solving problems immediately if alerts are raised.</em></p>

<hr />

<p>Determining the drinkability of water is the perfect case for a data reliability tool. Not only can we detect if there is missing data, but also when values fall outside of safe ranges. If the water resources management center has a good data assertion tool to detect the water quality, they can solve the problem immediately. Also, the manager and the employee can see a daily report to know the data distribution and water status. If PipeRider’s data assertions raise an alert, they can make a decision and better manage it.”</p>

<p>In this article, I will use a Kaggle water quality dataset and show how PipeRider can help detect data issues through data profiling and custom assertions.</p>

<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is a data quality toolkit for data professionals. With PipeRider, you can profile your data sources, create highly customizable data quality assertions, and generate insightful reports.</p>

<h2 id="get-the-dataset-from-kaggle">Get the dataset from Kaggle</h2>

<p>First, the Kaggle website stores the <a href="https://www.kaggle.com/datasets/adityakadiwal/water-potability">water quality</a> dataset. We need to download it and put it into the environment.</p>

<p>If you want to use the Kaggle CLI tool, you need to download the Kaggle key JSON file from the Kaggle website. The Proceed as follows:</p>

<p>Go to “Account”, go down the page, and find the “API” section.
Click the “Create New API Token” button.
We will download the “kaggle.json” file.
Put the file into <code class="language-plaintext highlighter-rouge">~/.kaggle/</code> folder.</p>

<p><img src="/img/posts/220727-1.webp" alt="Download the API token JSON file on the Kaggle Account page." /></p>

<p>Then, we can download the datasets through the Kaggle CLI tool.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>kaggle

kaggle datasets download <span class="nt">-d</span> adityakadiwal/water-potability

unzip water-potability.zip
</code></pre></div></div>

<h2 id="transfer-csv-to-sqlite">Transfer CSV to SQLite</h2>
<p>PipeRider supports four data sources: dbt integration, Postgres Connector, Snowflake Connector, and SQLite database. Here we use SQLite as our database example.</p>

<p>However, we need to transfer the CSV file into the SQLite database for PipeRider’s suitable database. Here, We can use the open source tool <a href="https://github.com/simonw/csvs-to-sqlite">csv-to-sqlite</a> to transfer the CSV files to the SQLite database.</p>

<p>You might also want to check the article of <a href="https://blog.piperider.io/transfer-the-csv-files-into-an-sqlite-database">Transfer the CSV Files into an SQLite Database</a></p>

<p>Follow the command line to transfer the CSV files to the SQLite database.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>csvs-to-sqlite
csvs-to-sqlite water-potability.csv water-potability.db
</code></pre></div></div>

<h2 id="add-build-in-assertion">Add build-in assertion</h2>
<p>Now, we can start to use PipeRider. We provide a Quick Start tutorial on the documentation page to show how to use PipeRider. You can follow the method to initialize, configure, diagnose and run PipeRider. After you run the <code class="language-plaintext highlighter-rouge">piperider run</code> command, you will get the “.piperider” folder in your project folder.</p>

<p><img src="/img/posts/220727-2.webp" alt="The structure of the .piperider folder" /></p>

<p>Although PipeRider provides an automatic assertion generation method, we want to configure our logic assertion. For example, the range of PH values is 0 to 14. If the value is over the content of values, then the data detection has some problem, and the users need to collect the data again. PipeRider provides two types of built-in assertions, one takes no parameter, and the other takes parameters.</p>

<p><a href="https://docs.piperider.io/cli/data-quality-assertions/assertion-configuratio">Built-In Assertions Guide</a></p>

<p>You can go to <code class="language-plaintext highlighter-rouge">.piperider/assertions/</code> to add assertions in <code class="language-plaintext highlighter-rouge">&lt;table&gt;.yml</code>. Here is an example of the built-in assertion yaml file.</p>

<script src="https://gist.github.com/LiuYuWei/27dd7c6ec5685f13de9810accb9a3f6a.js"></script>

<p>After modifying the YAML file, rerun the <code class="language-plaintext highlighter-rouge">piperider run</code> and see the assertion result.</p>

<p><img src="/img/posts/220727-3.webp" alt="The result of piperider run (We configure the built-in assertion YAML file.)" /></p>

<h2 id="add-custom-assertion">Add custom assertion</h2>

<p>We can see that some assertions failed because the maximum value range exceeded the WHO recommended value. If the user views the assertion error. The user needs to find the root cause of the datasets and improve the water quality.</p>

<p>In the description in the Kaggle datasets, we can see the content that tells us, “WHO has recommended maximum permissible limit of pH from 6.5 to 8.5.” Therefore, I want to test that the average value is in the range of 6.5 and 8.5. However, the built-in assertion method does not have this method. We need to write our own logic assertion to do the testing.</p>

<p>PipeRider provides a few built-in assertions and supports custom assertions as <em>plugins</em> that can satisfy your data quality check requirements. In this showcase, we add the new <em>assert_column_avg_in_range</em> assertion method and use the assertion to test the data profiling values.</p>

<p>PipeRider, by default, will load python files under <code class="language-plaintext highlighter-rouge">.piperider/plugins</code> custom assertion functions automatically. <code class="language-plaintext highlighter-rouge">.piperider/plugins</code> is created <code class="language-plaintext highlighter-rouge">piperider init</code> with a scaffolding of a custom assertion function, <code class="language-plaintext highlighter-rouge">customized_assertions.py</code>. You can rename the file or generate assertion functions in other python files.</p>

<script src="https://gist.github.com/LiuYuWei/d7168d5cda4b6296997cc489be3d5687.js"></script>

<p>After modifying the YAML file, rerun the <code class="language-plaintext highlighter-rouge">piperider run</code> and see the assertion result.</p>

<p><img src="/img/posts/220727-4.webp" alt="The result of piperider run (We add the custom assertion method.)" /></p>

<p>The result shows that the new assertion method is successfully tested. You can follow the python class structure to try the specific assertion method.</p>

<p>Also, You can check the history assertion result in Pipeider UI.</p>

<p><img src="/img/posts/220727-5.webp" alt="Assertion testing in PipeRider UI" /></p>

<hr />

<h3 id="i-am-simon">I am Simon</h3>
<p>Hi, I am Simon, Customer Success Engineer in InfuseAI. Please give me applause and also welcome to provide me with some suggestions if you think the article is helpful for you. Welcome to discuss with me in InfuseAI <a href="https://discord.gg/xKxsdPx4d5">Discord</a>.<br />
Linkedin: <a href="https://www.linkedin.com/in/simonliuyuwei/">https://www.linkedin.com/in/simonliuyuwei/</a></p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider can help detect data issues in a water quality dataset through data profiling and custom assertions, which can be useful for determining the drinkability of water and solving problems immediately if alerts are raised.]]></summary></entry></feed>