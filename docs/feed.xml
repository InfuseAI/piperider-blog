<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-01T23:47:34+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">PipeRider</title><subtitle>Data Reliability Automated</subtitle><author><name>PipeRider</name></author><entry><title type="html">Guide to Data Reliability</title><link href="http://localhost:4000/guide-to-data-reliability.html" rel="alternate" type="text/html" title="Guide to Data Reliability" /><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>http://localhost:4000/guide-to-data-reliability</id><content type="html" xml:base="http://localhost:4000/guide-to-data-reliability.html"><![CDATA[<p>Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.</p>

<p>You may be collecting a large amount of information within your data stack, but are you certain that these data sets are complete, accurate, and up-to-date? If not, these data sets might cost you a lot.</p>

<p>IBM estimated that the yearly cost of poor quality data, in the US alone, in 2016, is a whopping $3.1 trillion. In 2021, Gartner reported that every year, unreliable data costs organizations an average of $12.9 million. And it’s safe to say that the number has very likely increased as data-driven decision-making is adopted by every business imaginable.</p>

<p>That’s why ensuring your data is trustworthy by improving data reliability is very important.</p>

<h2 id="what-is-data-reliability">What is Data Reliability?</h2>

<p>Data reliability means that data is complete, accurate, and valid. It’s the foundation for building trust in your data across the organization. One of the main objectives of ensuring data reliability is building data trust, which is also used to maintain data security, data quality, and regulatory compliance.</p>

<p>Reliable data helps decision-makers take the guesswork out of the daily and strategic decision-making process to keep their organizations running. But if your data is unreliable, those same decisions become less accurate and can ultimately affect your organization.</p>

<h2 id="why-data-reliability">Why Data Reliability</h2>

<p>When unreliable data is used in making a key strategic decision, it can result in a mistake that damages an organization’s reputation, and bottom line, or even causes its future. Data reliability issues might not seem like a big deal at first glance, but they can snowball over time if left unchecked.</p>

<p>For example, you use customer data to develop targeted online ads or recommend products to your consumers. If the data you use isn’t accurate, then there’s a good chance that the advertising budget will be wasted on either poor results or zero return on investment.</p>

<p>The unsettling feeling when you are not sure if you can trust your data to make a decision can be highly stressful, ut there are actions you can take to improve your data reliability.</p>

<h2 id="how-to-improve-data-reliability">How to Improve Data Reliability</h2>

<p>Like many other managerial tasks, the process to improve your data reliability follows a series of logical steps. There are eight action items that your organization can take to improve your data reliability:</p>

<ol>
  <li>Assess Data Status</li>
  <li>Build Data Infrastructure</li>
  <li>Clean Existing Data</li>
  <li>Optimize Data Collection Processes</li>
  <li>Break Down All Data Silos</li>
  <li>Integrate Data Stack to Connect Data</li>
  <li>Organize Your Data</li>
  <li>Use Reports and Dashboards</li>
</ol>

<h3 id="assess-data-status">Assess Data Status</h3>

<p>Assessing your current data status is the first thing to do to improve data reliability. It helps you to get a general view of how your organization treats data. You should also employ data profiling. Data profiling is the process of examining and analyzing data. This helps you understand if your data is healthy. Assess your current situation to understand:</p>

<p>What are your data sources;
How and where you have stored the data;
How and where you have used the data;
The criteria used to determine data reliability.</p>

<h3 id="build-data-infrastructure">Build Data Infrastructure</h3>

<p>Once you’ve assessed your current situation, you can start updating your data infrastructure. No matter what the original data sources are, you need a secure and easy-to-use data repository. You need to define how your data will be stored, formatted, and organized. There are several steps you can take to create a data infrastructure:</p>

<p>Refine your strategy.
Build a data model.
Choose your data repository type – data lake, data warehouse, or hybrid.
Build an extract, transform, and load (ETL) process.
Implement ongoing data governance.</p>

<h3 id="clean-existing-data">Clean Existing Data</h3>

<p>If you have data sets in place already, you should examine the existing data and remove data that is:</p>

<ul>
  <li>inaccurate;</li>
  <li>incomplete;</li>
  <li>duplicative;</li>
  <li>outdated;</li>
  <li>incorrectly formatted.</li>
</ul>

<p>You should employ data profiling to analyze your data continually, so you can clean, and update data errors as soon as they are spotted.</p>

<h3 id="optimize-data-collection-processes">Optimize Data Collection Processes</h3>

<p>Start by analyzing internal processes for data input. Automate data entry wherever possible to minimize human errors. Make sure that all data entry follows your standardized formats and is accurate, complete, and valid.</p>

<p>Next, look at other data sources you obtain new data from. Make sure that their data formats follow your standardized format and remove inaccurate and unreliable data.</p>

<h3 id="break-down-all-data-silos">Break Down All Data Silos</h3>

<p>Organizations collect data from different departments or locations. This is necessary due to operational requirements or structure setup. But this might create independent data silos that would affect data reliability.</p>

<p>Not only do silos make it difficult to find and share data across your organization, but they also often adhere to different standards of organization and quality.</p>

<p>To ensure the most reliable data is available to those who need it internally, you need to break down your organization’s data silos. You should employ a central data repository for all departments and locations to minimize potential damage to data quality.</p>

<h3 id="integrate-data-stack-to-connect-data">Integrate Data Stack to Connect Data</h3>

<p>Quite often, different departments or locations use various tools and platforms. If you can get everyone to use the same tool and platform, great. If not, you should connect data from these tools and platforms across your entire organization to have a unified view of all your data. Therefore, when a piece of data is updated in one location, it is automatically updated wherever else it is used.</p>

<h3 id="organize-your-data">Organize Your Data</h3>
<p>Every organization has its unique way of organizing data to meet its unique needs. Organizing data makes it easier to locate specific data and speeds up your data retrieval process.</p>

<p>Typically, you will find labels, tags, groups, and other information stored in metadata. Depending on the type and use of your data, you may find data segmented by customer age, gender, geographic location, demographics, etc. No matter how you organize your data, make sure you understand the overall organization’s expectations and what it would like to achieve using the data.</p>

<h3 id="use-reports-and-dashboards">Use Reports and Dashboards</h3>

<p>Finally, make sure you are able to get insight from your data with reports and dashboards. For example, a data profile report can continue to alert you of data errors when it occurs. Other reports that track key metrics in a visual way with detailed analyses put peace of mind in you when it comes to making data-driven decisions.</p>

<h2 id="automate-your-data-reliability-with-piperider">Automate Your Data Reliability With PipeRider</h2>

<p>It may feel overwhelming when you manage a large amount of data, but once you lay the groundwork and build the foundation, there are many tools you can use to make the journey easier. If you’re interested in learning more about your data, with the aim of improving your data reliability, PipeRider can help you.</p>

<p>PipeRider is an open-source, free, and easy-to-use data reliability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements, enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.]]></summary></entry><entry><title type="html">Data Observability Explained and How to Integrate It into Your Workflow</title><link href="http://localhost:4000/guide-to-data-observability.html" rel="alternate" type="text/html" title="Data Observability Explained and How to Integrate It into Your Workflow" /><published>2022-09-23T00:00:00+08:00</published><updated>2022-09-23T00:00:00+08:00</updated><id>http://localhost:4000/guide-to-data-observability</id><content type="html" xml:base="http://localhost:4000/guide-to-data-observability.html"><![CDATA[<p>The number of data sources that data teams have to deal with is ever increasing. According to a <a href="https://www.matillion.com/resources/blog/matillion-and-idg-survey-data-growth-is-real-and-3-other-key-findings">recent survey</a> by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.</p>

<h2 id="what-is-data-observability">What is Data Observability?</h2>

<p>Data observability is the capability to comprehend, assess, and manage the state of data consumed by various technologies throughout the entire data lifecycle.</p>

<p>With data observability, your team can have a better understanding of your data. So they can gather consistent, standardized data from APIs, support data lake observability, facilitate routine queries to data warehouses, and share high-quality data across the entire organization.</p>

<h2 id="why-data-observability">Why Data Observability?</h2>

<p>One of the benefits of data observability is that teams can monitor data pipelines and quickly identify data issues with end-to-end data visibility.</p>

<p>Before data observability, teams might struggle with various data issues such as outdated data, broken data pipelines, or missing data. These issues might be caused by uncertainty in data standards or different data models from different data providers.</p>

<p>With data observability, your team can</p>

<ul>
  <li>standardize data for monitoring;</li>
  <li>debug and triage proactively;</li>
  <li>understand how data interacts with different tools;</li>
  <li>identify issues early;</li>
  <li>minimize the negative impact of data issues.</li>
</ul>

<p>Data observability also makes it possible for your team to automate parts of your monitoring process to constantly improve data quality with less time spent.</p>

<h2 id="what-does-data-observability-track">What Does Data Observability Track?</h2>

<p>Data profiling is an essential part of data observability. Through the following data profiling techniques, you can further understand your data and apply checks that will alert you to issues with your data.</p>

<p>Row-level validation and column-level profiling provide information about the system-wide performance of your data.
Anomaly detection helps spot problems before they damage data quality.
A statistics summary provides an in-depth understanding of the elements of your data observability framework.
Execution metadata and delays analysis throughout data pipelines to prevent data downtime.</p>

<p>These observability techniques should give you a comprehensive insight into the overall data health, potential data issues, and the quality of your data.</p>

<p>Incorporate Data Observability into Work to Improve Data Quality (h2)
According to research, one-third of data analysts spend more than 40 percent of their time on standardizing data to make it ready for analysis, and 57 percent of organizations still regard the “work of transforming their data to be very difficult.” It is obvious that ensuring consistent and accurate data can be a difficult and expensive task for organizations.</p>

<p>Therefore, having proper and solid data observability set up not only saves time but also a lot of resources, including money - But how do you incorporate data observability into your data quality workflow? You should start by developing a framework, then a strategy, and based on these two, choose the right tool for data observability.</p>

<h2 id="how-to-develop-a-data-observability-framework">How to Develop a Data Observability Framework</h2>

<p>Start your data observability journey by creating an efficient data-driven framework focusing on data quality, consistency, and reliability.</p>

<p>A data observability framework should answer the following questions:</p>

<ul>
  <li>How fresh and up-to-date is our data?</li>
  <li>What expected data value should we verify to ensure credible data?</li>
  <li>What data do we need to track and test to see when the data is broken?</li>
  <li>What is the responsibility of each team to various data sets?</li>
  <li>What other workflow, such as gathering metadata, or mapping upstream data sources and downstream users, do we need?</li>
</ul>

<p>The framework should give your team an overall view of standardized data across the organization, letting them quickly identify and fix problems.</p>

<h2 id="how-to-develop-a-data-observability-strategy">How to Develop a Data Observability Strategy</h2>

<p>Once you have a framework in place, many teams may jump right into integrating data observability into the entire data stack. But putting data observability into practice goes beyond the tools you employ.</p>

<p>You should start with preparing your team to adopt a culture of data-driven collaboration. Think about how to integrate data across different teams and sources, and also consider if implementing a new observability tool will affect existing workflow and resources.</p>

<p>Then incorporate the framework into your strategy to determine a standardized library/guidelines with the characteristics of quality data. Your team can use the guidelines to connect data from all sources.</p>

<p>Finally, incorporate your data sources into the observability tool. To obtain the metrics, logs, and traces required to provide end-to-end visibility, you might need to create new observability pipelines. Correlate the metrics you are tracking in your tool with targeted organization goals after adding the governance and data management rules. By using your observability tool to identify and address problems, you can also find new ways to automate some of your data management processes.</p>

<h2 id="how-to-choose-the-right-data-observability-tool">How to Choose the Right Data Observability Tool</h2>

<p>While there’s no one tool to fit every organization’s needs, a good observability tool should be able to:</p>

<ul>
  <li>gather, examine, sample, and process telemetry data from various data sources;</li>
  <li>detect and alert problems in datasets;</li>
  <li>provide end-to-end visibility;</li>
  <li>display data visualizations.</li>
</ul>

<p>In order to choose a suitable data observability tool, you’ll need to examine your current data stack and get a full picture of how data is gathered and distributed. Then you can look into a tool that is ready to integrate all of your data sources. Your chosen tool should be able to monitor your data in real-time throughout its lifecycle and monitor existing data without extraction. In addition, it should also be able to automate your data observability with minimum effort.</p>

<p>Ultimately, your organization’s specific data stack and data engineering requirements will determine the right tool for you. For the best implementation experience, give top priority to finding a tool that requires less work to standardize your data, map your data, or monitor your data.</p>

<h2 id="integrate-data-observability-with-piperider">Integrate Data Observability with PipeRider</h2>

<p>PipeRider is an open-source, free, and easy-to-use data observability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The number of data sources that data teams have to deal with is ever increasing. According to a recent survey by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.]]></summary></entry><entry><title type="html">Add Data Profiling and Assertions to dbt with PipeRider</title><link href="http://localhost:4000/add-data-profiling-and-assertions-to-dbt-with-piperider.html" rel="alternate" type="text/html" title="Add Data Profiling and Assertions to dbt with PipeRider" /><published>2022-07-26T00:00:00+08:00</published><updated>2022-07-26T00:00:00+08:00</updated><id>http://localhost:4000/add-data-profiling-and-assertions-to-dbt-with-piperider</id><content type="html" xml:base="http://localhost:4000/add-data-profiling-and-assertions-to-dbt-with-piperider.html"><![CDATA[<h3 id="tldr">tl;dr</h3>

<p>Data profiling helps you understand your data. When used with data assertions, a data profile can be used to determine data reliability. PipeRider is a non-intrusive open-source platform for adding profiling and assertions to data sources (such as dbt, Postgres, Snowflake). Star PipeRider on <a href="https://github.com/InfuseAI/piperider">Github</a>.</p>

<p><img src="/img/posts/220726-1.png" alt="PipeRider generates an HTML report with assertion results" title="PipeRider generates an HTML report with assertion results" /></p>

<h3 id="data-profile">Data profile</h3>

<p>Profiling is super important for understanding your data. A data profile is essentially data <em>about</em> your data. On its own a data profile can provide interesting insights about the data, but the real value comes from pairing the profile with data assertions. Together they form the basis of data reliability. Through data assertions you can define the acceptable norms for your data, and then test that your profile meets this specification.</p>

<h3 id="datapipelinerider">[data]Pipe[line]Rider</h3>

<p>With PipeRider you can easily add data profiling and assertions to your dbt project* and start profiling and testing your dbt models.</p>

<p>PipeRider shows profiling results on the command line, and also in an HTML report. Plus, if you run your dbt tests as part of the PipeRider run, they’ll also be included in the report.</p>

<p><em>PipeRider also works with other data sources, such as Snowflake, Postgres, SQLite, but for this tutorial I’ll focus on dbt.</em></p>

<p>If you’d prefer to watch how this is done then check out the <a href="https://www.youtube.com/watch?v=dnUq35s8AoY">YouTube video</a>. Otherwise read on.</p>

<h3 id="dbt-yeah-you-know-me">dbt, yeah you know me</h3>

<p>This guide assumes that you already have a dbt project ready to go, and that your dbt project is using either Snowflake or Postgres as a data source. Let’s add data reliability to your dbt project:</p>

<h3 id="install-piperider">Install PipeRider</h3>

<p>Install PipeRider using pip and specifying the data source your dbt project uses. I recommend using a virtual environment such as <code class="language-plaintext highlighter-rouge">venv</code> or <code class="language-plaintext highlighter-rouge">Conda</code> .</p>

<p><strong>Postgres</strong></p>

<p><code class="language-plaintext highlighter-rouge">pip install 'piperider[postgres]'</code></p>

<p><strong>Snowflake</strong></p>

<p><code class="language-plaintext highlighter-rouge">pip install 'piperider[snowflake]'</code></p>

<p>(don’t forget the quotes)</p>

<h3 id="initialize-piperider">Initialize PipeRider</h3>

<p>Inside your dbt project, run the following command to initialize PipeRider:</p>

<p><code class="language-plaintext highlighter-rouge">piperider init</code></p>

<p><img src="/img/posts/220726-2.png" alt="PipeRider init automatically detects your dbt project" title="PipeRider init automatically detects your dbt project" /></p>

<p>PipeRider will then automatically detect your dbt data source settings from your <code class="language-plaintext highlighter-rouge">dbt_project.yml</code> and <code class="language-plaintext highlighter-rouge">profiles.yml</code>.</p>

<h3 id="check-your-connection">Check your connection</h3>

<p>To test the connection to your data source, run the diagnose command:</p>

<p><code class="language-plaintext highlighter-rouge">piperider diagnose</code></p>

<p>In the Check Connections section, you should see your dbt models listed.</p>

<p><img src="/img/posts/220726-3.png" alt="Check that PipeRider can connect to your data source" title="Check that PipeRider can connect to your data source" /></p>

<h3 id="run-piperider">Run PipeRider</h3>

<p>Now all you need to do is run PipeRider!</p>

<p><code class="language-plaintext highlighter-rouge">piperider run</code></p>

<p>Since this is a dbt project, you likely already have some dbt tests configured. If you want your dbt tests to appear on your PipeRider report you can add the <code class="language-plaintext highlighter-rouge">--dbt-test</code> or <code class="language-plaintext highlighter-rouge">--dbt-build</code> option to the command. Check the <a href="https://docs.piperider.io/piperider-cli">PipeRider docs</a> for more info on available options.</p>

<h3 id="profile-and-assertions">Profile and Assertions</h3>

<p>PipeRider will profile the tables and columns from your data source and, as this is the first run, prompt to generate some recommended assertion files.</p>

<p><img src="/img/posts/220726-4.png" alt="Data profiled, do you ant to generate recommended assertions?" title="Data profiled, do you ant to generate recommended assertions?" /></p>

<p>If you choose ‘<strong>yes</strong>’, PipeRider will make assertion files with some sensible defaults based on the current structure of your data.</p>

<p>If you choose ‘<strong>no</strong>’, then PipeRider will create skeleton assertion files.</p>

<p>Choose ‘yes’ and you’ll see that PipeRider creates assertion files for your tables, and then tests them against the recommended assertions.</p>

<p><img src="/img/posts/220726-5.png" alt="PipeRider tests the data source against the assertions" title="PipeRider tests the data source against the assertions" /></p>

<p>As it’s the first run, and you’re using recommended assertions, all of the assertions pass. You’ll find out how to edit the assertions below.</p>

<h3 id="html-report">HTML Report</h3>

<p>At the end of the CLI output, you’ll also find a link to the PipeRider report.</p>

<p><img src="/img/posts/220726-6.png" alt="PipeRider run summary and link to the generated HTML report" title="PipeRider run summary and link to the generated HTML report" /></p>

<p>The report contains the data profile for each table, plus the PipeRider assertion and dbt test results (if you ran those as well).</p>

<p><img src="/img/posts/220726-7.png" alt="A sample of PipeRider's data profile report" title="A sample of PipeRider's data profile report" /></p>

<h3 id="assertions">Assertions</h3>

<p>The recommended assertions provide some sensible defaults that you can use as a starting off point. If you’re confident your data is currently in good health, then you can leave them as-is.</p>

<p>Most likely, you’ll want to review the data profile in your PipeRider report, and then adjust the recommended assertions as you see fit.</p>

<p>The assertion files for your tables can be found in <code class="language-plaintext highlighter-rouge">.piperider/assertions</code></p>

<p><img src="/img/posts/220726-8.png" alt="Assertions files can be found in .piperider/assertions" title="Assertions files can be found in .piperider/assertions" /></p>

<p>Open one of the assertion files and you’ll see the defaults that PipeRider has set up for you.</p>

<p><img src="/img/posts/220726-9.png" alt="An example of a PipeRider recommended assertions file" title="An example of a PipeRider recommended assertions file" /></p>

<p>PipeRider comes with some <a href="https://docs.piperider.io/data-quality-assertions/assertion-configuration">built-in assertions</a> you can use to test your data profile. If you want to get your hands dirty then you can also make <a href="https://docs.piperider.io/data-quality-assertions/custom-assertions">custom assertions</a> — if it’s in your data profile, you can test it.</p>

<p>After updating your assertion files, simply run PipeRider again to test your data profile against the new assertions. Failed assertion results can be viewed both on the CLI, and in the generated reports.</p>

<p><img src="/img/posts/220726-10.png" alt="PipeRider CLI output with failed assertion" title="PipeRider CLI output with failed assertion" /></p>

<h3 id="whats-next">What’s next?</h3>

<p>There’s lots more to check out with PipeRider, such as <a href="https://docs.piperider.io/how-to/compare-reports">comparing reports</a>, regenerating assertions, and <a href="https://docs.piperider.io/how-to/github-action">GitHub actions</a>. Check out the <a href="https://docs.piperider.io/">PipeRider documentation</a> for full details of all features.</p>

<h3 id="infuseai-is-making-data-reliability-tools">InfuseAI is making data reliability tools</h3>

<p>We’re InfuseAI: lovers of open-source, data-quality aficionados.</p>

<p>Tell us you want better data reliability by:</p>

<ul>
  <li>Staring PipeRider on <a href="https://github.com/InfuseAI/piperider">Github</a></li>
  <li>Joining the InfuseAI <a href="https://discord.gg/328QcXnkKD">Discord community</a></li>
  <li>Saying hi to <a href="https://twitter.com/InfuseAI">@infuseai</a> on Twitter</li>
  <li>Clapping or giving us some feedback in the comments</li>
</ul>

<h3 id="if-you-read-here-you-might-want-to-learn-more-about-data-reliability">If you read here, you might want to learn more about data reliability:</h3>
<ul>
  <li><a href="https://blog.piperider.io/data-reliability-automated-with-piperider.html">Data Reliability Automated with PipeRider</a></li>
  <li><a href="https://blog.piperider.io/guide-to-data-reliability.html">Guide to Data Reliability</a></li>
</ul>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr]]></summary></entry><entry><title type="html">Test your data quality in minutes with PipeRider</title><link href="http://localhost:4000/test-your-data-quality-in-minutes-with-piperider.html" rel="alternate" type="text/html" title="Test your data quality in minutes with PipeRider" /><published>2022-07-06T00:00:00+08:00</published><updated>2022-07-06T00:00:00+08:00</updated><id>http://localhost:4000/test-your-data-quality-in-minutes-with-piperider</id><content type="html" xml:base="http://localhost:4000/test-your-data-quality-in-minutes-with-piperider.html"><![CDATA[<h1 id="test-your-data-quality-in-minutes-with-piperider">Test your data quality in minutes with PipeRider</h1>

<p><strong>tl;dr</strong> If you missed out on PipeRider’s initial release, then now is a great time to take it for a spin. <strong>Data reliability just got even more reliable with better dbt integration</strong>, data assertion recommendations, and reporting enhancements. PipeRider is <a href="https://github.com/InfuseAI/piperider">open-source</a> and easy to <a href="https://docs.piperider.io/quick-start?utm_source=blog&amp;utm_medium=20220706&amp;utm_id=medium">get started</a>.</p>

<h2 id="piperider-recap">PipeRider Recap</h2>

<p>PipeRider is your open-source data reliability toolkit that connects to your existing data pipelines and provides data profiling, data quality assertions, convenient HTML reports, and integration with popular data warehouses. <a href="https://blog.infuseai.io/data-reliability-automated-with-piperider-7a823521ef11">Read more</a> about the story behind PipeRider’s creation.</p>

<p><img src="/img/posts/220706-2.png" alt="test your data quality within minutes" /></p>

<h2 id="whats-new">What’s New?</h2>

<p>Recent updates to PipeRider include the following features:</p>

<ul>
  <li><a href="https://blog.infuseai.io/test-your-data-quality-in-minutes-with-piperider-91da57a5c5d5#31b3">Recommended data assertion generation</a> — PipeRider’s intelligently generated assertions give you a head-start on data reliability.</li>
  <li><a href="https://blog.infuseai.io/test-your-data-quality-in-minutes-with-piperider-91da57a5c5d5#7d35">Improved dbt integration</a> — PipeRider will auto-detect your dbt project and data source settings. You can also run dbt tests with PipeRider and dbt test results are included in your report.</li>
  <li><a href="https://blog.infuseai.io/test-your-data-quality-in-minutes-with-piperider-91da57a5c5d5#7511">Improved reporting</a> — Automatically generated reports provide data profiling information and assertion results.</li>
</ul>

<p>Let’s take a deeper dive into these new features.</p>

<h2 id="recommended-assertions">Recommended assertions</h2>

<p>The first time you run PipeRider and your data is profiled, PipeRider will offer to generate recommended assertions based on the profile of your data source.</p>

<p>PipeRider analyzes the contents of your data source and makes intelligent suggestions based on the content, such as:</p>

<ul>
  <li>Asserting which columns require data and should not be null</li>
  <li>Asserting the schema type for columns</li>
  <li>Asserting the acceptable range of minimum and maximum values for numerical columns</li>
</ul>

<p><img src="/img/posts/220706-3.png" alt="PipeRider's CLI tool offers to generate recommended data assertions based on your data profile" title="Generate recommended assertions with PipeRider on first run" /></p>

<p>By using the recommended assertions you give yourself a head start by not having to manually write all of the assertions. Instead, you can tweak and add to the recommendations if necessary.</p>

<p><img src="/img/posts/220706-4.png" alt="An example of a recommended assertions file generate by PipeRider, a data reliability tool" title="Example of PipeRider's recommended assertions for a data source" /></p>

<p>If you’d rather not use the recommendations, PipeRider can also generate empty assertion template files, complete with column names, ready for you to customize with <a href="https://docs.piperider.io/data-quality-assertions/assertion-configuration">built-in assertions</a>, or your own <a href="https://docs.piperider.io/data-quality-assertions/custom-assertions">custom assertions</a>.</p>

<h2 id="dbt-integration">dbt integration</h2>

<p>You can now initialize PipeRider inside your dbt project, this brings PipeRider’s profiling, assertions, and reporting features to your dbt models.</p>

<p><img src="/img/posts/220706-5.png" alt="PipeRider auto-detects your dbt project settings which makes adding data profiling to a dbt project easy" title="PipeRider auto detects your dbt project settings" /></p>

<p>PipeRider will automatically detect your dbt project settings and treat your dbt models as if they were part of your PipeRider project. This includes -</p>

<ul>
  <li>Profiling dbt models.</li>
  <li>Generating recommended assertions for dbt models.</li>
  <li>Testing dbt model data-profiles with PipeRider assertions.</li>
  <li>Including dbt test results in PipeRider reports.</li>
</ul>

<p><img src="/img/posts/220706-6.png" alt="PipeRider can run both dbt tests and PipeRider's own assertions with one command" title="PipeRider and dbt tests can be run with one command" /></p>

<p>You can also build your dbt models using PipeRider, which means it’s possible to condense your dbt and PipeRider workflow to one command: <code class="language-plaintext highlighter-rouge">piperider run --dbt-build --dbt-test</code>. This one command will:</p>

<ol>
  <li>Build your dbt models.</li>
  <li>Profile your data with PipeRider’s profiler.</li>
  <li>Run your dbt tests.</li>
  <li>Test your data profile with PipeRider assertions</li>
  <li>Generate an HTML report.</li>
</ol>

<p>Full details and other available options can be found in the <a href="https://docs.piperider.io/piperider-cli">command reference</a>.</p>

<h2 id="improved-reporting">Improved reporting</h2>

<p>Reports are now automatically generated each time you run PipeRider and include the following information.</p>

<ul>
  <li>All tables included</li>
  <li>Per-table profiling data</li>
  <li>Per-table test results</li>
  <li>Per-table dbt test results</li>
</ul>

<h3 id="report-overview">Report overview</h3>

<p>The report overview page shows a quick view of which tables or dbt models have been profiled and tested, along with stats about passed and failed tests.</p>

<p><img src="/img/posts/220706-7.png" alt="PipeRider report overview showing both PipeRider assertion results and dbt test results" title="PipeRider report overview showing both PipeRider assertion results and dbt test results" /></p>

<h3 id="per-table-reports">Per-table reports</h3>

<p>Click through to each table to view the data profile detailed test results for that table.</p>

<p><img src="/img/posts/220706-6.png" alt="View detailed per-table information on data profile and test results in PipeRider's report" title="PipeRider data table report features data profile and test results" /></p>

<h3 id="dbt-test-results">dbt test results</h3>

<p>If the PipeRider is run on a dbt project, then the report will include a tab with details of dbt-specific test results.</p>

<p><img src="/img/posts/220706-8.png" alt="View dbt test results in your PipeRider report" title="View dbt test results in your PipeRider report" /></p>

<h3 id="compare-reports">Compare reports</h3>

<p>Two reports can also be compared showing the differences between the data profile for each run, together with the expected and actual results for each test.</p>

<p><img src="/img/posts/220706-9.png" alt="Compare results for two PipeRider runs that shows the difference in data profile and test results" title="Compare two reports from different PipeRider runs" /></p>

<h3 id="no-excuse-for-unreliable-data">No excuse for unreliable data</h3>

<p>PipeRider is so easy to use there really is no excuse for not picking up on that data drift. All you need to do it:</p>

<ol>
  <li><a href="https://docs.piperider.io/install-piperider">Install PipeRider</a> (a quick install with pip)</li>
  <li>Point it to your data warehouse — if you’re using dbt your settings will be auto-detected</li>
  <li>With one <a href="https://docs.piperider.io/piperider-cli">command</a> you can generate a data profile, with suggested assertions, and output a report</li>
</ol>

<p>Check out the <a href="https://docs.piperider.io/quick-start?utm_source=blog&amp;utm_medium=20220706&amp;utm_id=medium">quick-start guide</a>, which includes a sample dataset, for how to use the main features of PipeRider.</p>

<h2 id="more-info">More info</h2>

<p>For help using PipeRider, or if you have ideas how we can make it better, get in touch via:</p>

<ul>
  <li>InfuseAI <a href="https://discord.gg/328QcXnkKD">Discord community</a></li>
  <li><a href="https://twitter.com/InfuseAI">@infuseai</a> on Twitter</li>
  <li>⭐️ Star us on <a href="https://github.com/InfuseAI/piperider">Github</a></li>
</ul>

<h2 id="want-to-learn-more-about-data-quality">Want to learn more about data quality?</h2>
<p>You may also like to read:</p>
<ul>
  <li><a href="https://blog.piperider.io/data-reliability-automated-with-piperider.html">Data Reliability Automated with PipeRider</a></li>
  <li><a href="https://blog.piperider.io/guide-to-data-reliability.html">Guide to Data Reliability</a></li>
</ul>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Test your data quality in minutes with PipeRider]]></summary></entry><entry><title type="html">Data Reliability Automated with PipeRider</title><link href="http://localhost:4000/data-reliability-automated-with-piperider.html" rel="alternate" type="text/html" title="Data Reliability Automated with PipeRider" /><published>2022-06-22T00:00:00+08:00</published><updated>2022-06-22T00:00:00+08:00</updated><id>http://localhost:4000/data-reliability-automated-with-piperider</id><content type="html" xml:base="http://localhost:4000/data-reliability-automated-with-piperider.html"><![CDATA[<p>tl;dr <em>PipeRider is an open-source data reliability toolkit for identifying data quality issues across pipelines. PipeRider was created after months of industry research and it’s </em><a href="https://github.com/InfuseAI/piperider"><em>available now</em></a><em>. Start learning more about the quality of your data by taking PipeRider for a </em><a href="https://docs.piperider.io/quick-start?utm_source=medium&amp;utm_medium=piperider_intro&amp;utm_id=blog"><em>test drive</em></a><em>.</em></p>

<p>It’s been an interesting and fun time at <a href="https://www.infuseai.io/?utm_source=medium&amp;utm_medium=piperider_intro&amp;utm_id=blog">InfuseAI</a> during the latter half of 2021 and the beginning of 2022 as we’ve been working on our new open-source data reliability product, <a href="https://piperider.io/?utm_source=medium&amp;utm_medium=piperider_intro&amp;utm_id=blog">PipeRider</a>.</p>

<hr />

<p><strong>PipeRider — Data Reliability Automated</strong></p>

<p>For a few months we were deep in research as we interviewed many data industry professionals about the tools they use in their daily workflow, and the common issues they run into when working with data. The result of our research was a plan for a new product that would fix these issues at the source — the data source.</p>

<p>That’s where PipeRider comes in — our data reliability toolkit. But first, let’s rewind and look at some of the issues that were raised during our research and how these came to determine what exactly PipeRider would become.</p>

<h2 id="trust-in-data">Trust in Data</h2>

<p>In a data-centric world, trusting your data is essential to make informed business decisions. For organizations that rely heavily on data, blindly trusting the quality of that data is a big ask. Business decisions are so reliant on quality data and, in some cases, trust so lacking, that during our research we heard about instances of data scientists <em>manually</em> checking data to ensure its reliability. An unsustainable situation.</p>

<p><img src="/img/posts/220622-2.png" alt="" /></p>

<h3 id="gut-feeling-is-no-way-to-make-business-decisions">‘Gut feeling’ is no way to make business decisions</h3>

<p>When the data sources were too abundant, making manual checking impossible, sometimes decisions were made on “gut feeling”, with data sources serving only as a reference. Exactly the opposite of what should be happening in data-centric environments.</p>

<p>We found other situations in which BI had so much distrust in the data reports they were receiving that they sought out data engineers to ask questions face-to-face.</p>

<p><img src="/img/posts/220622-3.png" alt="" /></p>

<p>The main takeaway here is that a lack of robust data reliability testing leads to a distrust of data, which in turn leads to wasted time, second-guessing, and decisions led by gut-feeling. In other words — Lost revenue. In those cases when untested data was incorrectly trusted, errors were only noticed when some anomaly was discovered downstream, far too late to prevent any negative effects</p>

<h3 id="learn-from-your-data-nightmares">Learn from your data-nightmares</h3>

<p>We’re not recounting these data-nightmares for dramatic effect, we needed to learn from these mistakes, as the oft-used George Santayana quote says <em>“<strong>Those who cannot remember the past are condemned to repeat it</strong>“</em>… Maybe we should coin a new phrase? <em>“Those who do not test their data, are condemned to data decay”</em>. (Not as catchy? Oh well)</p>

<p>The research gave us insight into some essential attributes of a data reliability tool:</p>

<ul>
  <li>It should help manage the complexity of data pipelines, providing data profiles to help you keep ahead of show-stopping data changes. The last thing you want is to be checking logs after something breaks.</li>
  <li>It should fit into your existing tool ecosystem, and enable you to seamlessly insert it between stages in the pipeline. Users can’t be expected to completely rework their pipelines to fit in a data quality tool.</li>
  <li>It should allow users to trust their data, and prove the data is trustworthy by providing useful and insightful reporting.</li>
</ul>

<h2 id="riding-the-data-pipeline">Riding the data pipeline</h2>

<p>PipeRider is a data reliability tool which ensures the health of your data. It does this by profiling the data and schema and then applying data assertions to ensure that the integrity of the data is retained.</p>

<h3 id="maintain-a-high-profile">Maintain a high-profile</h3>

<p>Hands on, the idea behind PipeRider is simple -</p>

<ol>
  <li>Connect it to your data source at any point in your pipeline.</li>
  <li>Run PipeRider to generate a data profile.</li>
  <li>Test the profile against your data assertions.</li>
</ol>

<p>The data profile works on both the table and column level and provides information such as the number of rows, schema structure, distribution of column data, ranges etc.</p>

<p><img src="/img/posts/220622-4.png" alt="PipeRider profiles a data source" title="PipeRider profiles a data source" /></p>

<p>Assertions provide a way for you to define your expectations of the data, and then test your data source against these assertions. Each time you run PipeRider, your data is profiled and checked against these assertions. If any of the profiled data falls outside the bounds of your assertions then the test fails and an alert raised.</p>

<h3 id="be-assertive">Be assertive</h3>

<p>PipeRider comes bundled with a set of data assertions that you can implement by adding them to your data assertions file. These ready made assertions allow you to test factors of your data such as:</p>

<ul>
  <li>Ensuring that certain column exists.</li>
  <li>Ensuring the uniqueness of a column.</li>
  <li>Ensuring the data type of a column.</li>
  <li>Testing for null values.</li>
  <li>Applying acceptable ranges to minimum and maximum values of a column.</li>
  <li>Monitoring for schema changes between runs.</li>
</ul>

<p><img src="/img/posts/220622-5.png" alt="PipeRider run with assertion results" title="PipeRider run with assertion results" /></p>

<p>When your data source is profiled for the first time, PipeRider will output an assertion template for each table. Then you only need to apply your desired assertions to each column and the next time PipeRider runs, your data will be tested against the assertion files.</p>

<h2 id="customized-assertions-for-your-needs">Customized assertions for your needs</h2>

<p>In addition to the built-in assertions, PipeRider also has a plugin system that allows you to create your own custom assertions. So long as the information is contained in the PipeRider data profile, then you can test against it. All that is required is a little Python knowledge and you can create assertions to test all aspects of your data profile — a task that is well within reach of all data engineers.</p>

<h2 id="data-quality-reports">Data quality reports</h2>

<p>Running data checks on the command line isn’t for everyone, especially departments who need the visualize the data in a meaningful way. Each time PipeRider runs, it automatically creates a report on the current state of your data.</p>

<p><img src="/img/posts/220622-6.png" alt="Data quality reports are generated each time PipeRider runs" title="Data quality reports are generated each time PipeRider runs" /></p>

<p>If any data assertions have failed, or the schema has changed in any way, then the report will show this information, along with the profiling information for the data source. Reports provide a convenient way to share data profiling and assertion results to users who aren’t interested in running commands in the terminal.</p>

<p>Comparison reports are also available to compare the results of two different PipeRider reports and see how data has changed between profiling and testing runs.</p>

<h2 id="the-future-is-automated">The future is automated</h2>

<p>PipeRider has only just seen its first public release, but we’re moving fast on our roadmap to implement even more features to ensure the reliability of your data.</p>

<p>One feature we’re really excited about is automatically suggesting tests based on your data profile. Right now it’s up to you set decide on which assertions to use for which columns, but the new version of our profiler will intelligently analyze your data source and suggest recommended tests based on the structure of your data.</p>

<p>For instance, PipeRider might suggest that a certain column should be within a numerical range, the content should belong to a set of values, or automatically detect that a column should adhere to some business logic format such as email address, date, IP etc. This could really speed up the time to implement data reliability checks and even shed light on previously overlooked data .</p>

<p>In addition, we’ve got more data warehouse support coming, insight into data lineage, integration with dbt tests, and also enhanced notifications through popular communication platforms such as Slack.</p>

<h2 id="join-us-on-the-data-reliability-journey">Join us on the data reliability journey</h2>

<p>As the developer adage goes — release early, release often. We’ve released PipeRider with what we feel is a solid base of features that we are already building on, and we want you to come with us on the journey and help us make PipeRider meet your needs.</p>

<p>PipeRider is an open source project, and with all our projects at InfuseAI, that means you are welcome to contribute to the project. A contribution could be anything — code if you’re a developer, a bug report or feature request if you’re a user, or simply your thoughts about the project after using it.</p>

<p>It’s easy to take PipeRider for a test drive, just head over to the <a href="https://docs.piperider.io/quick-start?utm_source=medium&amp;utm_medium=piperider_intro&amp;utm_id=blog">Quick Start</a> guide in our documentation. We’ve got a sample dataset ready for you to use, so you won’t need to connect to an existing project to try it out.</p>

<p>After using PipeRider we’d appreciate your feedback! Get in touch through whichever platform you prefer:</p>

<ul>
  <li>GitHub: <a href="https://github.com/InfuseAI/piperider">https://github.com/InfuseAI/piperider</a></li>
  <li>Twitter: <a href="https://twitter.com/InfuseAI">@InfuseAI</a></li>
  <li>Discord: <a href="https://discord.com/invite/328QcXnkKD">InfuseAI Community</a></li>
</ul>

<hr />
<h4 id="learn-more-about">Learn more about</h4>
<ul>
  <li><a href="https://blog.piperider.io/guide-to-data-observability.html">Data Observability Explained and How to Integrate It into Your Workflow</a></li>
  <li><a href="https://blog.piperider.io/guide-to-data-reliability.html">Guide to Data Reliability</a></li>
</ul>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider is an open-source data reliability toolkit for identifying data quality issues across pipelines. PipeRider was created after months of industry research and it’s available now. Start learning more about the quality of your data by taking PipeRider for a test drive.]]></summary></entry></feed>