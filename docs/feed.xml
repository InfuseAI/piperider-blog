<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://blog.piperider.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.piperider.io/" rel="alternate" type="text/html" /><updated>2023-01-19T18:05:04+08:00</updated><id>https://blog.piperider.io/feed.xml</id><title type="html">PipeRider</title><subtitle>Data Reliability Automated</subtitle><author><name>PipeRider</name></author><entry><title type="html">Why you lack confidence merging dbt data model PRs</title><link href="https://blog.piperider.io/why-you-lack-confidence-merging-dbt-data-model-PRs.html" rel="alternate" type="text/html" title="Why you lack confidence merging dbt data model PRs" /><published>2023-01-19T00:00:00+08:00</published><updated>2023-01-19T00:00:00+08:00</updated><id>https://blog.piperider.io/why-you-lack-confidence-merging-dbt-data-model-PRs</id><content type="html" xml:base="https://blog.piperider.io/why-you-lack-confidence-merging-dbt-data-model-PRs.html"><![CDATA[<p>We love dbt - Its popularity seems to have skyrocketed over the last year or so. Everywhere I look people are writing about learning and using dbt. Applying software development practices to data projects, and the beauty of ELT, has really struck a chord with data engineers.</p>

<p>With dbt projects managed in Git, data engineers can take advantage of versioning, pull requests, issues, and all the related good stuff such as continuous integration (CI) and continuous deployment (CD) to automate tasks when pull requests (PR) are made on a project. This opens the door to so much possibility for augmenting the building and deployment process with extra features.</p>

<h2 id="with-great-power-comes-great-responsibility">With great power, comes great responsibility</h2>

<p>The power that dbt and Git/CI brings is great, and it makes collaborating on data projects so much easier, but with more engineers working together on projects, and increasing size and complexity of data pipelines, the process of reviewing data project pull requests is getting much more difficult.</p>

<h2 id="are-you-confident-merging-changes-to-production">Are you confident merging changes to production?</h2>

<p>Reviewing a pull request already has a <a href="https://www.getdbt.com/blog/how-to-review-an-analytics-pull-request/">laundry list of things to consider</a>, but there’s one aspect which can be particularly difficult - Ensuring that the “<a href="https://www.getdbt.com/blog/how-to-review-an-analytics-pull-request/#-data-returns-expected-results">data returns expected results</a>”, especially when you’re looking at model changes to an existing pipeline and need to determine the impact of the change.</p>

<p>If you can’t determine the full impact of the changes, then it’s impossible to be completely confident in merging data model changes. The amount of confidence you have depends on the method you use to check the data, and for that, you’re on your own.</p>

<h2 id="how-do-you-check-the-data">How do you check the data?</h2>

<p>Comparing the data from production with the data from the PR isn’t always straightforward, and it’s the part of the process when ‘silent errors’ can sneak into the pipeline, especially with <a href="https://docs.getdbt.com/docs/build/metrics">key metrics</a>. So, what are the common ways to check the data, and are they any good?</p>

<h3 id="manually-check">Manually check</h3>

<p>This method involves querying the data, checking values against known correct values, and generally trying to get a ‘feel’ for if the data is correct. The problem is that it’s a slow process, and manual checks are error prone. It’s too easy to miss something and have errors sneak into production.</p>

<h3 id="bi-tools">BI Tools</h3>

<p>The data ends up in BI tools anyway. Couldn’t you just check the data there? You could load the PR data into a BI tool but, again, it’s a slow process, and there’s no great way to compare production and PR environments in a BI tool.</p>

<h3 id="dbt-tests">dbt tests</h3>

<p>dbt has assertions that you can use to test the data in a pass/fail sense: <code class="language-plaintext highlighter-rouge">unique</code>, <code class="language-plaintext highlighter-rouge">not null</code>, <code class="language-plaintext highlighter-rouge">accepted values</code> and <code class="language-plaintext highlighter-rouge">relationships</code>.  dbt tests are useful for some cases but, for metrics, there’s no way to define what ‘correct’ data looks like. The only way to know if a change in metrics is desired is to compare metrics from the PR environment with production and then assess, given the modeling changes, if the impact of these changes are expected.</p>

<h2 id="solution-automated-metrics-comparison">Solution: Automated metrics comparison</h2>

<blockquote>
  <p>One of the many benefits of running dbt in a CI environment is being able to query the production data and see how it compares to the data built by the changes in the PR.</p>
</blockquote>

<p style="text-align: right">-- Erin Vaughan, Director of Customer Success @ dbt labs</p>

<p>As Erin mentions above, a CI process is invaluable for dbt projects, and with dbt’s <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">state</a> method (<a href="https://docs.getdbt.com/guides/legacy/best-practices#run-only-modified-models-to-test-changes-slim-ci">SlimCI</a>) you can create a PR environment and <a href="https://discourse.getdbt.com/t/how-we-sped-up-our-ci-runs-by-10x-using-slim-ci/2603">only run/test models that have changed</a>.</p>

<p>This is where our solution comes in - To automatically query and compare key metrics as part of CI/CD and provide a summary of changes for the review to quickly grasp the impact.</p>

<p>The CI process goes something like:</p>

<ul>
  <li>Create a PR environment (using dbt state)</li>
  <li>Query key metrics from PR environment</li>
  <li>Query key metrics from production environment</li>
  <li>Compare the metrics from both environments</li>
  <li><strong>Attach a metrics comparison summary to the PR comment</strong></li>
</ul>

<p><img src="/img/posts/piperider_compare-business-metrics.png" alt="Compare metrics" title="Compare metrics with PipeRider" /></p>
<p style="text-align: center">Compare metrics using PipeRider</p>

<h3 id="confidence-acquired">Confidence Acquired</h3>

<p>The PR reviewer can now not only be alerted to model changes that affect metrics, but also see the impact of those changes. They can then determine if those changes are desirable and expected, or are the result of an error in the modeling queries.</p>

<p>With this knowledge to hand, data engineers and PR reviewers can <a href="https://blog.infuseai.io/how-to-be-more-confident-making-data-model-changes-76a2f65feffa">be more confident merging data model changes into production</a>, and stop letting silent errors slip into production.</p>

<h2 id="piperider">PipeRider</h2>

<p>At PipeRider, we’re already well into solving this problem and you can try out our solution right now. PipeRider is our data reliability tool with a special focus on dbt project integration and automation with CI.</p>

<ul>
  <li>Read more about <a href="https://docs.piperider.io/cli/continuous-integration-ci">using PipeRider with continuous intergration</a> in the documentation</li>
  <li>You can also <a href="https://docs.piperider.io/cli/dbt-integration/metrics">add dbt metrics to your PipeRider reports</a>, and compare metics between runs</li>
</ul>

<p>If you’re interested in better understanding the impact of data model changes, get started with <a href="https://docs.piperider.io/cli/dbt-integration">PipeRider and dbt</a>, or sign-up at <a href="https://piperider.io">piperider.io</a> to arrange a call.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[We love dbt - Its popularity seems to have skyrocketed over the last year or so. Everywhere I look people are writing about learning and using dbt. Applying software development practices to data projects, and the beauty of ELT, has really struck a chord with data engineers.]]></summary></entry><entry><title type="html">Share the Spectacular World Cup construction with ChatGPT</title><link href="https://blog.piperider.io/share-the-spectacular-world-cup-construction-with-chatgpt.html" rel="alternate" type="text/html" title="Share the Spectacular World Cup construction with ChatGPT" /><published>2022-12-15T00:00:00+08:00</published><updated>2022-12-15T00:00:00+08:00</updated><id>https://blog.piperider.io/share-the-spectacular-world-cup-construction-with-chatgpt</id><content type="html" xml:base="https://blog.piperider.io/share-the-spectacular-world-cup-construction-with-chatgpt.html"><![CDATA[<p>The <a href="https://www.fifa.com/fifaplus/en/tournaments/mens/worldcup/qatar2022">World Cup</a> ⚽️ and <a href="https://chat.openai.com/chat">ChatGPT</a> have recently been the most popular subjects all across the world. The World Cup is the largest football event conducted every four years, and it is being held in Qatar this year, and ChatGPT is the natural language model for dialogue built by <a href="https://openai.com/">OpenAI</a>.</p>

<p>On 18 December 2022, the World Cup Final will be held in the <a href="https://www.qatar2022.qa/en/tournament/stadiums/lusail-stadium">Lusail Stadium</a> of Lusail city. Lusail city was still desert in 2010 but the new municipality now includes an urban center and an artificial archipelago.</p>

<p>Left: Satellite image of Lusail city in 2010; Right: Satellite image of Lusail city in 2022 (Source: <a href="https://www.bloomberg.com/graphics/2022-what-qatar-built-for-the-world-cup">https://www.bloomberg.com/graphics/2022-what-qatar-built-for-the-world-cup</a>): 
<img src="221215-chatgpt-1.webp" alt="" /></p>

<p>As ChatGPT would have limited knowledge of the world after 2021, so I’m curious about how ChatGPT will introduce Lusail city. 🤔</p>

<p><img src="/img/posts/221215-chatgpt-2.webp" alt="" /></p>

<p>Anyway, let’s try!</p>

<p><img src="/img/posts/221215-chatgpt-3.webp" alt="" /></p>

<p>It looks great! However, several World Cup aspects are currently missing.</p>

<p>Feed the contents of the answers to the Text-to-Image generator <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">Stable Diffusion</a>. 🎨</p>

<p>The World Cup element is still missing from the created visualization, as we can see.</p>

<p><img src="/img/posts/221215-chatgpt-4.webp" alt="" /></p>

<p>Okay! I would like to collect some latest information about Lusail city toward World Cup and share it with ChatGPT to see if it can gather more exciting news!</p>

<p>I created the scripts to parse the web contents from the following websites:</p>

<ul>
  <li><a href="https://www.qatar2022.qa/en/tournament/stadiums/lusail-stadium">https://www.qatar2022.qa/en/tournament/stadiums/lusail-stadium</a></li>
  <li><a href="https://www.bloomberg.com/graphics/2022-what-qatar-built-for-the-world-cup">https://www.bloomberg.com/graphics/2022-what-qatar-built-for-the-world-cup</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Lusail">https://en.wikipedia.org/wiki/Lusail</a></li>
</ul>

<p>Stored in a qatar-worldcup.csv file with described contents and parsed sources.</p>

<p><img src="/img/posts/221215-chatgpt-5.webp" alt="" /></p>

<p>Before we share the raw data with ChatGPT, let’s run the <strong>qatar-worldcup.csv</strong> file through <a href="https://infusesai.pse.is/4ntk57">PipeRider</a>, a data reliability tool that can ensure data quality.</p>

<p>We can examine the data characteristics in PipeRider’s data report and discover that there are some <strong>missing</strong> and <strong>duplicate</strong> contents.</p>

<p><img src="/img/posts/221215-chatgpt-6.webp" alt="" /></p>

<p><img src="/img/posts/221215-chatgpt-7.webp" alt="" /></p>

<p>In <strong>qatar-worldcup.csv</strong>, there are numerous missing values as a result of the script’s attempt to parse text from the image.</p>

<p><img src="/img/posts/221215-chatgpt-8.webp" alt="" /></p>

<p>As for the duplicates, they mainly resulted from the bug of retry logic in our parsing script.</p>

<p><img src="/img/posts/221215-chatgpt-9.webp" alt="" /></p>

<p>After fixing the known issues, the new data report now looks promising!</p>

<p><img src="/img/posts/221215-chatgpt-10.webp" alt="" /></p>

<p>Next, I used another script to automatically communicate parsed contents with ChatGPT. Here are some conversations.</p>

<p><img src="/img/posts/221215-chatgpt-11.webp" alt="" /></p>

<p><img src="/img/posts/221215-chatgpt-12.webp" alt="" /></p>

<p><img src="/img/posts/221215-chatgpt-13.webp" alt="" /></p>

<p>Cool! At the moment, it seems like our friend ChatGPT knows more about Lusail City in terms of the World Cup.</p>

<p>Let me ask the same question again!</p>

<p><img src="/img/posts/221215-chatgpt-14.webp" alt="" /></p>

<p>Excellent! The 2022 FIFA World Cup is included in the introduction of Lusail city!</p>

<p>Again, use the <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">Stable Diffusion</a> to visualize Lusail city from the new ChatGPT’s perspective.</p>

<p><img src="/img/posts/221215-chatgpt-15.webp" alt="" /></p>

<p>Fantastic! Despite the fact that it is far from the actual city of Lusail. 😂</p>

<p>But the visualization shows how much more realistically ChatGPT can explain how a city looks from a sports perspective and how it looks now.</p>

<p>When it becomes a football expert, which I believe it will, I will ask about its predictions for future World Cup champions. 🏆🏆🏆</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The World Cup ⚽️ and ChatGPT have recently been the most popular subjects all across the world. The World Cup is the largest football event conducted every four years, and it is being held in Qatar this year, and ChatGPT is the natural language model for dialogue built by OpenAI.]]></summary></entry><entry><title type="html">Data reliability testing for dbt state with PipeRider</title><link href="https://blog.piperider.io/data-reliability-dbt-state-piperider.html" rel="alternate" type="text/html" title="Data reliability testing for dbt state with PipeRider" /><published>2022-12-06T00:00:00+08:00</published><updated>2022-12-06T00:00:00+08:00</updated><id>https://blog.piperider.io/data-reliability-dbt-state-piperider</id><content type="html" xml:base="https://blog.piperider.io/data-reliability-dbt-state-piperider.html"><![CDATA[<p><em>tl;dr PipeRider now supports <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">dbt state</a>! You can profile and run data reliability tests on only the tables that have been modified as part of your dbt state</em></p>

<hr />

<p>One of the great features of dbt is <a href="https://docs.getdbt.com/reference/node-selection/syntax">node selection</a> and the <a href="https://docs.getdbt.com/reference/node-selection/methods#the-state-method">‘state’ method</a>. Using state, you are able to specify a subset of models (or other resources) to work on. For instance, you might use <code class="language-plaintext highlighter-rouge">state:modified</code> to only build your dbt models that have changed.</p>

<p>The logic behind state is that <strong>there’s no point rebuilding everything if you’ve only changed part of your project</strong>. Likewise, if you’re using a data quality tool with your dbt project (and you should), then the same logic should apply - <strong>you only want to run your data quality and reliability tests against the modified models</strong>.</p>

<p>The latest version of <a href="https://github.com/infuseai/piperider">PipeRider (0.14)</a>, the open-source data reliability tool, adds exactly this feature. If you’re a dbt user and you’re not using PipeRider, you really should check it out. PipeRider adds profiling and data assertions to a dbt project with zero config.</p>

<p>Here’s a <a href="https://www.youtube.com/watch?v=2J2Cu84HonU">video</a> that shows it in action, or scroll down for the relevant commands.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/2J2Cu84HonU" frameborder="0" allowfullscreen=""></iframe></div>

<h2 id="run-data-profiling-and-assertions-on-dbt-state">Run data profiling and assertions on dbt state</h2>

<p>It’s really straightforward to use. Let’s say you’ve modified a dbt model and then built your project specifying only the affected models:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dbt build <span class="nt">--select</span> state:modified+ <span class="nt">--state</span> target
</code></pre></div></div>

<p>Now, when you run PipeRider, all you need to do is specify the dbt state using the <code class="language-plaintext highlighter-rouge">--dbt-state</code> option:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>piperider run <span class="nt">--dbt-state</span> target
</code></pre></div></div>

<p>PipeRider will only profile and test models that are included in the specified dbt state, saving you time and computing resources.</p>

<h2 id="compare-data-profiles">Compare data profiles</h2>

<p>PipeRider also has a handy feature to quickly compare data profile reports:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">piperider</span> <span class="n">compare</span><span class="o">-</span><span class="n">reports</span> <span class="c1">--last</span>
</code></pre></div></div>

<p>You’ll be prompted to select the reports to compare or, if you specify the <code class="language-plaintext highlighter-rouge">--last</code> option, it’ll automatically compare the last two reports - great!</p>

<p>Taking it a step further, you can specify to only compare tables that appear either in the base or target report. This ties in nicely to dbt state because you might not want to include all of the unmodified tables in the comparison report.</p>

<p>To specify which report’s tables to limit the comparison to, use the <code class="language-plaintext highlighter-rouge">--tables-from</code> option.</p>

<div class="language-jsx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">piperider</span> <span class="nx">compare</span><span class="o">-</span><span class="nx">reports</span> <span class="o">--</span><span class="nx">last</span> <span class="o">--</span><span class="nx">tables</span><span class="o">-</span><span class="k">from</span> <span class="nx">target</span><span class="o">-</span><span class="nx">only</span>
</code></pre></div></div>

<p>This command auto compares the last two reports, and only compares tables that appeared in the <em>target</em> report. Which, in this case, would be the report that only includes tables modified as per the dbt state.  This is perfect if you want a report that only focuses on the comparison of modified models before-and-after your latest changes.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">dbt state is a powerful way to build only modified/specified models<br /><br />You can now profile and test <a href="https://twitter.com/hashtag/dbt?src=hash&amp;ref_src=twsrc%5Etfw">#dbt</a> states with PipeRider, available now in version 0.14<br /><br />👀 Read more<a href="https://t.co/vvV3IMhH2Y">https://t.co/vvV3IMhH2Y</a><br /><br />⭐️Star on GitHub<a href="https://t.co/rqp6ynbyeM">https://t.co/rqp6ynbyeM</a><a href="https://twitter.com/hashtag/dataquality?src=hash&amp;ref_src=twsrc%5Etfw">#dataquality</a> <a href="https://twitter.com/hashtag/datareliability?src=hash&amp;ref_src=twsrc%5Etfw">#datareliability</a> <a href="https://twitter.com/hashtag/opensource?src=hash&amp;ref_src=twsrc%5Etfw">#opensource</a> <a href="https://t.co/jk2qvO7uPM">pic.twitter.com/jk2qvO7uPM</a></p>&mdash; InfuseAI (@InfuseAI) <a href="https://twitter.com/InfuseAI/status/1600150571402477569?ref_src=twsrc%5Etfw">December 6, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="get-started-with-piperider">Get started with PipeRider</h2>

<p>As I mentioned above, if you’re a dbt user then PipeRider should be your go-to data reliability tool.  If you already have a dbt project, all you need to do is <a href="https://docs.piperider.io/cli/dbt-integration">initialize PipeRider inside the project</a> and your data source settings will be automatically detected:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># inside your dbt project folder</span>
piperider init
</code></pre></div></div>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[tl;dr PipeRider now supports dbt state! You can profile and run data reliability tests on only the tables that have been modified as part of your dbt state]]></summary></entry><entry><title type="html">5 Database Schema Changes Data Engineers Need to Beware Of</title><link href="https://blog.piperider.io/database-schema-changes-to-beware-of.html" rel="alternate" type="text/html" title="5 Database Schema Changes Data Engineers Need to Beware Of" /><published>2022-11-21T00:00:00+08:00</published><updated>2022-11-21T00:00:00+08:00</updated><id>https://blog.piperider.io/database-schema-changes-to-beware-of</id><content type="html" xml:base="https://blog.piperider.io/database-schema-changes-to-beware-of.html"><![CDATA[<p>As a data or analytics engineer you need to anticipate the unexpected, especially when building pipelines from data sources that you have no control over. While a data pipeline might be a house of cards, with data observability you’re able to keep an eye on any changes to the underlying structure, and be prepared for schema changes.</p>

<p>Structure is the key point here, as one of the main ways that a data source can change is the schema, or structure, of a database table. The schema is important because the pipelines you build may have specific downstream uses that are directly tied to the current structure of the data. By staying ahead of schema changes you’ll know about potential issues before they affect the data and downstream uses.</p>

<p>As Angela, a data engineer at Sam’s Club, put it in a <a href="https://twitter.com/i/spaces/1OwGWwZAZDnGQ?s=20">recent interview</a>:</p>

<blockquote>
  <p>“you never want your end users to actually contact you, (and if they do) you want to be already in the know”</p>
</blockquote>

<p>Here are 5 schema changes to look out for, and why you should beware of them:</p>

<h2 id="1-additional-columns">1. Additional columns</h2>

<p>Additional columns might seem like one of the more benign changes at first. After all, how could adding some columns to a database affect your pipeline?</p>

<h3 id="why-you-should-beware">Why you should beware</h3>

<p>After a new column is added, there’s no easy way to know if the data was backfilled, without manually checking. That is, for each row, was the data retroactively added? If not, there may be null, zero, or empty values, and you won’t know if they represent valid data or not. This could be problematic if you start using the new column without first knowing the backfill status.</p>

<h2 id="2-column-type-and-attribute-changes">2. Column type and attribute changes</h2>

<p>Column types are important because they enforce the type of data contained in the column. The way you use the data of a column is based on the expectation that the data is of a certain type. Examples of column type changes include:</p>

<ul>
  <li>A numeric column changing to string, and vice-versa.</li>
  <li>A numeric type change, such as a decimal changing to an integer.</li>
  <li>Date is a tricky one — is it Date, Datetime, or Timestamp?</li>
</ul>

<h3 id="why-you-should-beware-1">Why you should beware</h3>

<p>If you’re expecting a float and get an integer, this could affect the accuracy of any calculations based on the data, especially if you require a certain precision (see column attributes).</p>

<p>Sometimes, column types might not even be enforced, so you might run into problems with mixing types, such as storing text and numbers in the same column. This means you need to be very careful about checking the format of the data.</p>

<h2 id="3-column-attributes-and-constraints">3. Column attributes and constraints</h2>

<p>Column attributes refer to the format of the stored data. Things like:</p>

<ul>
  <li>String length</li>
  <li>Size of an integer</li>
  <li>Decimal precision</li>
  <li>Date format</li>
  <li>Default values</li>
  <li>If NULL values are allowed</li>
</ul>

<h3 id="why-you-should-beware-2">Why you should beware</h3>

<p>Changes to numeric types such as integer size, or precision, could affect downstream calculations.</p>

<p>The format of the date could also wreak havoc on your dashboard — is it DD-MM-YYYY, or MM-DD-YYYY? What time zone is this for? These are important questions to ask if you’re building a dashboard that relies on time-specific data.</p>

<p>If the limit on the length of strings changes, it could result in truncated data. An issue that might go unnoticed if not manually checked for.</p>

<h2 id="4-column-renaming">4. Column renaming</h2>

<p>Renaming a column is a big change. If tables are being used in production you’d hope to be notified of a change such as this, but with third party data sources that might not happen. This one could really get you if you’re not paying attention.</p>

<h3 id="why-you-should-beware-3">Why you should beware</h3>

<p>Your transformations and data modeling queries rely heavily on column naming. One column name-change could result in a broken pipeline causing data to stop flowing downstream.</p>

<h2 id="5-column-misuse">5. Column misuse</h2>

<p>Column misuse is more of a data quality issue than a schema change but, like a schema change, could cause issues for you when using the data. When a column is repurposed, it’s used for a purpose it wasn’t originally intended for. If there’s no data dictionary, or documentation is not kept up-to-date, then you could easily be caught out by this one.</p>

<h3 id="why-you-should-beware-4">Why you should beware</h3>

<p>One example of column misuse is putting additional data into a column, instead of creating another column. For instance, a product sales table might start including product attributes, such as size and color, in the description column, instead of creating new columns for the attributes.</p>

<p>The meaning of a column might also change over time and, instead of adjusting the database accordingly, an existing column is ‘repurposed’ for use. Without you knowing, the column now includes completely different data than what you expect.</p>

<h2 id="how-to-avoid-being-caught-out">How to avoid being caught out</h2>

<p>As mentioned in the introduction, it’s all a matter of having the knowledge that a change has taken place — As the saying goes “<a href="https://knowyourmeme.com/memes/france-is-bacon">knowledge is power, France is bacon</a>”. If you know a change has taken place, you can react and make sure that data pipeline continues to work, before downstream issues occur.</p>

<p>For data teams, that knowledge comes from <a href="https://blog.infuseai.io/data-monitoring-be-the-master-of-your-pipeline-ba464b66888">data monitoring and observability</a>. With proper data observability in place you can know the current structure of the database, and how that compares to previous states.</p>

<p>Highlighting that schema changes have taken place, along with other data changes, is one of the things <a href="https://blog.infuseai.io/how-to-be-more-confident-making-data-model-changes-76a2f65feffa">we’re working on</a> with <a href="https://github.com/infuseai/piperider">PipeRider</a>.</p>

<h2 id="your-experiences">Your experiences</h2>

<p>Have you been bitten by schema change? Let me know what happened and how you discovered the change.</p>

<p>If I missed any important or obvious schema changes to look out for, please tell me and I’ll improve the list.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Watch out for that schema change! It&#39;s a doozy!<br /><br />As a data engineer, you&#39;re always alerted to upstream table changes, right? (yes that&#39;s sarcasm😛)<br /><br />Use PipeRider to make sure you don&#39;t get caught off guard:<a href="https://t.co/2sFy2ikXYA">https://t.co/2sFy2ikXYA</a><a href="https://twitter.com/hashtag/snowflake?src=hash&amp;ref_src=twsrc%5Etfw">#snowflake</a> <a href="https://twitter.com/hashtag/dataengineering?src=hash&amp;ref_src=twsrc%5Etfw">#dataengineering</a> <a href="https://twitter.com/hashtag/datareliability?src=hash&amp;ref_src=twsrc%5Etfw">#datareliability</a> <a href="https://t.co/m9zpuUP1jY">pic.twitter.com/m9zpuUP1jY</a></p>&mdash; InfuseAI (@InfuseAI) <a href="https://twitter.com/InfuseAI/status/1599780636826472449?ref_src=twsrc%5Etfw">December 5, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[As a data or analytics engineer you need to anticipate the unexpected, especially when building pipelines from data sources that you have no control over. While a data pipeline might be a house of cards, with data observability you’re able to keep an eye on any changes to the underlying structure, and be prepared for schema changes.]]></summary></entry><entry><title type="html">How do you know you can trust your data?</title><link href="https://blog.piperider.io/how-do-you-know-you-can-trust-your-data.html" rel="alternate" type="text/html" title="How do you know you can trust your data?" /><published>2022-10-23T00:00:00+08:00</published><updated>2022-10-23T00:00:00+08:00</updated><id>https://blog.piperider.io/how-do-you-know-you-can-trust-your-data</id><content type="html" xml:base="https://blog.piperider.io/how-do-you-know-you-can-trust-your-data.html"><![CDATA[<p>Every decision in business is made based on supporting data. “Data-driven” is more than just a buzzword for meetings, it’s a way for a company to be self-aware. Using metrics derived from all sorts of data, it’s possible to understand the performance of each operational unit in a business. From sales figures to marketing efforts, from time sheets to billable hours, data helps us determine what is the correct decision to make, backed up by proof in the form of metrics.</p>

<p>With data being such a valuable commodity, there has to be trust in it. By the time data makes its way downstream, it could have undergone many transformations on the way. With each ingestion, transformation, or abstraction, there is the possibility of mistakes being introduced into the data pipeline.</p>

<p>At each step of the way along the pipeline, how do you ensure that the data is correct and can be trusted?</p>

<p><img src="/img/posts/221023-1.webp" alt="" />
<em>If you can’t trust your data, you can’t make data-driven decisions.</em></p>

<h2>　</h2>
<h2 id="can-we-put-that-in-writing">Can we put that in writing?</h2>
<p>Recently, <a href="https://dataproducts.substack.com/p/the-rise-of-data-contracts">data contracts</a> have been hailed as the savior of the relationship between data producer and data consumer. Data contracts define the ‘rules of the game’ by explicitly stating things like schema structure, ingestion frequency, source tables, and even contact information for stakeholders.</p>

<p>By clearly <a href="https://davidsj.substack.com/p/yet-another-post-on-data-contracts#%C2%A7what-a-data-contract-should-contain">defining the requirements of the the data</a>, the data producer knows what data to serve, and in what format, and the data consumer knows what they will be receiving.</p>

<p>Once the scale of your data pipeline reaches a certain threshold, and you’re <a href="/data-monitoring-be-the-master-of-your-pipeline.html">collecting metrics <em>about</em> the pipeline</a>, then it might be time to introduce data contracts. As Gleb, of Data Fold, <a href="https://www.datafold.com/blog/the-best-data-contract-is-the-pull-request">puts it</a>, when “metadata becomes big data… when something breaks, figuring out what the origin is becomes a huge task”. If you’re clear about what’s happening between the connections in the pipeline, this task is made a whole lot easier.</p>

<h2 id="-1">　</h2>
<h2 id="quality-data-prove-it">Quality data? Prove it!</h2>

<p>If you don’t have the luxury of implementing a new system such as data contracts, there are still other things you can do to ensure data quality, and subsequently data trust. Through data profiling and data assertions you’re able to ensure that the data continually meets your requirements.</p>

<p>Data assertions are data ‘contracts’ on some level. Assertions enable you to verify the structure and contents of the data source. If a data assertion fails then you know that some predetermined requirement of the data has not been met. If the the data assertions pass then this is proof that the data meets the needs for downstream use.</p>

<p>Actual proof could take the form of a <a href="https://docs.piperider.io/data-profile-and-metrics/data-profile">data profile</a> that shows a breakdown of metrics about the data, or even better, a set of <a href="https://docs.piperider.io/cli/data-quality-assertions">data assertions</a> and results that prove the data is fit for purpose. By generating profiling reports regularly and automating assertions after ingestions or transformations, you can be sure that unexpected errors are not introduced and the pipeline is functioning correctly.</p>

<h2 id="-2">　</h2>
<h2 id="the-path-to-data-trust">The path to data trust</h2>
<p>The path to trust in a data pipeline comes from confidence that the data is complete. In turn, confidence in that data comes from proving that the data meets the needs of the data consumer, either enforced through data contracts, or demonstrated with assertion test results.</p>

<p>Without some form of proof of data quality, or method to determine that data is not changing in a way that might break downstream usage, there’s no real way to trust that data is accurate.</p>

<p>What methods are you using to test data after ingestions or transformations?</p>

<p>Are you using data contracts, either in production or testing?</p>

<p>I’d love to hear about your use-cases and experiences — please leave a comment and share your experience.</p>

<h2 id="-3">　</h2>
<h3 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h3>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is an open-source data reliability CLI tool that adds data profiling and assertions to data warehouses such as BigQuery, Snowflake, Redshift and more. Data profile and assertions results are provided in an HTML report each time you run PipeRider.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Every decision in business is made based on supporting data. “Data-driven” is more than just a buzzword for meetings, it’s a way for a company to be self-aware. Using metrics derived from all sorts of data, it’s possible to understand the performance of each operational unit in a business. From sales figures to marketing efforts, from time sheets to billable hours, data helps us determine what is the correct decision to make, backed up by proof in the form of metrics.]]></summary></entry><entry><title type="html">Building Reliable Data Pipelines With Profiling and Assertions</title><link href="https://blog.piperider.io/building-reliable-data-pipelines-with-profiling-and-assertions.html" rel="alternate" type="text/html" title="Building Reliable Data Pipelines With Profiling and Assertions" /><published>2022-10-19T00:00:00+08:00</published><updated>2022-10-19T00:00:00+08:00</updated><id>https://blog.piperider.io/building-reliable-data-pipelines-with-profiling-and-assertions</id><content type="html" xml:base="https://blog.piperider.io/building-reliable-data-pipelines-with-profiling-and-assertions.html"><![CDATA[<p>The process of creating a data pipeline involves taking data from various sources and transforming it to make it useful for specific analytical and/or business purposes. It sounds easy enough but, with increasingly complex pipelines, errors can easily go unnoticed.</p>

<p>Creating error-free data pipelines takes a lot of effort from data engineers, but by using data profiling the task of building and maintaining reliable data pipelines is a lot more manageable.</p>

<hr />

<h2 id="data-profiling-and-assertions-for-quality-assurance">Data profiling and assertions for quality assurance</h2>
<p>The metrics that data profiling provides can be used to ensure that data meets the requirements it’s intended for, and also enables you (the data engineer) to spot anomalies or anything out of the ordinary. Essentially providing a layer of quality assurance to the pipeline.</p>

<p>When paired with data assertions, the profile can be tested to ensure that metrics are within desired thresholds. This is not only useful for building the pipeline, but also for continued testing throughout the life of the pipeline. Data profiling is an essential part of creating a dedicated data monitoring pipeline so you can become <a href="/data-monitoring-be-the-master-of-your-pipeline.html">‘master of your pipeline’</a>.</p>

<h2>　</h2>
<h2 id="add-data-profiling-and-assertion-to-your-elt-pipeline">Add data profiling and assertion to your ELT pipeline</h2>
<p>The idea is to augment the ELT process with two new stages — profiling, and assertions, creating (drum-roll) ELTPA. In this updated process, a testing loop occurs during the pipeline creation to help ensure that the resulting data is robust and reliable, and then continually once the pipeline is in use.</p>

<p>The following diagram demonstrates the process.
<img src="/img/posts/221019-1.webp" alt="" /></p>

<h2 id="-1">　</h2>
<h2 id="pipeline-creation">Pipeline creation</h2>
<p>After data is transformed, it is profiled and tested with assertions. You can write assertions based on stakeholder requirements or, if you are modifying an existing pipeline, then you may already be aware of common issues.</p>

<p>If an assertion fails and generates an error, you can go back to the transformation phase and find the root cause of the problem before continuing the process.</p>

<h2 id="-2">　</h2>
<h2 id="pipeline-monitoring-and-observability">Pipeline monitoring and observability</h2>
<p>For continual monitoring, a data profile should also be regularly generated. If the data profile metrics reveal a change in data that needs investigation, you can compare the most recent report with previous reports to see what might have happened.</p>

<p>After the issue is resolved, you can update the data assertions to account for the issue and ensure that testing covers similar situations going forward.</p>

<h2 id="-3">　</h2>
<h2 id="conclusion">Conclusion</h2>
<p>A data profile is useful for creating the data pipeline and, when generated regularly, can also be used for monitoring data distribution over time and debugging issues. The addition of assertions makes it a great way to test for data reliability. In many cases, a data profile report can reduce the need for a complex BI dashboard.</p>

<p>If you like this article, please consider liking and sharing on <a href="https://twitter.com/InfuseAI/status/1582733321594753024?s=20&amp;t=Nw2rTYJOoFJAWdNIdMTxfw">Twitter</a>.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Improve your ELT pipeline creation process with data profiling and data assertions<br /><br />These extra steps at a layer of quality control to your pipeline for build and monitoring data pipelines<a href="https://t.co/cXtJx6BwL4">https://t.co/cXtJx6BwL4</a><a href="https://twitter.com/hashtag/dataobservability?src=hash&amp;ref_src=twsrc%5Etfw">#dataobservability</a> <a href="https://twitter.com/dataprofiling?ref_src=twsrc%5Etfw">@dataprofiling</a> <a href="https://twitter.com/hashtag/datareliability?src=hash&amp;ref_src=twsrc%5Etfw">#datareliability</a></p>&mdash; InfuseAI (@InfuseAI) <a href="https://twitter.com/InfuseAI/status/1582733321594753024?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="-4">　</h2>
<h3 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h3>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is an open-source data reliability CLI tool that adds data profiling and assertions to data warehouses such as BigQuery, Snowflake, Redshift and more. Data profile and assertions results are provided in an HTML report each time you run PipeRider.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The process of creating a data pipeline involves taking data from various sources and transforming it to make it useful for specific analytical and/or business purposes. It sounds easy enough but, with increasingly complex pipelines, errors can easily go unnoticed.]]></summary></entry><entry><title type="html">Data Monitoring — Be the Master of Your Pipeline</title><link href="https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline.html" rel="alternate" type="text/html" title="Data Monitoring — Be the Master of Your Pipeline" /><published>2022-10-18T00:00:00+08:00</published><updated>2022-10-18T00:00:00+08:00</updated><id>https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline</id><content type="html" xml:base="https://blog.piperider.io/data-monitoring-be-the-master-of-your-pipeline.html"><![CDATA[<p>Data pipelines have evolved from a single series of workloads (or DAG within a system), to more abstract semantics with a mix of orchestrated workflows, internal tools, in-warehouse transformations and 3rd party tools exports or ingestion.</p>

<p>The increasing complexity of data pipelines makes monitoring difficult, to implement an effective data monitoring strategy you need to understand the pipeline completely. This could range from tracking the status of pipeline tasks and setting up alerts, right up to being the ultimate “master of your pipeline” by setting up a dedicated monitoring pipeline with metrics.</p>

<hr />

<h2 id="data-monitoring-is-essential">Data monitoring is essential</h2>
<p>Once your data pipeline reaches a certain complexity, the requirement for some kind of monitoring is unavoidable. When you get the call (hopefully monitoring can help you avoid the call) that a dashboard is broken because data isn’t being updated, if you’ve got the pipeline metrics available you’ll know where to look first.</p>

<h3 id="understand-whats-happening">Understand what’s happening</h3>
<p>In its basic form, data monitoring means that you understand what’s happening in the pipeline. Start with understanding the normal execution time for tasks in your data pipelines, then set up SLAs that will raise alerts when they fail.</p>

<p>If you find that individual tasks are hanging, you might set up monitoring for each task separately, and then trigger an alert once a certain time threshold has been exceeded.</p>

<h3 id="how-long-is-too-long">How long is too long?</h3>
<p>How you determine what is a “normal” amount of time for tasks in your pipeline to execute will differ for each case. This is where some sleuthing on your part is required. You’ll need to track the execution time and then work out what is acceptable</p>

<p>Some data warehouses offer execution statistics and, if you use a data monitoring service, check if event logs are available that will contain lots of metadata about your task executions.</p>

<p>Writing tests for each pipeline can be an extensive task, and it requires a lot of effort to maintain. At some point you might consider a more structured approach.</p>

<h2>　</h2>
<h2 id="take-it-to-the-next-level">Take it to the next level</h2>
<p>To get serious about your data monitoring, you can set up a dedicated pipeline for monitoring metrics. In these more complex data monitoring efforts, you can monitor many metrics and set up alerts for each kind of metric.</p>

<h3 id="what-should-you-monitor">What should you monitor?</h3>
<p>Examples of metrics to monitor could include things like -</p>

<ul>
  <li>Data freshness</li>
  <li>Schema types</li>
  <li>The number of rows in a table</li>
  <li>The number of inserted rows</li>
  <li>The percentage of null values</li>
  <li>The acceptable range of values</li>
  <li>The distribution of data</li>
</ul>

<p>The type of metrics you monitor will depend on a mix of what is important to the stakeholder, what issues you are running into on a regular basis and, of course, metrics that you need to monitor to ensure that the pipeline is functioning.</p>

<h3 id="what-are-the-acceptable-thresholds">What are the acceptable thresholds?</h3>
<p>Once you know the metrics that need to be monitored, you need to figure out acceptable thresholds for these metrics. This will either involve asking the stakeholder, or employing some form of data profiling so you can track the metric over time and determine acceptable values and/or ranges.</p>

<h2 id="-1">　</h2>
<h2 id="conclusion">Conclusion</h2>
<p>The complexity of the data monitoring solution you employ will be based on the complexity of your pipeline. I’ve mentioned a few methods that you might use to monitor your data pipelines, but there are many more things to monitor — I’ve not even touched on business logic as that introduced a whole other set of complexity.</p>

<p>What methods are you using for monitoring your data pipelines?</p>

<p>How do you determine what are the important things to monitor and track the metrics over time?</p>

<h2 id="-2">　</h2>
<h3 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h3>
<p><a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a> is an open-source data reliability CLI tool that adds data profiling and assertions to data warehouses such as BigQuery, Snowflake, Redshift and more. Data profile and assertions results are provided in an HTML report each time you run PipeRider.</p>

<p>If you found this article useful please consider liking and <a href="https://twitter.com/InfuseAI/status/1582554616092184576?s=20&amp;t=U8_JhQpFZRqjShVTA98vTg">retweeting</a>❤.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Are you master of your domain, er... pipeline?<br /><br />Data pipelines aren&#39;t getting any simpler!<br /><br />What kind of monitoring are you running and which metrics are important to you?<a href="https://t.co/EevD4la0bo">https://t.co/EevD4la0bo</a><a href="https://twitter.com/hashtag/dataobservability?src=hash&amp;ref_src=twsrc%5Etfw">#dataobservability</a> <a href="https://twitter.com/hashtag/datamonitoring?src=hash&amp;ref_src=twsrc%5Etfw">#datamonitoring</a></p>&mdash; InfuseAI (@InfuseAI) <a href="https://twitter.com/InfuseAI/status/1582554616092184576?ref_src=twsrc%5Etfw">October 19, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Data pipelines have evolved from a single series of workloads (or DAG within a system), to more abstract semantics with a mix of orchestrated workflows, internal tools, in-warehouse transformations and 3rd party tools exports or ingestion.]]></summary></entry><entry><title type="html">5 Metrics Elon Musk Should Check In Twitter’s Data</title><link href="https://blog.piperider.io/5-metrics-elon-musk-should-check-in-twitter-data.html" rel="alternate" type="text/html" title="5 Metrics Elon Musk Should Check In Twitter’s Data" /><published>2022-10-13T00:00:00+08:00</published><updated>2022-10-13T00:00:00+08:00</updated><id>https://blog.piperider.io/5-metrics-elon-musk-should-check-in-twitter-data</id><content type="html" xml:base="https://blog.piperider.io/5-metrics-elon-musk-should-check-in-twitter-data.html"><![CDATA[<p>The value of Twitter has been <a href="https://www.theverge.com/2022/10/4/23387592/elon-musk-twitter-deal-lawsuit-faq">in the news</a> a lot recently. Antics related to Elon Musk’s possible Twitter buyout has led to a legal battle that may result in Twitter releasing a snapshot of data on its users and tweets. If Musk actually gets his hands on this data, what kind of things might he be looking for?</p>

<hr />

<p>Data profiling is a great way to analyze and explore a dataset. A data profile not only helps you to understand the structure of a dataset, but the statistical metrics are also great for exploratory analysis.</p>

<p>In this article, I take some publicly available Twitter datasets and run them through a <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">data profiler</a> to see what interesting metrics I can find.</p>

<h2>　</h2>
<h2 id="the-datasets">The datasets</h2>
<p>I used two datasets for this analysis:</p>

<ul>
  <li>A portion of the <a href="https://archive.org/details/archiveteam-twitter-stream-2021-06">Archive Team Twitter Stream</a> dataset from June 2021. This dataset comes in JSON form, so you’ll need to use a tool such as <a href="https://github.com/cantino/twitter_to_csv">twitter_to_csv</a> to convert the raw Twitter API JSON to CSV.</li>
  <li>A pre-prepared dataset from Kaggle. I found the Gender classification set, with data from 2015, was the most useful as it retained much of the user data, such as profile image URL, retweet count, and profile description.</li>
</ul>

<p>For both methods I used <a href="https://piperider.io/?utm_source=piperiderblog&amp;utm_medium=blog">PipeRider</a>, an open-source CLI tool for profiling datasets. It outputs an HTML report which makes it easy to see the metrics. There are also data assertions if you need to add testing to your dataset.</p>

<h2 id="-1">　</h2>
<h2 id="the-metrics">The metrics</h2>
<p>So, what things might Elon Musk, or anyone analyzing the value of a social network data need to look out for? What interesting metrics can be found, and what are the implications in regard to determining the ‘value’ of Twitter users and Tweets.</p>

<h2 id="-2">　</h2>
<h3 id="1-account-creation-date">1. Account creation date</h3>
<p>Looking at the account created date in the gender dataset, there are active accounts that date back all the way to 2006.</p>

<p><strong>Implication</strong></p>

<p>Twitter has good user retention, as these are users that have been coming back for years.</p>

<p>Twitter account creation date metric via PipeRider: 
<img src="/img/posts/221013-1.webp" alt="" /></p>

<h2 id="-3">　</h2>
<h3 id="2-profile-image">2. Profile image</h3>
<p>You might not think that a profile image could indicate much, but the notorious <a href="https://www.vox.com/2017/3/31/15139192/twitter-egg-profile-default-change">Twitter egg</a> fiasco proves otherwise. 18.3% of profile images in the Kaggle dataset are duplicates, and it looks like this is because they are using the default image.</p>

<p>The Archive Team dataset was around the same, at 17.1% for my sampling.</p>

<p>Twitter profile image metrics via PipeRider:
<img src="/img/posts/221013-2.webp" alt="" /></p>

<p><strong>Implication</strong></p>

<p>This could be an indication of throw-away accounts, bots, trolls, or generally users who aren’t invested enough to update their profile image.</p>

<h2 id="-4">　</h2>
<h3 id="3-duplicated-tweet-content">3. Duplicated Tweet content</h3>
<p>This doesn’t refer to retweets, or quote tweets, but to tweets with identical content. Why would a tweet contain the same content? There was 8.7% of tweets with duplicated content in the Kaggle dataset.</p>

<p><strong>Implication</strong></p>

<p>This could be an indication of a bot farm spreading propaganda, or users could simply be clicking the “share this article” button on news websites.</p>

<p>Twitter metrics indicate possible bot messages via PipeRider: 
<img src="/img/posts/221013-3.webp" alt="" /></p>

<p>The Kaggle dataset shows a lot bizarre of Weather Chanel tweets.</p>

<p>Interestingly, these tweets cannot be found by searching Twitter now. All I can find is <a href="https://twitter.com/ArdentCrayon/status/722136691809525764?s=20&amp;t=9IY8SlHv4k813B-xxwjGxw">someone asking</a> why there are so many bots spamming the weather channel.</p>

<p>The Archive Team dataset shows many retweets of an account for a Korean band (I think). The <a href="https://twitter.com/WeareoneEXO">account</a> has 13million followers, so I suppose a few thousand fans retweeting isn’t out of the ordinary.</p>

<p>Tweet content metrics via PipeRider:
<img src="/img/posts/221013-4.webp" alt="" /></p>

<h2 id="-5">　</h2>
<h3 id="4-retweet-count">4. Retweet count</h3>
<p>Retweet count was extremely low in both datasets — 99.9% of the tweets in the Kaggle set were not retweeted.</p>

<p><strong>Implication</strong></p>

<p>It could mean low interaction rates on Twitter, but is probably just an indication that the dataset time frame is too narrow — only an hour, and that it takes a while for a tweet to gain momentum. This could be one to analyze over a period of a few hours or days.</p>

<p>Retweet metrics via PipeRider:
<img src="/img/posts/221013-5.webp" alt="" /></p>

<h2 id="-6">　</h2>
<h3 id="5-follower-count">5. Follower count</h3>
<p>Follower count not only indicates how popular you are, but also if users are interacting and ‘listening’ to each other on Twitter. Are most users shouting into the wind? According to the Archive Team dataset, 4800 users were talking to themselves during that time frame.</p>

<p><strong>Implication</strong></p>

<p>It’s difficult to gauge this one without more analysis of the accounts with zero followers. They could be the bots we mentioned earlier, or just angry loners shouting in the wind.</p>

<p>Follower count metrics via PipeRider: 
<img src="/img/posts/221013-6.webp" alt="" /></p>

<h2 id="-7">　</h2>
<h2 id="conclusion">Conclusion</h2>
<p>Data exploration is fun, even if you’re not a billionaire mulling over the potential purchase of a $44 billion social network.</p>

<p>Data profiling can provide interesting insights into a dataset that you may not have previously thought of. Even the process of cleaning and transforming data in preparation for analysis can yield interesting results. For instance, while importing the Archive Team dataset, I found that there were many ‘deleted’ entries listed. I presume these are deleted tweets, a figure that could also offer some valuable insight.</p>

<h2 id="-8">　</h2>
<h2 id="add-data-profiling-to-your-data-reliability-strategy">Add data profiling to your data reliability strategy</h2>
<p>If you’re interested in the tool I used here, check out PipeRider — it’s a CLI tool that profiles all the popular data warehouses and provides an HTML data profile report.</p>

<p>Also, before you finish the reading the contracts for your Twitter purchase, please follow us on Twitter — we’re <a href="https://twitter.com/infuseai">@infuseai</a> ❤</p>

<p>If you found this article interesting please <a href="https://twitter.com/InfuseAI/status/1580574430442377217?s=20&amp;t=GwuHVBwmjPMAU8nWlXetBw">retweet</a> to support more content like this.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">5 Metrics that Elon Musk should check in the Twitter data<br /><br />What would you want to check before dropping $44m on a social network?<a href="https://t.co/LzSxGz5Lz9">https://t.co/LzSxGz5Lz9</a><a href="https://twitter.com/hashtag/opensource?src=hash&amp;ref_src=twsrc%5Etfw">#opensource</a> <a href="https://twitter.com/hashtag/dataprofiling?src=hash&amp;ref_src=twsrc%5Etfw">#dataprofiling</a> <a href="https://twitter.com/hashtag/eda?src=hash&amp;ref_src=twsrc%5Etfw">#eda</a> <a href="https://twitter.com/hashtag/dataanalysis?src=hash&amp;ref_src=twsrc%5Etfw">#dataanalysis</a> <a href="https://twitter.com/hashtag/dataobservability?src=hash&amp;ref_src=twsrc%5Etfw">#dataobservability</a></p>&mdash; InfuseAI (@InfuseAI) <a href="https://twitter.com/InfuseAI/status/1580574430442377217?ref_src=twsrc%5Etfw">October 13, 2022</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The value of Twitter has been in the news a lot recently. Antics related to Elon Musk’s possible Twitter buyout has led to a legal battle that may result in Twitter releasing a snapshot of data on its users and tweets. If Musk actually gets his hands on this data, what kind of things might he be looking for?]]></summary></entry><entry><title type="html">Guide to Data Reliability</title><link href="https://blog.piperider.io/guide-to-data-reliability.html" rel="alternate" type="text/html" title="Guide to Data Reliability" /><published>2022-09-27T00:00:00+08:00</published><updated>2022-09-27T00:00:00+08:00</updated><id>https://blog.piperider.io/guide-to-data-reliability</id><content type="html" xml:base="https://blog.piperider.io/guide-to-data-reliability.html"><![CDATA[<p>Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.</p>

<p>You may be collecting a large amount of information within your data stack, but are you certain that these data sets are complete, accurate, and up-to-date? If not, these data sets might cost you a lot.</p>

<p>IBM estimated that the yearly cost of poor quality data, in the US alone, in 2016, is a whopping $3.1 trillion. In 2021, Gartner reported that every year, unreliable data costs organizations an average of $12.9 million. And it’s safe to say that the number has very likely increased as data-driven decision-making is adopted by every business imaginable.</p>

<p>That’s why ensuring your data is trustworthy by improving data reliability is very important.</p>

<h2 id="what-is-data-reliability">What is Data Reliability?</h2>

<p>Data reliability means that data is complete, accurate, and valid. It’s the foundation for building trust in your data across the organization. One of the main objectives of ensuring data reliability is building data trust, which is also used to maintain data security, data quality, and regulatory compliance.</p>

<p>Reliable data helps decision-makers take the guesswork out of the daily and strategic decision-making process to keep their organizations running. But if your data is unreliable, those same decisions become less accurate and can ultimately affect your organization.</p>

<h2 id="why-data-reliability">Why Data Reliability</h2>

<p>When unreliable data is used in making a key strategic decision, it can result in a mistake that damages an organization’s reputation, and bottom line, or even causes its future. Data reliability issues might not seem like a big deal at first glance, but they can snowball over time if left unchecked.</p>

<p>For example, you use customer data to develop targeted online ads or recommend products to your consumers. If the data you use isn’t accurate, then there’s a good chance that the advertising budget will be wasted on either poor results or zero return on investment.</p>

<p>The unsettling feeling when you are not sure if you can trust your data to make a decision can be highly stressful, ut there are actions you can take to improve your data reliability.</p>

<h2 id="how-to-improve-data-reliability">How to Improve Data Reliability</h2>

<p>Like many other managerial tasks, the process to improve your data reliability follows a series of logical steps. There are eight action items that your organization can take to improve your data reliability:</p>

<ol>
  <li>Assess Data Status</li>
  <li>Build Data Infrastructure</li>
  <li>Clean Existing Data</li>
  <li>Optimize Data Collection Processes</li>
  <li>Break Down All Data Silos</li>
  <li>Integrate Data Stack to Connect Data</li>
  <li>Organize Your Data</li>
  <li>Use Reports and Dashboards</li>
</ol>

<h3 id="assess-data-status">Assess Data Status</h3>

<p>Assessing your current data status is the first thing to do to improve data reliability. It helps you to get a general view of how your organization treats data. You should also employ data profiling. Data profiling is the process of examining and analyzing data. This helps you understand if your data is healthy. Assess your current situation to understand:</p>

<p>What are your data sources;
How and where you have stored the data;
How and where you have used the data;
The criteria used to determine data reliability.</p>

<h3 id="build-data-infrastructure">Build Data Infrastructure</h3>

<p>Once you’ve assessed your current situation, you can start updating your data infrastructure. No matter what the original data sources are, you need a secure and easy-to-use data repository. You need to define how your data will be stored, formatted, and organized. There are several steps you can take to create a data infrastructure:</p>

<p>Refine your strategy.
Build a data model.
Choose your data repository type – data lake, data warehouse, or hybrid.
Build an extract, transform, and load (ETL) process.
Implement ongoing data governance.</p>

<h3 id="clean-existing-data">Clean Existing Data</h3>

<p>If you have data sets in place already, you should examine the existing data and remove data that is:</p>

<ul>
  <li>inaccurate;</li>
  <li>incomplete;</li>
  <li>duplicative;</li>
  <li>outdated;</li>
  <li>incorrectly formatted.</li>
</ul>

<p>You should employ data profiling to analyze your data continually, so you can clean, and update data errors as soon as they are spotted.</p>

<h3 id="optimize-data-collection-processes">Optimize Data Collection Processes</h3>

<p>Start by analyzing internal processes for data input. Automate data entry wherever possible to minimize human errors. Make sure that all data entry follows your standardized formats and is accurate, complete, and valid.</p>

<p>Next, look at other data sources you obtain new data from. Make sure that their data formats follow your standardized format and remove inaccurate and unreliable data.</p>

<h3 id="break-down-all-data-silos">Break Down All Data Silos</h3>

<p>Organizations collect data from different departments or locations. This is necessary due to operational requirements or structure setup. But this might create independent data silos that would affect data reliability.</p>

<p>Not only do silos make it difficult to find and share data across your organization, but they also often adhere to different standards of organization and quality.</p>

<p>To ensure the most reliable data is available to those who need it internally, you need to break down your organization’s data silos. You should employ a central data repository for all departments and locations to minimize potential damage to data quality.</p>

<h3 id="integrate-data-stack-to-connect-data">Integrate Data Stack to Connect Data</h3>

<p>Quite often, different departments or locations use various tools and platforms. If you can get everyone to use the same tool and platform, great. If not, you should connect data from these tools and platforms across your entire organization to have a unified view of all your data. Therefore, when a piece of data is updated in one location, it is automatically updated wherever else it is used.</p>

<h3 id="organize-your-data">Organize Your Data</h3>
<p>Every organization has its unique way of organizing data to meet its unique needs. Organizing data makes it easier to locate specific data and speeds up your data retrieval process.</p>

<p>Typically, you will find labels, tags, groups, and other information stored in metadata. Depending on the type and use of your data, you may find data segmented by customer age, gender, geographic location, demographics, etc. No matter how you organize your data, make sure you understand the overall organization’s expectations and what it would like to achieve using the data.</p>

<h3 id="use-reports-and-dashboards">Use Reports and Dashboards</h3>

<p>Finally, make sure you are able to get insight from your data with reports and dashboards. For example, a data profile report can continue to alert you of data errors when it occurs. Other reports that track key metrics in a visual way with detailed analyses put peace of mind in you when it comes to making data-driven decisions.</p>

<h2 id="automate-your-data-reliability-with-piperider">Automate Your Data Reliability With PipeRider</h2>

<p>It may feel overwhelming when you manage a large amount of data, but once you lay the groundwork and build the foundation, there are many tools you can use to make the journey easier. If you’re interested in learning more about your data, with the aim of improving your data reliability, PipeRider can help you.</p>

<p>PipeRider is an open-source, free, and easy-to-use data reliability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements, enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[Modern organizations have gained unprecedented access to quantitative and qualitative data. With all this information available, it’s become best practice for every team to make data-driven decisions. But there’s a problem.]]></summary></entry><entry><title type="html">Data Observability Explained and How to Integrate It into Your Workflow</title><link href="https://blog.piperider.io/guide-to-data-observability.html" rel="alternate" type="text/html" title="Data Observability Explained and How to Integrate It into Your Workflow" /><published>2022-09-23T00:00:00+08:00</published><updated>2022-09-23T00:00:00+08:00</updated><id>https://blog.piperider.io/guide-to-data-observability</id><content type="html" xml:base="https://blog.piperider.io/guide-to-data-observability.html"><![CDATA[<p>The number of data sources that data teams have to deal with is ever increasing. According to a <a href="https://www.matillion.com/resources/blog/matillion-and-idg-survey-data-growth-is-real-and-3-other-key-findings">recent survey</a> by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.</p>

<h2 id="what-is-data-observability">What is Data Observability?</h2>

<p>Data observability is the capability to comprehend, assess, and manage the state of data consumed by various technologies throughout the entire data lifecycle.</p>

<p>With data observability, your team can have a better understanding of your data. So they can gather consistent, standardized data from APIs, support data lake observability, facilitate routine queries to data warehouses, and share high-quality data across the entire organization.</p>

<h2 id="why-data-observability">Why Data Observability?</h2>

<p>One of the benefits of data observability is that teams can monitor data pipelines and quickly identify data issues with end-to-end data visibility.</p>

<p>Before data observability, teams might struggle with various data issues such as outdated data, broken data pipelines, or missing data. These issues might be caused by uncertainty in data standards or different data models from different data providers.</p>

<p>With data observability, your team can</p>

<ul>
  <li>standardize data for monitoring;</li>
  <li>debug and triage proactively;</li>
  <li>understand how data interacts with different tools;</li>
  <li>identify issues early;</li>
  <li>minimize the negative impact of data issues.</li>
</ul>

<p>Data observability also makes it possible for your team to automate parts of your monitoring process to constantly improve data quality with less time spent.</p>

<h2 id="what-does-data-observability-track">What Does Data Observability Track?</h2>

<p>Data profiling is an essential part of data observability. Through the following data profiling techniques, you can further understand your data and apply checks that will alert you to issues with your data.</p>

<p>Row-level validation and column-level profiling provide information about the system-wide performance of your data.
Anomaly detection helps spot problems before they damage data quality.
A statistics summary provides an in-depth understanding of the elements of your data observability framework.
Execution metadata and delays analysis throughout data pipelines to prevent data downtime.</p>

<p>These observability techniques should give you a comprehensive insight into the overall data health, potential data issues, and the quality of your data.</p>

<p>Incorporate Data Observability into Work to Improve Data Quality (h2)
According to research, one-third of data analysts spend more than 40 percent of their time on standardizing data to make it ready for analysis, and 57 percent of organizations still regard the “work of transforming their data to be very difficult.” It is obvious that ensuring consistent and accurate data can be a difficult and expensive task for organizations.</p>

<p>Therefore, having proper and solid data observability set up not only saves time but also a lot of resources, including money - But how do you incorporate data observability into your data quality workflow? You should start by developing a framework, then a strategy, and based on these two, choose the right tool for data observability.</p>

<h2 id="how-to-develop-a-data-observability-framework">How to Develop a Data Observability Framework</h2>

<p>Start your data observability journey by creating an efficient data-driven framework focusing on data quality, consistency, and reliability.</p>

<p>A data observability framework should answer the following questions:</p>

<ul>
  <li>How fresh and up-to-date is our data?</li>
  <li>What expected data value should we verify to ensure credible data?</li>
  <li>What data do we need to track and test to see when the data is broken?</li>
  <li>What is the responsibility of each team to various data sets?</li>
  <li>What other workflow, such as gathering metadata, or mapping upstream data sources and downstream users, do we need?</li>
</ul>

<p>The framework should give your team an overall view of standardized data across the organization, letting them quickly identify and fix problems.</p>

<h2 id="how-to-develop-a-data-observability-strategy">How to Develop a Data Observability Strategy</h2>

<p>Once you have a framework in place, many teams may jump right into integrating data observability into the entire data stack. But putting data observability into practice goes beyond the tools you employ.</p>

<p>You should start with preparing your team to adopt a culture of data-driven collaboration. Think about how to integrate data across different teams and sources, and also consider if implementing a new observability tool will affect existing workflow and resources.</p>

<p>Then incorporate the framework into your strategy to determine a standardized library/guidelines with the characteristics of quality data. Your team can use the guidelines to connect data from all sources.</p>

<p>Finally, incorporate your data sources into the observability tool. To obtain the metrics, logs, and traces required to provide end-to-end visibility, you might need to create new observability pipelines. Correlate the metrics you are tracking in your tool with targeted organization goals after adding the governance and data management rules. By using your observability tool to identify and address problems, you can also find new ways to automate some of your data management processes.</p>

<h2 id="how-to-choose-the-right-data-observability-tool">How to Choose the Right Data Observability Tool</h2>

<p>While there’s no one tool to fit every organization’s needs, a good observability tool should be able to:</p>

<ul>
  <li>gather, examine, sample, and process telemetry data from various data sources;</li>
  <li>detect and alert problems in datasets;</li>
  <li>provide end-to-end visibility;</li>
  <li>display data visualizations.</li>
</ul>

<p>In order to choose a suitable data observability tool, you’ll need to examine your current data stack and get a full picture of how data is gathered and distributed. Then you can look into a tool that is ready to integrate all of your data sources. Your chosen tool should be able to monitor your data in real-time throughout its lifecycle and monitor existing data without extraction. In addition, it should also be able to automate your data observability with minimum effort.</p>

<p>Ultimately, your organization’s specific data stack and data engineering requirements will determine the right tool for you. For the best implementation experience, give top priority to finding a tool that requires less work to standardize your data, map your data, or monitor your data.</p>

<h2 id="integrate-data-observability-with-piperider">Integrate Data Observability with PipeRider</h2>

<p>PipeRider is an open-source, free, and easy-to-use data observability tool with data profiling and data quality checks through assertions. It executes no-code data profiling and test assertions against your dataset with simple commands. It recommends assertions to save you time and renders your test results into a visual report in minutes. Using the data profiling report you can verify that the data meets your requirements enabling you to trust your data and make better decisions. PipeRider embraces the modern data stack and connects anywhere on your data pipeline that uses a supported data source.</p>]]></content><author><name>PipeRider</name></author><summary type="html"><![CDATA[The number of data sources that data teams have to deal with is ever increasing. According to a recent survey by Matillion and IDG, the number of data sources per organization is around 400, with over 20 percent of organizations having 1000 or more. The sheer amount of data makes managing and tracking it increasingly difficult, never mind understanding the bigger picture. That’s where data observability comes in.]]></summary></entry></feed>